{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a26ad83",
   "metadata": {},
   "source": [
    "# Competition 3: Team 21\n",
    "\n",
    "112062649 王俊皓\n",
    "\n",
    "112062650 廖士傑\n",
    "\n",
    "##  Reverse Image Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "414f827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class experimental_settings:\n",
    "    def __init__(self,\n",
    "                 enc=True,\n",
    "                 gen=True,\n",
    "                 dis=True,\n",
    "                 enc_do_batchnorm=False,\n",
    "                 delete_checkpoint=False):\n",
    "        self.enc = enc\n",
    "        self.gen = gen\n",
    "        self.dis = dis\n",
    "        self.enc_do_batchnorm = enc_do_batchnorm\n",
    "        self.delete_checkpoint = delete_checkpoint # not implemented yet\n",
    "        \n",
    "        # ============================ #\n",
    "        # automatic\n",
    "        # ============================ #\n",
    "        \n",
    "        self.caption_type = 'sentence' if self.enc else 'id'\n",
    "\n",
    "\n",
    "expSettings = experimental_settings(enc=True,\n",
    "                                    gen=True,\n",
    "                                    dis=True,\n",
    "                                    delete_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32322ac2",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "15576cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7b646",
   "metadata": {},
   "source": [
    "GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "d992c613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71f7c6",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "58104ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "3bf3f5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "d14b0e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n",
      "tf.Tensor(b'the flower shown has yellow anther red pistil and bright red petals <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def id2Sent(ids):\n",
    "    return \" \".join([id2word_dict[idx] for idx in ids]).strip()\n",
    "\n",
    "#def batch_id2Sent(batch_ids):\n",
    "    #return [id2Sent(ids) for ids in batch_ids]\n",
    "    \n",
    "def batch_id2Sent(batch_ids):\n",
    "    def process_single(ids):\n",
    "        # Convert a single tensor of IDs to a sentence\n",
    "        ids = ids.numpy()  # Convert Tensor to NumPy\n",
    "        sentence = \" \".join([id2word_dict.get(idx, \"<UNK>\") for idx in ids])  # Handle unknown IDs\n",
    "        return sentence\n",
    "\n",
    "    # Use tf.py_function to apply Python function inside the TensorFlow graph\n",
    "    sentences = tf.map_fn(\n",
    "        lambda ids: tf.py_function(process_single, [ids], tf.string),\n",
    "        batch_ids,\n",
    "        fn_output_signature=tf.string\n",
    "    )\n",
    "    return sentences\n",
    "\n",
    "\n",
    "print(sent2IdList(text))\n",
    "print(id2Sent(sent2IdList(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "23133915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "text2ImgData = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(text2ImgData)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "d2c49264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption2string(cap):\n",
    "    output = []\n",
    "    for sen in cap:\n",
    "        s = \" \".join([id2word_dict[idx] for idx in sen]).strip()\n",
    "        output.append(s.split(' <PAD>')[0])\n",
    "    return output\n",
    "\n",
    "# adding caption as strings\n",
    "text2ImgData['Captions_string'] = text2ImgData['Captions'].apply(caption2string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "cdf9314c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>Captions_string</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \\\n",
       "ID                                   \n",
       "6734  ./102flowers/image_06734.jpg   \n",
       "6736  ./102flowers/image_06736.jpg   \n",
       "6737  ./102flowers/image_06737.jpg   \n",
       "6738  ./102flowers/image_06738.jpg   \n",
       "6739  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                        Captions_string  \n",
       "ID                                                       \n",
       "6734  [the petals of the flower are pink in color an...  \n",
       "6736  [this flower has white petals and yellow pisti...  \n",
       "6737  [the petals on this flower are pink with white...  \n",
       "6738  [the flower has a smooth purple petal with whi...  \n",
       "6739  [this white flower has bright yellow stamen wi...  "
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2ImgData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "f4a6b09d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the petals of the flower are pink in color and have a yellow center',\n",
       "  'this flower is pink and white in color with petals that are multi colored',\n",
       "  'the purple petals have shades of white with white anther and filament',\n",
       "  'this flower has large pink petals and a white stigma in the center',\n",
       "  'this flower has petals that are pink and has a yellow stamen',\n",
       "  'a flower with short and wide petals that is light purple',\n",
       "  'this flower has small pink petals with a yellow center',\n",
       "  'this flower has large rounded pink petals with curved edges and purple veins',\n",
       "  'this flower has purple petals as well as a white stamen']]"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2ImgData['Captions_string'][:1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "d5efa616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_SIZE = 64\n",
    "IMAGE_HEIGHT = IMAGE_SIZE\n",
    "IMAGE_WIDTH = IMAGE_SIZE\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path, caption_type='id'):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    if caption_type == 'id':\n",
    "        caption = tf.cast(caption, tf.int32)\n",
    "    elif caption_type == 'sentence':\n",
    "        caption = tf.convert_to_tensor(caption, dtype=tf.string)\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator, caption_type='id'):\n",
    "    # load the training data into two NumPy arrays\n",
    "    if filenames != None:\n",
    "        df = pd.read_pickle(filenames)\n",
    "    else:\n",
    "        df = text2ImgData\n",
    "    \n",
    "    if caption_type == 'id':\n",
    "        captions = df['Captions'].values\n",
    "    elif caption_type == 'sentence':\n",
    "        captions = df['Captions_string'].values\n",
    "    else:\n",
    "        raise ValueError('for dataset_generator, caption_type= should be \\'id\\' or \\'sentence\\'.')\n",
    "        \n",
    "    caption = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose one of them randomly for training\n",
    "    \n",
    "    # ============================================ #\n",
    "    # TODO: augmentation\n",
    "    # idea 1 (difficulty: easy)\n",
    "    #     training data has multiple captions, right now it picks a random one.\n",
    "    #     we can make it so that every caption is an entry and multiple captions link to the same image.\n",
    "    # idea 2 (difficulty: medium)\n",
    "    #     after text embedding, use the average of 2 caption embeddings to generate a new caption.\n",
    "    #     the data does not need to have an image tied to it, it just have the label 0 (fake image).\n",
    "    # ============================================ #\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    if caption_type == 'id':\n",
    "        caption = caption.astype(np.int)\n",
    "        \n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    datagen_func = lambda cap, img: data_generator(cap, img, caption_type=caption_type)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(datagen_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "710669ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = dataset_generator(\n",
    "    #data_path + '/text2ImgData.pkl',\n",
    "    None,\n",
    "    BATCH_SIZE, \n",
    "    training_data_generator, \n",
    "    caption_type=expSettings.caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "d9640f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (64, 64, 64, 3)\n",
      "Caption shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "# dataset testing ground\n",
    "for img, cap in dataset.take(1):\n",
    "    print(\"Image shape:\", img.numpy().shape)\n",
    "    print(\"Caption shape:\", cap.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "4e941524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2DTranspose, Conv2D, BatchNormalization, LeakyReLU, Dense\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "# custom layers\n",
    "class flattened_dense(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a dense layer that is made compatible with convolution layers\n",
    "    by flattening the input first and followed by a dense layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=64, kernel_initializer=\"glorot_uniform\"):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(channels, kernel_initializer=kernel_initializer)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        fl = self.flatten(inputs)\n",
    "        return self.dense(fl)\n",
    "    \n",
    "class conv_block(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a convolution layer with batch normalization and leaky relu activation\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=128, kernel_size=1, strides=1, kernel_initializer='glorot_uniform'):\n",
    "        super().__init__()\n",
    "        self.conv = Conv2D(filters=filters,\n",
    "                           kernel_size = (kernel_size, kernel_size),\n",
    "                           strides=(strides, strides),\n",
    "                           padding='same',\n",
    "                           kernel_initializer=kernel_initializer)\n",
    "        self.bn = BatchNormalization()\n",
    "        self.activation = LeakyReLU(alpha=0.1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.activation(self.bn(self.conv(inputs)))\n",
    "    \n",
    "class deconv_block(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a deconvolution layer with batch normalization and leaky relu activation\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=128, kernel_size=4, strides=2, kernel_initializer='glorot_uniform'):\n",
    "        super().__init__()\n",
    "        self.deconv = Conv2DTranspose(filters=filters,\n",
    "                                    kernel_size = (kernel_size, kernel_size),\n",
    "                                    strides=(strides, strides),\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=kernel_initializer)\n",
    "        self.bn = BatchNormalization()\n",
    "        self.activation = LeakyReLU(alpha=0.1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.activation(self.bn(self.deconv(inputs)))\n",
    "    \n",
    "class dense_block(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a dense layer with batch normalization and leaky relu activation\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=128, kernel_initializer='glorot_uniform'):\n",
    "        super().__init__()\n",
    "        self.d = Dense(filters, kernel_initializer=kernel_initializer)\n",
    "        self.bn = BatchNormalization()\n",
    "        self.activation = LeakyReLU(alpha=0.1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self.d(inputs)\n",
    "        outputs = self.bn(outputs)\n",
    "        return self.activation(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "7b79a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas, experimental=False, do_batchnorm=False):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.exp=experimental\n",
    "        self.do_batchnorm = do_batchnorm\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "        if self.exp:\n",
    "            self.embed = hub.load('./checkpoints/universal_sentence_encoder')\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        if self.exp:\n",
    "            with tf.device('/CPU:0'): # TODO if you find a way to use GPU, go for it.\n",
    "                output_last = self.embed(text)\n",
    "                \n",
    "            state = hidden # not updating state for compatibility reasons\n",
    "            \n",
    "        else:\n",
    "            text = self.embedding(text)\n",
    "            output, state = self.gru(text, initial_state = hidden)\n",
    "            output_last = output[:, -1, :]\n",
    "        \n",
    "        # normalization in-batch\n",
    "        if self.do_batchnorm:\n",
    "            mean = tf.reduce_mean(output_last, axis=0, keepdims=True)  # Mean across the batch\n",
    "            std = tf.math.reduce_std(output_last, axis=0, keepdims=True)  # Std across the batch\n",
    "            normalized = (output_last - mean) / (std + 1e-6)  # Avoid division by zero\n",
    "        else:\n",
    "            normalized = output_last\n",
    "        \n",
    "        return normalized, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "e7dcc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas, experimental=False):\n",
    "        super(Generator, self).__init__()\n",
    "        self.exp = experimental\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'], kernel_initializer=\"glorot_uniform\")\n",
    "        self.d2 = tf.keras.layers.Dense(64*64*3, kernel_initializer=\"glorot_uniform\")\n",
    "        if self.exp:\n",
    "            self.deconv_depth = int(math.log(IMAGE_SIZE, 2)) - 1\n",
    "            self.starter = dense_block(filters=2*2*256)\n",
    "            self.deconv = [deconv_block(filters=(2 ** (8 - i)), kernel_initializer=HeNormal()) for i in range(self.deconv_depth)]\n",
    "            self.headf = conv_block(filters=3, kernel_size=1, strides=1)\n",
    "            \n",
    "    def call(self, text, noise_z, debug_output=False):\n",
    "        # deconvolution\n",
    "        if self.exp:\n",
    "            noisy_text = tf.concat([text, noise_z], axis=1) * 10 # amplify the input a bit, they seem fairly close to 0.\n",
    "            img = self.starter(noisy_text)\n",
    "            \n",
    "            img = tf.reshape(img, [-1, 2, 2, 256])\n",
    "            debug = []\n",
    "            for i in range(self.deconv_depth):\n",
    "                debug.append(img)\n",
    "                img = self.deconv[i](img)\n",
    "\n",
    "            img = self.headf(img)\n",
    "            logits = tf.reshape(img, [-1, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "            output = tf.nn.tanh(logits)\n",
    "        \n",
    "        # concatenate input text and random noise\n",
    "        else:\n",
    "            text = self.flatten(text)\n",
    "            text = self.d1(text)\n",
    "            text = tf.nn.leaky_relu(text)\n",
    "            text_concat = tf.concat([noise_z, text], axis=1)\n",
    "            text_concat = self.d2(text_concat)\n",
    "        \n",
    "            logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "            output = tf.nn.tanh(logits)\n",
    "            debug_output = output\n",
    "        \n",
    "        if debug_output:\n",
    "            return logits, output, debug\n",
    "        else:\n",
    "            return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "a78cfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas, experimental=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.exp = experimental\n",
    "        self.hparas = hparas\n",
    "        if self.exp:\n",
    "            #self.resnet_base = ResNet50(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), weights='imagenet', include_top=False)\n",
    "            #for layer in self.resnet_base.layers:\n",
    "                #layer.trainable = False\n",
    "            self.conv1 = conv_block(filters=256, kernel_size=3, strides=1)\n",
    "            self.conv2 = conv_block(filters=64, kernel_size=3, strides=1)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, img, text):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d_text(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        if self.exp:\n",
    "            #img = self.resnet_base(img)\n",
    "            img = self.conv1(img)\n",
    "            img = self.conv2(img)\n",
    "        img = self.flatten(img)\n",
    "        img = self.d_img(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=1)\n",
    "        \n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac84cb",
   "metadata": {},
   "source": [
    "Parameters and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "46263c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LR_GEN': 1e-3,\n",
    "    'LR_DIS': 1e-4,\n",
    "    'LR_DECAY': 0.5,                          # unused\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 1000,                            # number of epoch for demo\n",
    "    'N_SAMPLE': num_training_sample,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 1                           # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "a3d61caf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(hparas, \n",
    "                           experimental=expSettings.enc,\n",
    "                           do_batchnorm=expSettings.enc_do_batchnorm)\n",
    "\n",
    "generator = Generator(hparas,\n",
    "                      experimental=expSettings.gen)\n",
    "\n",
    "discriminator = Discriminator(hparas,\n",
    "                              experimental=expSettings.dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "90824abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (64, 64, 64, 3)\n",
      "Caption shape: (64,)\n",
      "Caption embed shape: (64, 512)\n"
     ]
    }
   ],
   "source": [
    "# test text encoder\n",
    "for img, cap in dataset.take(1):\n",
    "    print(\"Image shape:\", img.numpy().shape)\n",
    "    print(\"Caption shape:\", cap.shape)\n",
    "    with tf.device('/CPU:0'):\n",
    "        output, _ = text_encoder(cap, 0)\n",
    "        print(\"Caption embed shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "220a1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "3a45f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "21f720d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR_GEN'], clipvalue=2.0)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR_DIS'], clipvalue=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "24d5f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "if expSettings.delete_checkpoint:\n",
    "    for f in os.listdir(hparas['CHECKPOINTS_DIR']):\n",
    "        file_path = os.path.join(hparas['CHECKPOINTS_DIR'], f)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "\n",
    "# one benefit of tf.train.Checkpoint() API is we can save everything seperately\n",
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 text_encoder=text_encoder,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "ckptManager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=hparas['CHECKPOINTS_DIR'], max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "b35f60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, caption, hidden, imshow=False):\n",
    "    # random noise for generator\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=0.1)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        text_embed, hidden = text_encoder(caption, hidden)\n",
    "        _, fake_image = generator(text_embed, noise)\n",
    "        if imshow:\n",
    "            plt.imshow(fake_image[0])\n",
    "\n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "        g_loss = generator_loss(fake_output)\n",
    "        d_loss = discriminator_loss(real_logits, fake_logits)\n",
    "\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grad_d = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_d, discriminator.trainable_variables))\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "c25ce5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5d5af",
   "metadata": {},
   "source": [
    "Sample Debugging (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "d68c0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "09b260b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size, caption_type='id'):\n",
    "    if caption_type == 'sentence':\n",
    "        caption = caption2string(caption)\n",
    "    caption = np.asarray(caption)\n",
    "    if caption_type == 'id':\n",
    "        caption = caption.astype(np.int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "e086c8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'], caption_type=expSettings.caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "2946f2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption shape: (64,)\n",
      "Caption embeddings: tf.Tensor(\n",
      "[[-0.01346336  0.06881763 -0.05529514 ... -0.01518281 -0.00633275\n",
      "   0.01500697]\n",
      " [-0.01346336  0.06881763 -0.05529514 ... -0.01518281 -0.00633275\n",
      "   0.01500697]\n",
      " [-0.01346336  0.06881763 -0.05529514 ... -0.01518281 -0.00633275\n",
      "   0.01500697]\n",
      " ...\n",
      " [-0.00790838  0.06233827  0.01107207 ... -0.02398296 -0.03450323\n",
      "  -0.00868433]\n",
      " [-0.00790838  0.06233827  0.01107207 ... -0.02398296 -0.03450323\n",
      "  -0.00868433]\n",
      " [-0.00790838  0.06233827  0.01107207 ... -0.02398296 -0.03450323\n",
      "  -0.00868433]], shape=(64, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# test the sample dataset\n",
    "for cap in sample_sentence.take(1):\n",
    "    print(\"Caption shape:\", cap.numpy().shape)\n",
    "    emb, _ = text_encoder(cap, None)\n",
    "    print(\"Caption embeddings:\", emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "c4af2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples/demo'):\n",
    "    os.makedirs('samples/demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34dfd3",
   "metadata": {},
   "source": [
    "Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "620661c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        imshow = False\n",
    "        \n",
    "        for image, caption in dataset:\n",
    "            g_loss, d_loss = train_step(image, caption, hidden, imshow=imshow)\n",
    "            imshow = False\n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        print('======================================')\n",
    "        \n",
    "        # save the model\n",
    "        if True:\n",
    "            ckptManager.save()\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], 'samples/demo/train_{:02d}.jpg'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "6c752c76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/GatherV2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/GatherV2_grad/Reshape:0\", shape=(None, 320), dtype=float32), dense_shape=Tensor(\"gradients/EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/GatherV2_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen_loss: 0.6016, disc_loss: 1.2478\n",
      "Time for epoch 1 is 41.5400 sec\n",
      "======================================\n",
      "Epoch 2, gen_loss: 0.6682, disc_loss: 0.3734\n",
      "Time for epoch 2 is 27.1890 sec\n",
      "======================================\n",
      "Epoch 3, gen_loss: 0.6774, disc_loss: 0.2520\n",
      "Time for epoch 3 is 26.4374 sec\n",
      "======================================\n",
      "Epoch 4, gen_loss: 0.4882, disc_loss: 2.6493\n",
      "Time for epoch 4 is 26.1994 sec\n",
      "======================================\n",
      "Epoch 5, gen_loss: 0.4476, disc_loss: 1.7436\n",
      "Time for epoch 5 is 25.2123 sec\n",
      "======================================\n",
      "Epoch 6, gen_loss: 0.4663, disc_loss: 1.5268\n",
      "Time for epoch 6 is 25.3700 sec\n",
      "======================================\n",
      "Epoch 7, gen_loss: 0.4686, disc_loss: 1.4509\n",
      "Time for epoch 7 is 25.6592 sec\n",
      "======================================\n",
      "Epoch 8, gen_loss: 0.4756, disc_loss: 1.4349\n",
      "Time for epoch 8 is 25.9640 sec\n",
      "======================================\n",
      "Epoch 9, gen_loss: 0.4673, disc_loss: 1.4188\n",
      "Time for epoch 9 is 26.0919 sec\n",
      "======================================\n",
      "Epoch 10, gen_loss: 0.4730, disc_loss: 1.4159\n",
      "Time for epoch 10 is 25.3220 sec\n",
      "======================================\n",
      "Epoch 11, gen_loss: 0.4692, disc_loss: 1.4185\n",
      "Time for epoch 11 is 25.2707 sec\n",
      "======================================\n",
      "Epoch 12, gen_loss: 0.4745, disc_loss: 1.4034\n",
      "Time for epoch 12 is 25.0686 sec\n",
      "======================================\n",
      "Epoch 13, gen_loss: 0.4700, disc_loss: 1.4318\n",
      "Time for epoch 13 is 25.4873 sec\n",
      "======================================\n",
      "Epoch 14, gen_loss: 0.4715, disc_loss: 1.4215\n",
      "Time for epoch 14 is 25.3357 sec\n",
      "======================================\n",
      "Epoch 15, gen_loss: 0.4698, disc_loss: 1.4277\n",
      "Time for epoch 15 is 27.8072 sec\n",
      "======================================\n",
      "Epoch 16, gen_loss: 0.4708, disc_loss: 1.4065\n",
      "Time for epoch 16 is 25.0532 sec\n",
      "======================================\n",
      "Epoch 17, gen_loss: 0.4741, disc_loss: 1.4057\n",
      "Time for epoch 17 is 25.9321 sec\n",
      "======================================\n",
      "Epoch 18, gen_loss: 0.4735, disc_loss: 1.4048\n",
      "Time for epoch 18 is 25.0299 sec\n",
      "======================================\n",
      "Epoch 19, gen_loss: 0.4717, disc_loss: 1.3956\n",
      "Time for epoch 19 is 26.8216 sec\n",
      "======================================\n",
      "Epoch 20, gen_loss: 0.4749, disc_loss: 1.3988\n",
      "Time for epoch 20 is 26.0751 sec\n",
      "======================================\n",
      "Epoch 21, gen_loss: 0.4711, disc_loss: 1.4074\n",
      "Time for epoch 21 is 25.1652 sec\n",
      "======================================\n",
      "Epoch 22, gen_loss: 0.4734, disc_loss: 1.3982\n",
      "Time for epoch 22 is 25.2843 sec\n",
      "======================================\n",
      "Epoch 23, gen_loss: 0.4738, disc_loss: 1.3948\n",
      "Time for epoch 23 is 25.2479 sec\n",
      "======================================\n",
      "Epoch 24, gen_loss: 0.4731, disc_loss: 1.3926\n",
      "Time for epoch 24 is 24.7334 sec\n",
      "======================================\n",
      "Epoch 25, gen_loss: 0.4734, disc_loss: 1.3997\n",
      "Time for epoch 25 is 25.1284 sec\n",
      "======================================\n",
      "Epoch 26, gen_loss: 0.6564, disc_loss: 0.3140\n",
      "Time for epoch 26 is 24.8172 sec\n",
      "======================================\n",
      "Epoch 27, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 27 is 26.2169 sec\n",
      "======================================\n",
      "Epoch 28, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 28 is 25.2480 sec\n",
      "======================================\n",
      "Epoch 29, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 29 is 25.4504 sec\n",
      "======================================\n",
      "Epoch 30, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 30 is 25.0241 sec\n",
      "======================================\n",
      "Epoch 31, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 31 is 25.6334 sec\n",
      "======================================\n",
      "Epoch 32, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 32 is 25.6217 sec\n",
      "======================================\n",
      "Epoch 33, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 33 is 24.6343 sec\n",
      "======================================\n",
      "Epoch 34, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 34 is 25.3402 sec\n",
      "======================================\n",
      "Epoch 35, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 35 is 24.8602 sec\n",
      "======================================\n",
      "Epoch 36, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 36 is 25.4905 sec\n",
      "======================================\n",
      "Epoch 37, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 37 is 25.4022 sec\n",
      "======================================\n",
      "Epoch 38, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 38 is 25.4549 sec\n",
      "======================================\n",
      "Epoch 39, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 39 is 25.2983 sec\n",
      "======================================\n",
      "Epoch 40, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 40 is 24.9392 sec\n",
      "======================================\n",
      "Epoch 41, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 41 is 25.2038 sec\n",
      "======================================\n",
      "Epoch 42, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 42 is 24.9411 sec\n",
      "======================================\n",
      "Epoch 43, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 43 is 24.7675 sec\n",
      "======================================\n",
      "Epoch 44, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 44 is 25.5127 sec\n",
      "======================================\n",
      "Epoch 45, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 45 is 24.8632 sec\n",
      "======================================\n",
      "Epoch 46, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 46 is 25.8533 sec\n",
      "======================================\n",
      "Epoch 47, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 47 is 25.6468 sec\n",
      "======================================\n",
      "Epoch 48, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 48 is 24.8373 sec\n",
      "======================================\n",
      "Epoch 49, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 49 is 24.8401 sec\n",
      "======================================\n",
      "Epoch 50, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 50 is 25.1940 sec\n",
      "======================================\n",
      "Epoch 51, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 51 is 25.1530 sec\n",
      "======================================\n",
      "Epoch 52, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 52 is 25.3264 sec\n",
      "======================================\n",
      "Epoch 53, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 53 is 26.0587 sec\n",
      "======================================\n",
      "Epoch 54, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 54 is 34.8469 sec\n",
      "======================================\n",
      "Epoch 55, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 55 is 25.2026 sec\n",
      "======================================\n",
      "Epoch 56, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 56 is 25.0066 sec\n",
      "======================================\n",
      "Epoch 57, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 57 is 24.7326 sec\n",
      "======================================\n",
      "Epoch 58, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 58 is 24.8132 sec\n",
      "======================================\n",
      "Epoch 59, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 59 is 25.3487 sec\n",
      "======================================\n",
      "Epoch 60, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 60 is 25.5756 sec\n",
      "======================================\n",
      "Epoch 61, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 61 is 26.5517 sec\n",
      "======================================\n",
      "Epoch 62, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 62 is 24.8812 sec\n",
      "======================================\n",
      "Epoch 63, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 63 is 24.7732 sec\n",
      "======================================\n",
      "Epoch 64, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 64 is 25.2850 sec\n",
      "======================================\n",
      "Epoch 65, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 65 is 24.5827 sec\n",
      "======================================\n",
      "Epoch 66, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 66 is 25.7190 sec\n",
      "======================================\n",
      "Epoch 67, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 67 is 25.0269 sec\n",
      "======================================\n",
      "Epoch 68, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 68 is 25.4052 sec\n",
      "======================================\n",
      "Epoch 69, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 69 is 24.7731 sec\n",
      "======================================\n",
      "Epoch 70, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 70 is 25.7620 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 71 is 25.2902 sec\n",
      "======================================\n",
      "Epoch 72, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 72 is 24.5724 sec\n",
      "======================================\n",
      "Epoch 73, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 73 is 24.8404 sec\n",
      "======================================\n",
      "Epoch 74, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 74 is 24.8449 sec\n",
      "======================================\n",
      "Epoch 75, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 75 is 25.1368 sec\n",
      "======================================\n",
      "Epoch 76, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 76 is 25.7529 sec\n",
      "======================================\n",
      "Epoch 77, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 77 is 25.1122 sec\n",
      "======================================\n",
      "Epoch 78, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 78 is 24.9009 sec\n",
      "======================================\n",
      "Epoch 79, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 79 is 25.1153 sec\n",
      "======================================\n",
      "Epoch 80, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 80 is 24.5983 sec\n",
      "======================================\n",
      "Epoch 81, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 81 is 24.8122 sec\n",
      "======================================\n",
      "Epoch 82, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 82 is 25.7323 sec\n",
      "======================================\n",
      "Epoch 83, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 83 is 25.0919 sec\n",
      "======================================\n",
      "Epoch 84, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 84 is 25.2741 sec\n",
      "======================================\n",
      "Epoch 85, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 85 is 24.7649 sec\n",
      "======================================\n",
      "Epoch 86, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 86 is 25.6738 sec\n",
      "======================================\n",
      "Epoch 87, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 87 is 24.7192 sec\n",
      "======================================\n",
      "Epoch 88, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 88 is 24.8747 sec\n",
      "======================================\n",
      "Epoch 89, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 89 is 25.2767 sec\n",
      "======================================\n",
      "Epoch 90, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 90 is 24.6919 sec\n",
      "======================================\n",
      "Epoch 91, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 91 is 24.9441 sec\n",
      "======================================\n",
      "Epoch 92, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 92 is 25.7352 sec\n",
      "======================================\n",
      "Epoch 93, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 93 is 24.6674 sec\n",
      "======================================\n",
      "Epoch 94, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 94 is 25.7094 sec\n",
      "======================================\n",
      "Epoch 95, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 95 is 25.1230 sec\n",
      "======================================\n",
      "Epoch 96, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 96 is 24.9710 sec\n",
      "======================================\n",
      "Epoch 97, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 97 is 26.2834 sec\n",
      "======================================\n",
      "Epoch 98, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 98 is 25.4456 sec\n",
      "======================================\n",
      "Epoch 99, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 99 is 25.3598 sec\n",
      "======================================\n",
      "Epoch 100, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 100 is 25.3791 sec\n",
      "======================================\n",
      "Epoch 101, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 101 is 24.6803 sec\n",
      "======================================\n",
      "Epoch 102, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 102 is 25.2897 sec\n",
      "======================================\n",
      "Epoch 103, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 103 is 24.6254 sec\n",
      "======================================\n",
      "Epoch 104, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 104 is 25.2608 sec\n",
      "======================================\n",
      "Epoch 105, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 105 is 24.7950 sec\n",
      "======================================\n",
      "Epoch 106, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 106 is 25.2433 sec\n",
      "======================================\n",
      "Epoch 107, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 107 is 25.1034 sec\n",
      "======================================\n",
      "Epoch 108, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 108 is 24.6680 sec\n",
      "======================================\n",
      "Epoch 109, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 109 is 24.5087 sec\n",
      "======================================\n",
      "Epoch 110, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 110 is 25.6727 sec\n",
      "======================================\n",
      "Epoch 111, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 111 is 24.8280 sec\n",
      "======================================\n",
      "Epoch 112, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 112 is 25.8695 sec\n",
      "======================================\n",
      "Epoch 113, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 113 is 25.0643 sec\n",
      "======================================\n",
      "Epoch 114, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 114 is 24.4392 sec\n",
      "======================================\n",
      "Epoch 115, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 115 is 25.9602 sec\n",
      "======================================\n",
      "Epoch 116, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 116 is 25.0110 sec\n",
      "======================================\n",
      "Epoch 117, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 117 is 24.6696 sec\n",
      "======================================\n",
      "Epoch 118, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 118 is 24.5787 sec\n",
      "======================================\n",
      "Epoch 119, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 119 is 24.7610 sec\n",
      "======================================\n",
      "Epoch 120, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 120 is 25.2483 sec\n",
      "======================================\n",
      "Epoch 121, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 121 is 24.7926 sec\n",
      "======================================\n",
      "Epoch 122, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 122 is 24.7614 sec\n",
      "======================================\n",
      "Epoch 123, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 123 is 24.7666 sec\n",
      "======================================\n",
      "Epoch 124, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 124 is 25.6251 sec\n",
      "======================================\n",
      "Epoch 125, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 125 is 26.1965 sec\n",
      "======================================\n",
      "Epoch 126, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 126 is 24.5189 sec\n",
      "======================================\n",
      "Epoch 127, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 127 is 24.4968 sec\n",
      "======================================\n",
      "Epoch 128, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 128 is 25.4761 sec\n",
      "======================================\n",
      "Epoch 129, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 129 is 24.7660 sec\n",
      "======================================\n",
      "Epoch 130, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 130 is 25.4187 sec\n",
      "======================================\n",
      "Epoch 131, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 131 is 25.1311 sec\n",
      "======================================\n",
      "Epoch 132, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 132 is 24.7239 sec\n",
      "======================================\n",
      "Epoch 133, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 133 is 24.6142 sec\n",
      "======================================\n",
      "Epoch 134, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 134 is 24.5871 sec\n",
      "======================================\n",
      "Epoch 135, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 135 is 25.1440 sec\n",
      "======================================\n",
      "Epoch 136, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 136 is 24.7458 sec\n",
      "======================================\n",
      "Epoch 137, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 137 is 24.8312 sec\n",
      "======================================\n",
      "Epoch 138, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 138 is 24.6063 sec\n",
      "======================================\n",
      "Epoch 139, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 139 is 24.4894 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 140 is 25.2296 sec\n",
      "======================================\n",
      "Epoch 141, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 141 is 25.5632 sec\n",
      "======================================\n",
      "Epoch 142, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 142 is 25.8658 sec\n",
      "======================================\n",
      "Epoch 143, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 143 is 24.8975 sec\n",
      "======================================\n",
      "Epoch 144, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 144 is 25.0770 sec\n",
      "======================================\n",
      "Epoch 145, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 145 is 24.6022 sec\n",
      "======================================\n",
      "Epoch 146, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 146 is 25.0293 sec\n",
      "======================================\n",
      "Epoch 147, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 147 is 25.3141 sec\n",
      "======================================\n",
      "Epoch 148, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 148 is 25.1373 sec\n",
      "======================================\n",
      "Epoch 149, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 149 is 25.0335 sec\n",
      "======================================\n",
      "Epoch 150, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 150 is 24.7834 sec\n",
      "======================================\n",
      "Epoch 151, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 151 is 24.8829 sec\n",
      "======================================\n",
      "Epoch 152, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 152 is 24.7521 sec\n",
      "======================================\n",
      "Epoch 153, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 153 is 24.4202 sec\n",
      "======================================\n",
      "Epoch 154, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 154 is 24.7255 sec\n",
      "======================================\n",
      "Epoch 155, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 155 is 24.7349 sec\n",
      "======================================\n",
      "Epoch 156, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 156 is 25.3338 sec\n",
      "======================================\n",
      "Epoch 157, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 157 is 25.3880 sec\n",
      "======================================\n",
      "Epoch 158, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 158 is 25.1210 sec\n",
      "======================================\n",
      "Epoch 159, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 159 is 24.9490 sec\n",
      "======================================\n",
      "Epoch 160, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 160 is 24.8015 sec\n",
      "======================================\n",
      "Epoch 161, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 161 is 24.6900 sec\n",
      "======================================\n",
      "Epoch 162, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 162 is 25.4921 sec\n",
      "======================================\n",
      "Epoch 163, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 163 is 24.9944 sec\n",
      "======================================\n",
      "Epoch 164, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 164 is 24.8804 sec\n",
      "======================================\n",
      "Epoch 165, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 165 is 25.1331 sec\n",
      "======================================\n",
      "Epoch 166, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 166 is 24.8449 sec\n",
      "======================================\n",
      "Epoch 167, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 167 is 25.4342 sec\n",
      "======================================\n",
      "Epoch 168, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 168 is 24.9779 sec\n",
      "======================================\n",
      "Epoch 169, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 169 is 24.7709 sec\n",
      "======================================\n",
      "Epoch 170, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 170 is 25.3500 sec\n",
      "======================================\n",
      "Epoch 171, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 171 is 25.5334 sec\n",
      "======================================\n",
      "Epoch 172, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 172 is 24.4665 sec\n",
      "======================================\n",
      "Epoch 173, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 173 is 25.2458 sec\n",
      "======================================\n",
      "Epoch 174, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 174 is 24.5525 sec\n",
      "======================================\n",
      "Epoch 175, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 175 is 24.5854 sec\n",
      "======================================\n",
      "Epoch 176, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 176 is 25.6648 sec\n",
      "======================================\n",
      "Epoch 177, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 177 is 24.8769 sec\n",
      "======================================\n",
      "Epoch 178, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 178 is 25.4452 sec\n",
      "======================================\n",
      "Epoch 179, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 179 is 24.4871 sec\n",
      "======================================\n",
      "Epoch 180, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 180 is 25.3201 sec\n",
      "======================================\n",
      "Epoch 181, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 181 is 25.6373 sec\n",
      "======================================\n",
      "Epoch 182, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 182 is 25.5279 sec\n",
      "======================================\n",
      "Epoch 183, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 183 is 24.9746 sec\n",
      "======================================\n",
      "Epoch 184, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 184 is 24.8931 sec\n",
      "======================================\n",
      "Epoch 185, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 185 is 24.7498 sec\n",
      "======================================\n",
      "Epoch 186, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 186 is 25.5074 sec\n",
      "======================================\n",
      "Epoch 187, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 187 is 25.0328 sec\n",
      "======================================\n",
      "Epoch 188, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 188 is 24.7441 sec\n",
      "======================================\n",
      "Epoch 189, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 189 is 24.8189 sec\n",
      "======================================\n",
      "Epoch 190, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 190 is 24.7429 sec\n",
      "======================================\n",
      "Epoch 191, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 191 is 24.8209 sec\n",
      "======================================\n",
      "Epoch 192, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 192 is 25.6942 sec\n",
      "======================================\n",
      "Epoch 193, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 193 is 24.6301 sec\n",
      "======================================\n",
      "Epoch 194, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 194 is 25.0689 sec\n",
      "======================================\n",
      "Epoch 195, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 195 is 25.9590 sec\n",
      "======================================\n",
      "Epoch 196, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 196 is 25.0697 sec\n",
      "======================================\n",
      "Epoch 197, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 197 is 25.5497 sec\n",
      "======================================\n",
      "Epoch 198, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 198 is 25.5687 sec\n",
      "======================================\n",
      "Epoch 199, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 199 is 25.7324 sec\n",
      "======================================\n",
      "Epoch 200, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 200 is 25.2446 sec\n",
      "======================================\n",
      "Epoch 201, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 201 is 25.1617 sec\n",
      "======================================\n",
      "Epoch 202, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 202 is 24.6052 sec\n",
      "======================================\n",
      "Epoch 203, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 203 is 25.0200 sec\n",
      "======================================\n",
      "Epoch 204, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 204 is 25.1099 sec\n",
      "======================================\n",
      "Epoch 205, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 205 is 25.0468 sec\n",
      "======================================\n",
      "Epoch 206, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 206 is 25.1138 sec\n",
      "======================================\n",
      "Epoch 207, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 207 is 25.5783 sec\n",
      "======================================\n",
      "Epoch 208, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 208 is 24.7164 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 209 is 24.4928 sec\n",
      "======================================\n",
      "Epoch 210, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 210 is 24.6867 sec\n",
      "======================================\n",
      "Epoch 211, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 211 is 25.0904 sec\n",
      "======================================\n",
      "Epoch 212, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 212 is 25.1508 sec\n",
      "======================================\n",
      "Epoch 213, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 213 is 25.7769 sec\n",
      "======================================\n",
      "Epoch 214, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 214 is 25.4227 sec\n",
      "======================================\n",
      "Epoch 215, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 215 is 25.0832 sec\n",
      "======================================\n",
      "Epoch 216, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 216 is 25.3719 sec\n",
      "======================================\n",
      "Epoch 217, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 217 is 24.5401 sec\n",
      "======================================\n",
      "Epoch 218, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 218 is 25.2200 sec\n",
      "======================================\n",
      "Epoch 219, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 219 is 25.5985 sec\n",
      "======================================\n",
      "Epoch 220, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 220 is 24.7471 sec\n",
      "======================================\n",
      "Epoch 221, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 221 is 24.7770 sec\n",
      "======================================\n",
      "Epoch 222, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 222 is 25.3011 sec\n",
      "======================================\n",
      "Epoch 223, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 223 is 24.7078 sec\n",
      "======================================\n",
      "Epoch 224, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 224 is 25.3273 sec\n",
      "======================================\n",
      "Epoch 225, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 225 is 24.9720 sec\n",
      "======================================\n",
      "Epoch 226, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 226 is 25.2205 sec\n",
      "======================================\n",
      "Epoch 227, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 227 is 24.9518 sec\n",
      "======================================\n",
      "Epoch 228, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 228 is 25.6627 sec\n",
      "======================================\n",
      "Epoch 229, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 229 is 25.4579 sec\n",
      "======================================\n",
      "Epoch 230, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 230 is 24.7909 sec\n",
      "======================================\n",
      "Epoch 231, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 231 is 25.3832 sec\n",
      "======================================\n",
      "Epoch 232, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 232 is 24.4796 sec\n",
      "======================================\n",
      "Epoch 233, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 233 is 24.6091 sec\n",
      "======================================\n",
      "Epoch 234, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 234 is 24.4958 sec\n",
      "======================================\n",
      "Epoch 235, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 235 is 24.8565 sec\n",
      "======================================\n",
      "Epoch 236, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 236 is 25.0660 sec\n",
      "======================================\n",
      "Epoch 237, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 237 is 24.4660 sec\n",
      "======================================\n",
      "Epoch 238, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 238 is 25.1886 sec\n",
      "======================================\n",
      "Epoch 239, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 239 is 24.8307 sec\n",
      "======================================\n",
      "Epoch 240, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 240 is 25.8510 sec\n",
      "======================================\n",
      "Epoch 241, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 241 is 25.7068 sec\n",
      "======================================\n",
      "Epoch 242, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 242 is 24.7699 sec\n",
      "======================================\n",
      "Epoch 243, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 243 is 24.8315 sec\n",
      "======================================\n",
      "Epoch 244, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 244 is 24.3539 sec\n",
      "======================================\n",
      "Epoch 245, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 245 is 24.6476 sec\n",
      "======================================\n",
      "Epoch 246, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 246 is 25.5025 sec\n",
      "======================================\n",
      "Epoch 247, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 247 is 24.8739 sec\n",
      "======================================\n",
      "Epoch 248, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 248 is 25.3319 sec\n",
      "======================================\n",
      "Epoch 249, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 249 is 24.5790 sec\n",
      "======================================\n",
      "Epoch 250, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 250 is 24.9898 sec\n",
      "======================================\n",
      "Epoch 251, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 251 is 24.5688 sec\n",
      "======================================\n",
      "Epoch 252, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 252 is 24.4678 sec\n",
      "======================================\n",
      "Epoch 253, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 253 is 24.9558 sec\n",
      "======================================\n",
      "Epoch 254, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 254 is 26.2575 sec\n",
      "======================================\n",
      "Epoch 255, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 255 is 24.5156 sec\n",
      "======================================\n",
      "Epoch 256, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 256 is 25.0338 sec\n",
      "======================================\n",
      "Epoch 257, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 257 is 25.2328 sec\n",
      "======================================\n",
      "Epoch 258, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 258 is 24.5285 sec\n",
      "======================================\n",
      "Epoch 259, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 259 is 24.4986 sec\n",
      "======================================\n",
      "Epoch 260, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 260 is 24.7493 sec\n",
      "======================================\n",
      "Epoch 261, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 261 is 24.6902 sec\n",
      "======================================\n",
      "Epoch 262, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 262 is 25.2046 sec\n",
      "======================================\n",
      "Epoch 263, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 263 is 25.8620 sec\n",
      "======================================\n",
      "Epoch 264, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 264 is 25.4104 sec\n",
      "======================================\n",
      "Epoch 265, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 265 is 25.1348 sec\n",
      "======================================\n",
      "Epoch 266, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 266 is 24.6312 sec\n",
      "======================================\n",
      "Epoch 267, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 267 is 24.6403 sec\n",
      "======================================\n",
      "Epoch 268, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 268 is 25.1619 sec\n",
      "======================================\n",
      "Epoch 269, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 269 is 25.3020 sec\n",
      "======================================\n",
      "Epoch 270, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 270 is 25.6113 sec\n",
      "======================================\n",
      "Epoch 271, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 271 is 24.5692 sec\n",
      "======================================\n",
      "Epoch 272, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 272 is 24.8294 sec\n",
      "======================================\n",
      "Epoch 273, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 273 is 24.7239 sec\n",
      "======================================\n",
      "Epoch 274, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 274 is 24.5759 sec\n",
      "======================================\n",
      "Epoch 275, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 275 is 25.2576 sec\n",
      "======================================\n",
      "Epoch 276, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 276 is 24.7500 sec\n",
      "======================================\n",
      "Epoch 277, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 277 is 25.1588 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 278 is 24.8829 sec\n",
      "======================================\n",
      "Epoch 279, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 279 is 24.6671 sec\n",
      "======================================\n",
      "Epoch 280, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 280 is 25.5843 sec\n",
      "======================================\n",
      "Epoch 281, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 281 is 24.9300 sec\n",
      "======================================\n",
      "Epoch 282, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 282 is 24.9442 sec\n",
      "======================================\n",
      "Epoch 283, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 283 is 25.4953 sec\n",
      "======================================\n",
      "Epoch 284, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 284 is 24.6973 sec\n",
      "======================================\n",
      "Epoch 285, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 285 is 25.3738 sec\n",
      "======================================\n",
      "Epoch 286, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 286 is 25.2020 sec\n",
      "======================================\n",
      "Epoch 287, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 287 is 24.8612 sec\n",
      "======================================\n",
      "Epoch 288, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 288 is 24.8689 sec\n",
      "======================================\n",
      "Epoch 289, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 289 is 24.7579 sec\n",
      "======================================\n",
      "Epoch 290, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 290 is 25.1090 sec\n",
      "======================================\n",
      "Epoch 291, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 291 is 24.5478 sec\n",
      "======================================\n",
      "Epoch 292, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 292 is 24.5833 sec\n",
      "======================================\n",
      "Epoch 293, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 293 is 25.2040 sec\n",
      "======================================\n",
      "Epoch 294, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 294 is 25.1402 sec\n",
      "======================================\n",
      "Epoch 295, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 295 is 25.3028 sec\n",
      "======================================\n",
      "Epoch 296, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 296 is 24.6447 sec\n",
      "======================================\n",
      "Epoch 297, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 297 is 25.3487 sec\n",
      "======================================\n",
      "Epoch 298, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 298 is 25.2170 sec\n",
      "======================================\n",
      "Epoch 299, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 299 is 26.0340 sec\n",
      "======================================\n",
      "Epoch 300, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 300 is 25.6060 sec\n",
      "======================================\n",
      "Epoch 301, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 301 is 24.7997 sec\n",
      "======================================\n",
      "Epoch 302, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 302 is 25.6136 sec\n",
      "======================================\n",
      "Epoch 303, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 303 is 24.5323 sec\n",
      "======================================\n",
      "Epoch 304, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 304 is 25.3548 sec\n",
      "======================================\n",
      "Epoch 305, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 305 is 24.3585 sec\n",
      "======================================\n",
      "Epoch 306, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 306 is 24.6532 sec\n",
      "======================================\n",
      "Epoch 307, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 307 is 24.6880 sec\n",
      "======================================\n",
      "Epoch 308, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 308 is 24.5390 sec\n",
      "======================================\n",
      "Epoch 309, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 309 is 24.6156 sec\n",
      "======================================\n",
      "Epoch 310, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 310 is 24.7733 sec\n",
      "======================================\n",
      "Epoch 311, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 311 is 24.6503 sec\n",
      "======================================\n",
      "Epoch 312, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 312 is 25.2200 sec\n",
      "======================================\n",
      "Epoch 313, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 313 is 25.4121 sec\n",
      "======================================\n",
      "Epoch 314, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 314 is 24.3515 sec\n",
      "======================================\n",
      "Epoch 315, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 315 is 25.3845 sec\n",
      "======================================\n",
      "Epoch 316, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 316 is 25.1912 sec\n",
      "======================================\n",
      "Epoch 317, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 317 is 24.5142 sec\n",
      "======================================\n",
      "Epoch 318, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 318 is 24.5047 sec\n",
      "======================================\n",
      "Epoch 319, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 319 is 25.4145 sec\n",
      "======================================\n",
      "Epoch 320, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 320 is 24.5339 sec\n",
      "======================================\n",
      "Epoch 321, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 321 is 24.5149 sec\n",
      "======================================\n",
      "Epoch 322, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 322 is 24.8322 sec\n",
      "======================================\n",
      "Epoch 323, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 323 is 25.4940 sec\n",
      "======================================\n",
      "Epoch 324, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 324 is 25.0795 sec\n",
      "======================================\n",
      "Epoch 325, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 325 is 24.6245 sec\n",
      "======================================\n",
      "Epoch 326, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 326 is 24.9002 sec\n",
      "======================================\n",
      "Epoch 327, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 327 is 24.5261 sec\n",
      "======================================\n",
      "Epoch 328, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 328 is 24.8277 sec\n",
      "======================================\n",
      "Epoch 329, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 329 is 25.0531 sec\n",
      "======================================\n",
      "Epoch 330, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 330 is 25.8008 sec\n",
      "======================================\n",
      "Epoch 331, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 331 is 24.5112 sec\n",
      "======================================\n",
      "Epoch 332, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 332 is 25.3252 sec\n",
      "======================================\n",
      "Epoch 333, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 333 is 24.3832 sec\n",
      "======================================\n",
      "Epoch 334, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 334 is 25.0577 sec\n",
      "======================================\n",
      "Epoch 335, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 335 is 24.6224 sec\n",
      "======================================\n",
      "Epoch 336, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 336 is 24.9470 sec\n",
      "======================================\n",
      "Epoch 337, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 337 is 24.7114 sec\n",
      "======================================\n",
      "Epoch 338, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 338 is 24.6318 sec\n",
      "======================================\n",
      "Epoch 339, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 339 is 25.2171 sec\n",
      "======================================\n",
      "Epoch 340, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 340 is 25.4409 sec\n",
      "======================================\n",
      "Epoch 341, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 341 is 24.5190 sec\n",
      "======================================\n",
      "Epoch 342, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 342 is 25.2770 sec\n",
      "======================================\n",
      "Epoch 343, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 343 is 24.6854 sec\n",
      "======================================\n",
      "Epoch 344, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 344 is 24.9436 sec\n",
      "======================================\n",
      "Epoch 345, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 345 is 24.4642 sec\n",
      "======================================\n",
      "Epoch 346, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 346 is 24.6321 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 347, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 347 is 24.8023 sec\n",
      "======================================\n",
      "Epoch 348, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 348 is 24.6599 sec\n",
      "======================================\n",
      "Epoch 349, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 349 is 25.3430 sec\n",
      "======================================\n",
      "Epoch 350, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 350 is 25.2267 sec\n",
      "======================================\n",
      "Epoch 351, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 351 is 24.8108 sec\n",
      "======================================\n",
      "Epoch 352, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 352 is 25.4222 sec\n",
      "======================================\n",
      "Epoch 353, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 353 is 24.7967 sec\n",
      "======================================\n",
      "Epoch 354, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 354 is 25.0349 sec\n",
      "======================================\n",
      "Epoch 355, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 355 is 24.8340 sec\n",
      "======================================\n",
      "Epoch 356, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 356 is 24.3762 sec\n",
      "======================================\n",
      "Epoch 357, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 357 is 25.3111 sec\n",
      "======================================\n",
      "Epoch 358, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 358 is 26.0092 sec\n",
      "======================================\n",
      "Epoch 359, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 359 is 24.7452 sec\n",
      "======================================\n",
      "Epoch 360, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 360 is 24.7777 sec\n",
      "======================================\n",
      "Epoch 361, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 361 is 24.3721 sec\n",
      "======================================\n",
      "Epoch 362, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 362 is 24.9299 sec\n",
      "======================================\n",
      "Epoch 363, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 363 is 25.2771 sec\n",
      "======================================\n",
      "Epoch 364, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 364 is 25.3993 sec\n",
      "======================================\n",
      "Epoch 365, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 365 is 24.4882 sec\n",
      "======================================\n",
      "Epoch 366, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 366 is 24.5991 sec\n",
      "======================================\n",
      "Epoch 367, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 367 is 24.9808 sec\n",
      "======================================\n",
      "Epoch 368, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 368 is 24.2239 sec\n",
      "======================================\n",
      "Epoch 369, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 369 is 25.2300 sec\n",
      "======================================\n",
      "Epoch 370, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 370 is 25.1339 sec\n",
      "======================================\n",
      "Epoch 371, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 371 is 25.4819 sec\n",
      "======================================\n",
      "Epoch 372, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 372 is 25.2811 sec\n",
      "======================================\n",
      "Epoch 373, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 373 is 25.8334 sec\n",
      "======================================\n",
      "Epoch 374, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 374 is 25.6222 sec\n",
      "======================================\n",
      "Epoch 375, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 375 is 24.6118 sec\n",
      "======================================\n",
      "Epoch 376, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 376 is 25.1308 sec\n",
      "======================================\n",
      "Epoch 377, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 377 is 24.5619 sec\n",
      "======================================\n",
      "Epoch 378, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 378 is 24.7951 sec\n",
      "======================================\n",
      "Epoch 379, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 379 is 24.5435 sec\n",
      "======================================\n",
      "Epoch 380, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 380 is 24.7899 sec\n",
      "======================================\n",
      "Epoch 381, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 381 is 24.6593 sec\n",
      "======================================\n",
      "Epoch 382, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 382 is 24.5900 sec\n",
      "======================================\n",
      "Epoch 383, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 383 is 25.1282 sec\n",
      "======================================\n",
      "Epoch 384, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 384 is 25.8881 sec\n",
      "======================================\n",
      "Epoch 385, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 385 is 25.1608 sec\n",
      "======================================\n",
      "Epoch 386, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 386 is 25.0641 sec\n",
      "======================================\n",
      "Epoch 387, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 387 is 25.0661 sec\n",
      "======================================\n",
      "Epoch 388, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 388 is 25.7435 sec\n",
      "======================================\n",
      "Epoch 389, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 389 is 25.2009 sec\n",
      "======================================\n",
      "Epoch 390, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 390 is 25.3941 sec\n",
      "======================================\n",
      "Epoch 391, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 391 is 25.2021 sec\n",
      "======================================\n",
      "Epoch 392, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 392 is 25.0098 sec\n",
      "======================================\n",
      "Epoch 393, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 393 is 25.4995 sec\n",
      "======================================\n",
      "Epoch 394, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 394 is 25.1990 sec\n",
      "======================================\n",
      "Epoch 395, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 395 is 25.0726 sec\n",
      "======================================\n",
      "Epoch 396, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 396 is 25.6533 sec\n",
      "======================================\n",
      "Epoch 397, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 397 is 25.2398 sec\n",
      "======================================\n",
      "Epoch 398, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 398 is 24.8761 sec\n",
      "======================================\n",
      "Epoch 399, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 399 is 24.9519 sec\n",
      "======================================\n",
      "Epoch 400, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 400 is 24.7530 sec\n",
      "======================================\n",
      "Epoch 401, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 401 is 24.9914 sec\n",
      "======================================\n",
      "Epoch 402, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 402 is 24.6349 sec\n",
      "======================================\n",
      "Epoch 403, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 403 is 25.4667 sec\n",
      "======================================\n",
      "Epoch 404, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 404 is 24.9867 sec\n",
      "======================================\n",
      "Epoch 405, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 405 is 25.5949 sec\n",
      "======================================\n",
      "Epoch 406, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 406 is 25.7055 sec\n",
      "======================================\n",
      "Epoch 407, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 407 is 24.9851 sec\n",
      "======================================\n",
      "Epoch 408, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 408 is 24.6219 sec\n",
      "======================================\n",
      "Epoch 409, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 409 is 24.5411 sec\n",
      "======================================\n",
      "Epoch 410, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 410 is 24.9696 sec\n",
      "======================================\n",
      "Epoch 411, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 411 is 25.0687 sec\n",
      "======================================\n",
      "Epoch 412, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 412 is 24.7679 sec\n",
      "======================================\n",
      "Epoch 413, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 413 is 25.4658 sec\n",
      "======================================\n",
      "Epoch 414, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 414 is 24.6227 sec\n",
      "======================================\n",
      "Epoch 415, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 415 is 25.3095 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 416 is 24.5186 sec\n",
      "======================================\n",
      "Epoch 417, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 417 is 24.7183 sec\n",
      "======================================\n",
      "Epoch 418, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 418 is 24.9083 sec\n",
      "======================================\n",
      "Epoch 419, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 419 is 24.5504 sec\n",
      "======================================\n",
      "Epoch 420, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 420 is 25.3769 sec\n",
      "======================================\n",
      "Epoch 421, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 421 is 24.8516 sec\n",
      "======================================\n",
      "Epoch 422, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 422 is 25.0270 sec\n",
      "======================================\n",
      "Epoch 423, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 423 is 24.9721 sec\n",
      "======================================\n",
      "Epoch 424, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 424 is 24.8761 sec\n",
      "======================================\n",
      "Epoch 425, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 425 is 25.2567 sec\n",
      "======================================\n",
      "Epoch 426, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 426 is 25.7530 sec\n",
      "======================================\n",
      "Epoch 427, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 427 is 24.9717 sec\n",
      "======================================\n",
      "Epoch 428, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 428 is 24.5108 sec\n",
      "======================================\n",
      "Epoch 429, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 429 is 24.6337 sec\n",
      "======================================\n",
      "Epoch 430, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 430 is 24.7883 sec\n",
      "======================================\n",
      "Epoch 431, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 431 is 25.4503 sec\n",
      "======================================\n",
      "Epoch 432, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 432 is 25.0649 sec\n",
      "======================================\n",
      "Epoch 433, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 433 is 24.7459 sec\n",
      "======================================\n",
      "Epoch 434, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 434 is 25.6357 sec\n",
      "======================================\n",
      "Epoch 435, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 435 is 24.7250 sec\n",
      "======================================\n",
      "Epoch 436, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 436 is 25.6038 sec\n",
      "======================================\n",
      "Epoch 437, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 437 is 24.7180 sec\n",
      "======================================\n",
      "Epoch 438, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 438 is 25.2221 sec\n",
      "======================================\n",
      "Epoch 439, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 439 is 24.5740 sec\n",
      "======================================\n",
      "Epoch 440, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 440 is 25.0651 sec\n",
      "======================================\n",
      "Epoch 441, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 441 is 24.6550 sec\n",
      "======================================\n",
      "Epoch 442, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 442 is 24.4570 sec\n",
      "======================================\n",
      "Epoch 443, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 443 is 24.5087 sec\n",
      "======================================\n",
      "Epoch 444, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 444 is 24.8572 sec\n",
      "======================================\n",
      "Epoch 445, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 445 is 24.6795 sec\n",
      "======================================\n",
      "Epoch 446, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 446 is 24.4759 sec\n",
      "======================================\n",
      "Epoch 447, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 447 is 25.0603 sec\n",
      "======================================\n",
      "Epoch 448, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 448 is 24.7380 sec\n",
      "======================================\n",
      "Epoch 449, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 449 is 24.5641 sec\n",
      "======================================\n",
      "Epoch 450, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 450 is 24.5104 sec\n",
      "======================================\n",
      "Epoch 451, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 451 is 24.7749 sec\n",
      "======================================\n",
      "Epoch 452, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 452 is 24.9197 sec\n",
      "======================================\n",
      "Epoch 453, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 453 is 25.4761 sec\n",
      "======================================\n",
      "Epoch 454, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 454 is 25.2559 sec\n",
      "======================================\n",
      "Epoch 455, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 455 is 25.5269 sec\n",
      "======================================\n",
      "Epoch 456, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 456 is 24.5155 sec\n",
      "======================================\n",
      "Epoch 457, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 457 is 25.0800 sec\n",
      "======================================\n",
      "Epoch 458, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 458 is 25.5701 sec\n",
      "======================================\n",
      "Epoch 459, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 459 is 24.6633 sec\n",
      "======================================\n",
      "Epoch 460, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 460 is 24.5429 sec\n",
      "======================================\n",
      "Epoch 461, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 461 is 24.6569 sec\n",
      "======================================\n",
      "Epoch 462, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 462 is 24.3932 sec\n",
      "======================================\n",
      "Epoch 463, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 463 is 25.5732 sec\n",
      "======================================\n",
      "Epoch 464, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 464 is 24.9593 sec\n",
      "======================================\n",
      "Epoch 465, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 465 is 33.7950 sec\n",
      "======================================\n",
      "Epoch 466, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 466 is 24.5339 sec\n",
      "======================================\n",
      "Epoch 467, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 467 is 24.8074 sec\n",
      "======================================\n",
      "Epoch 468, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 468 is 24.6771 sec\n",
      "======================================\n",
      "Epoch 469, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 469 is 25.0197 sec\n",
      "======================================\n",
      "Epoch 470, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 470 is 24.8893 sec\n",
      "======================================\n",
      "Epoch 471, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 471 is 25.1450 sec\n",
      "======================================\n",
      "Epoch 472, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 472 is 24.7665 sec\n",
      "======================================\n",
      "Epoch 473, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 473 is 25.5176 sec\n",
      "======================================\n",
      "Epoch 474, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 474 is 24.3789 sec\n",
      "======================================\n",
      "Epoch 475, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 475 is 24.6907 sec\n",
      "======================================\n",
      "Epoch 476, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 476 is 24.4442 sec\n",
      "======================================\n",
      "Epoch 477, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 477 is 25.6558 sec\n",
      "======================================\n",
      "Epoch 478, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 478 is 25.1241 sec\n",
      "======================================\n",
      "Epoch 479, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 479 is 24.3672 sec\n",
      "======================================\n",
      "Epoch 480, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 480 is 25.0450 sec\n",
      "======================================\n",
      "Epoch 481, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 481 is 24.8073 sec\n",
      "======================================\n",
      "Epoch 482, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 482 is 24.9501 sec\n",
      "======================================\n",
      "Epoch 483, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 483 is 25.2945 sec\n",
      "======================================\n",
      "Epoch 484, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 484 is 25.1797 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 485 is 25.0354 sec\n",
      "======================================\n",
      "Epoch 486, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 486 is 24.7497 sec\n",
      "======================================\n",
      "Epoch 487, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 487 is 24.4038 sec\n",
      "======================================\n",
      "Epoch 488, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 488 is 24.5250 sec\n",
      "======================================\n",
      "Epoch 489, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 489 is 25.6546 sec\n",
      "======================================\n",
      "Epoch 490, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 490 is 24.8781 sec\n",
      "======================================\n",
      "Epoch 491, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 491 is 24.8888 sec\n",
      "======================================\n",
      "Epoch 492, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 492 is 24.6202 sec\n",
      "======================================\n",
      "Epoch 493, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 493 is 24.7230 sec\n",
      "======================================\n",
      "Epoch 494, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 494 is 24.7520 sec\n",
      "======================================\n",
      "Epoch 495, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 495 is 24.5731 sec\n",
      "======================================\n",
      "Epoch 496, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 496 is 24.7539 sec\n",
      "======================================\n",
      "Epoch 497, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 497 is 24.9307 sec\n",
      "======================================\n",
      "Epoch 498, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 498 is 25.4814 sec\n",
      "======================================\n",
      "Epoch 499, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 499 is 25.2191 sec\n",
      "======================================\n",
      "Epoch 500, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 500 is 24.3846 sec\n",
      "======================================\n",
      "Epoch 501, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 501 is 24.7797 sec\n",
      "======================================\n",
      "Epoch 502, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 502 is 24.6474 sec\n",
      "======================================\n",
      "Epoch 503, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 503 is 24.4339 sec\n",
      "======================================\n",
      "Epoch 504, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 504 is 24.9559 sec\n",
      "======================================\n",
      "Epoch 505, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 505 is 24.8662 sec\n",
      "======================================\n",
      "Epoch 506, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 506 is 24.4271 sec\n",
      "======================================\n",
      "Epoch 507, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 507 is 24.4688 sec\n",
      "======================================\n",
      "Epoch 508, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 508 is 24.6723 sec\n",
      "======================================\n",
      "Epoch 509, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 509 is 24.4806 sec\n",
      "======================================\n",
      "Epoch 510, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 510 is 25.3323 sec\n",
      "======================================\n",
      "Epoch 511, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 511 is 24.5092 sec\n",
      "======================================\n",
      "Epoch 512, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 512 is 24.5641 sec\n",
      "======================================\n",
      "Epoch 513, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 513 is 24.9056 sec\n",
      "======================================\n",
      "Epoch 514, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 514 is 25.5552 sec\n",
      "======================================\n",
      "Epoch 515, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 515 is 24.8756 sec\n",
      "======================================\n",
      "Epoch 516, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 516 is 24.7189 sec\n",
      "======================================\n",
      "Epoch 517, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 517 is 24.9369 sec\n",
      "======================================\n",
      "Epoch 518, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 518 is 24.4950 sec\n",
      "======================================\n",
      "Epoch 519, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 519 is 25.0691 sec\n",
      "======================================\n",
      "Epoch 520, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 520 is 24.9388 sec\n",
      "======================================\n",
      "Epoch 521, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 521 is 24.7659 sec\n",
      "======================================\n",
      "Epoch 522, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 522 is 25.4841 sec\n",
      "======================================\n",
      "Epoch 523, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 523 is 24.5499 sec\n",
      "======================================\n",
      "Epoch 524, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 524 is 25.2593 sec\n",
      "======================================\n",
      "Epoch 525, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 525 is 25.3345 sec\n",
      "======================================\n",
      "Epoch 526, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 526 is 24.9334 sec\n",
      "======================================\n",
      "Epoch 527, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 527 is 25.0762 sec\n",
      "======================================\n",
      "Epoch 528, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 528 is 24.6638 sec\n",
      "======================================\n",
      "Epoch 529, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 529 is 24.7112 sec\n",
      "======================================\n",
      "Epoch 530, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 530 is 24.6543 sec\n",
      "======================================\n",
      "Epoch 531, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 531 is 24.5007 sec\n",
      "======================================\n",
      "Epoch 532, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 532 is 25.6813 sec\n",
      "======================================\n",
      "Epoch 533, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 533 is 24.5590 sec\n",
      "======================================\n",
      "Epoch 534, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 534 is 24.9160 sec\n",
      "======================================\n",
      "Epoch 535, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 535 is 24.6405 sec\n",
      "======================================\n",
      "Epoch 536, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 536 is 24.9410 sec\n",
      "======================================\n",
      "Epoch 537, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 537 is 24.5941 sec\n",
      "======================================\n",
      "Epoch 538, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 538 is 25.2913 sec\n",
      "======================================\n",
      "Epoch 539, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 539 is 24.5273 sec\n",
      "======================================\n",
      "Epoch 540, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 540 is 25.1739 sec\n",
      "======================================\n",
      "Epoch 541, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 541 is 25.5476 sec\n",
      "======================================\n",
      "Epoch 542, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 542 is 25.8031 sec\n",
      "======================================\n",
      "Epoch 543, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 543 is 25.4880 sec\n",
      "======================================\n",
      "Epoch 544, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 544 is 24.5779 sec\n",
      "======================================\n",
      "Epoch 545, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 545 is 24.8554 sec\n",
      "======================================\n",
      "Epoch 546, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 546 is 24.5773 sec\n",
      "======================================\n",
      "Epoch 547, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 547 is 24.7724 sec\n",
      "======================================\n",
      "Epoch 548, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 548 is 25.1649 sec\n",
      "======================================\n",
      "Epoch 549, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 549 is 24.7650 sec\n",
      "======================================\n",
      "Epoch 550, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 550 is 24.5588 sec\n",
      "======================================\n",
      "Epoch 551, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 551 is 24.5676 sec\n",
      "======================================\n",
      "Epoch 552, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 552 is 25.2621 sec\n",
      "======================================\n",
      "Epoch 553, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 553 is 25.0772 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 554, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 554 is 25.3071 sec\n",
      "======================================\n",
      "Epoch 555, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 555 is 25.1931 sec\n",
      "======================================\n",
      "Epoch 556, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 556 is 25.5291 sec\n",
      "======================================\n",
      "Epoch 557, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 557 is 25.3081 sec\n",
      "======================================\n",
      "Epoch 558, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 558 is 24.9676 sec\n",
      "======================================\n",
      "Epoch 559, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 559 is 24.5102 sec\n",
      "======================================\n",
      "Epoch 560, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 560 is 25.3340 sec\n",
      "======================================\n",
      "Epoch 561, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 561 is 25.0071 sec\n",
      "======================================\n",
      "Epoch 562, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 562 is 24.7358 sec\n",
      "======================================\n",
      "Epoch 563, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 563 is 24.4354 sec\n",
      "======================================\n",
      "Epoch 564, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 564 is 25.4522 sec\n",
      "======================================\n",
      "Epoch 565, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 565 is 24.5869 sec\n",
      "======================================\n",
      "Epoch 566, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 566 is 25.1351 sec\n",
      "======================================\n",
      "Epoch 567, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 567 is 24.4231 sec\n",
      "======================================\n",
      "Epoch 568, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 568 is 24.8472 sec\n",
      "======================================\n",
      "Epoch 569, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 569 is 24.7967 sec\n",
      "======================================\n",
      "Epoch 570, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 570 is 24.3975 sec\n",
      "======================================\n",
      "Epoch 571, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 571 is 25.3952 sec\n",
      "======================================\n",
      "Epoch 572, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 572 is 24.6029 sec\n",
      "======================================\n",
      "Epoch 573, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 573 is 24.7450 sec\n",
      "======================================\n",
      "Epoch 574, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 574 is 24.9021 sec\n",
      "======================================\n",
      "Epoch 575, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 575 is 24.4265 sec\n",
      "======================================\n",
      "Epoch 576, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 576 is 25.0024 sec\n",
      "======================================\n",
      "Epoch 577, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 577 is 25.4449 sec\n",
      "======================================\n",
      "Epoch 578, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 578 is 25.1177 sec\n",
      "======================================\n",
      "Epoch 579, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 579 is 25.3062 sec\n",
      "======================================\n",
      "Epoch 580, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 580 is 24.6228 sec\n",
      "======================================\n",
      "Epoch 581, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 581 is 24.4879 sec\n",
      "======================================\n",
      "Epoch 582, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 582 is 25.1552 sec\n",
      "======================================\n",
      "Epoch 583, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 583 is 25.3875 sec\n",
      "======================================\n",
      "Epoch 584, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 584 is 25.4728 sec\n",
      "======================================\n",
      "Epoch 585, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 585 is 25.0518 sec\n",
      "======================================\n",
      "Epoch 586, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 586 is 24.6803 sec\n",
      "======================================\n",
      "Epoch 587, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 587 is 25.2659 sec\n",
      "======================================\n",
      "Epoch 588, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 588 is 25.4400 sec\n",
      "======================================\n",
      "Epoch 589, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 589 is 24.6310 sec\n",
      "======================================\n",
      "Epoch 590, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 590 is 24.4777 sec\n",
      "======================================\n",
      "Epoch 591, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 591 is 25.8140 sec\n",
      "======================================\n",
      "Epoch 592, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 592 is 24.5721 sec\n",
      "======================================\n",
      "Epoch 593, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 593 is 24.9061 sec\n",
      "======================================\n",
      "Epoch 594, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 594 is 25.1743 sec\n",
      "======================================\n",
      "Epoch 595, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 595 is 25.1434 sec\n",
      "======================================\n",
      "Epoch 596, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 596 is 24.7070 sec\n",
      "======================================\n",
      "Epoch 597, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 597 is 25.1361 sec\n",
      "======================================\n",
      "Epoch 598, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 598 is 24.4944 sec\n",
      "======================================\n",
      "Epoch 599, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 599 is 24.7900 sec\n",
      "======================================\n",
      "Epoch 600, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 600 is 25.1700 sec\n",
      "======================================\n",
      "Epoch 601, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 601 is 24.5511 sec\n",
      "======================================\n",
      "Epoch 602, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 602 is 24.3592 sec\n",
      "======================================\n",
      "Epoch 603, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 603 is 25.0301 sec\n",
      "======================================\n",
      "Epoch 604, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 604 is 25.0918 sec\n",
      "======================================\n",
      "Epoch 605, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 605 is 24.6662 sec\n",
      "======================================\n",
      "Epoch 606, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 606 is 24.4089 sec\n",
      "======================================\n",
      "Epoch 607, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 607 is 25.2137 sec\n",
      "======================================\n",
      "Epoch 608, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 608 is 24.4340 sec\n",
      "======================================\n",
      "Epoch 609, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 609 is 24.3793 sec\n",
      "======================================\n",
      "Epoch 610, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 610 is 25.4903 sec\n",
      "======================================\n",
      "Epoch 611, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 611 is 24.4408 sec\n",
      "======================================\n",
      "Epoch 612, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 612 is 24.4477 sec\n",
      "======================================\n",
      "Epoch 613, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 613 is 25.4328 sec\n",
      "======================================\n",
      "Epoch 614, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 614 is 24.6077 sec\n",
      "======================================\n",
      "Epoch 615, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 615 is 24.7850 sec\n",
      "======================================\n",
      "Epoch 616, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 616 is 25.1696 sec\n",
      "======================================\n",
      "Epoch 617, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 617 is 25.0889 sec\n",
      "======================================\n",
      "Epoch 618, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 618 is 24.6020 sec\n",
      "======================================\n",
      "Epoch 619, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 619 is 24.7027 sec\n",
      "======================================\n",
      "Epoch 620, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 620 is 25.4501 sec\n",
      "======================================\n",
      "Epoch 621, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 621 is 25.2517 sec\n",
      "======================================\n",
      "Epoch 622, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 622 is 24.9010 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 623, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 623 is 24.5730 sec\n",
      "======================================\n",
      "Epoch 624, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 624 is 25.2028 sec\n",
      "======================================\n",
      "Epoch 625, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 625 is 24.6240 sec\n",
      "======================================\n",
      "Epoch 626, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 626 is 24.6118 sec\n",
      "======================================\n",
      "Epoch 627, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 627 is 25.1500 sec\n",
      "======================================\n",
      "Epoch 628, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 628 is 24.2388 sec\n",
      "======================================\n",
      "Epoch 629, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 629 is 24.5730 sec\n",
      "======================================\n",
      "Epoch 630, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 630 is 24.6568 sec\n",
      "======================================\n",
      "Epoch 631, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 631 is 25.0801 sec\n",
      "======================================\n",
      "Epoch 632, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 632 is 24.4569 sec\n",
      "======================================\n",
      "Epoch 633, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 633 is 25.5512 sec\n",
      "======================================\n",
      "Epoch 634, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 634 is 24.6063 sec\n",
      "======================================\n",
      "Epoch 635, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 635 is 24.3640 sec\n",
      "======================================\n",
      "Epoch 636, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 636 is 24.8497 sec\n",
      "======================================\n",
      "Epoch 637, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 637 is 24.8598 sec\n",
      "======================================\n",
      "Epoch 638, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 638 is 24.5899 sec\n",
      "======================================\n",
      "Epoch 639, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 639 is 24.8645 sec\n",
      "======================================\n",
      "Epoch 640, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 640 is 25.5000 sec\n",
      "======================================\n",
      "Epoch 641, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 641 is 25.6418 sec\n",
      "======================================\n",
      "Epoch 642, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 642 is 24.5597 sec\n",
      "======================================\n",
      "Epoch 643, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 643 is 24.3288 sec\n",
      "======================================\n",
      "Epoch 644, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 644 is 25.3449 sec\n",
      "======================================\n",
      "Epoch 645, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 645 is 24.8464 sec\n",
      "======================================\n",
      "Epoch 646, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 646 is 24.5790 sec\n",
      "======================================\n",
      "Epoch 647, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 647 is 25.1197 sec\n",
      "======================================\n",
      "Epoch 648, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 648 is 24.5693 sec\n",
      "======================================\n",
      "Epoch 649, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 649 is 24.7468 sec\n",
      "======================================\n",
      "Epoch 650, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 650 is 25.3231 sec\n",
      "======================================\n",
      "Epoch 651, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 651 is 24.4220 sec\n",
      "======================================\n",
      "Epoch 652, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 652 is 25.3511 sec\n",
      "======================================\n",
      "Epoch 653, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 653 is 25.6799 sec\n",
      "======================================\n",
      "Epoch 654, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 654 is 24.5775 sec\n",
      "======================================\n",
      "Epoch 655, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 655 is 24.9180 sec\n",
      "======================================\n",
      "Epoch 656, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 656 is 24.6645 sec\n",
      "======================================\n",
      "Epoch 657, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 657 is 24.6524 sec\n",
      "======================================\n",
      "Epoch 658, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 658 is 24.6529 sec\n",
      "======================================\n",
      "Epoch 659, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 659 is 24.9848 sec\n",
      "======================================\n",
      "Epoch 660, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 660 is 25.1397 sec\n",
      "======================================\n",
      "Epoch 661, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 661 is 25.7018 sec\n",
      "======================================\n",
      "Epoch 662, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 662 is 25.1660 sec\n",
      "======================================\n",
      "Epoch 663, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 663 is 25.1670 sec\n",
      "======================================\n",
      "Epoch 664, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 664 is 26.0524 sec\n",
      "======================================\n",
      "Epoch 665, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 665 is 24.9270 sec\n",
      "======================================\n",
      "Epoch 666, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 666 is 24.4510 sec\n",
      "======================================\n",
      "Epoch 667, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 667 is 24.4960 sec\n",
      "======================================\n",
      "Epoch 668, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 668 is 25.9476 sec\n",
      "======================================\n",
      "Epoch 669, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 669 is 24.5121 sec\n",
      "======================================\n",
      "Epoch 670, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 670 is 24.6548 sec\n",
      "======================================\n",
      "Epoch 671, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 671 is 24.5974 sec\n",
      "======================================\n",
      "Epoch 672, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 672 is 24.6553 sec\n",
      "======================================\n",
      "Epoch 673, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 673 is 24.7041 sec\n",
      "======================================\n",
      "Epoch 674, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 674 is 25.4069 sec\n",
      "======================================\n",
      "Epoch 675, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 675 is 25.5976 sec\n",
      "======================================\n",
      "Epoch 676, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 676 is 24.5091 sec\n",
      "======================================\n",
      "Epoch 677, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 677 is 24.3801 sec\n",
      "======================================\n",
      "Epoch 678, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 678 is 25.3778 sec\n",
      "======================================\n",
      "Epoch 679, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 679 is 25.0142 sec\n",
      "======================================\n",
      "Epoch 680, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 680 is 24.5793 sec\n",
      "======================================\n",
      "Epoch 681, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 681 is 24.7259 sec\n",
      "======================================\n",
      "Epoch 682, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 682 is 24.6748 sec\n",
      "======================================\n",
      "Epoch 683, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 683 is 24.7820 sec\n",
      "======================================\n",
      "Epoch 684, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 684 is 24.9062 sec\n",
      "======================================\n",
      "Epoch 685, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 685 is 24.8142 sec\n",
      "======================================\n",
      "Epoch 686, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 686 is 24.8183 sec\n",
      "======================================\n",
      "Epoch 687, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 687 is 24.3541 sec\n",
      "======================================\n",
      "Epoch 688, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 688 is 25.4310 sec\n",
      "======================================\n",
      "Epoch 689, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 689 is 25.0060 sec\n",
      "======================================\n",
      "Epoch 690, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 690 is 25.0926 sec\n",
      "======================================\n",
      "Epoch 691, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 691 is 24.7809 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 692, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 692 is 25.2313 sec\n",
      "======================================\n",
      "Epoch 693, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 693 is 24.5053 sec\n",
      "======================================\n",
      "Epoch 694, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 694 is 25.4425 sec\n",
      "======================================\n",
      "Epoch 695, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 695 is 24.8048 sec\n",
      "======================================\n",
      "Epoch 696, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 696 is 25.3962 sec\n",
      "======================================\n",
      "Epoch 697, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 697 is 24.4190 sec\n",
      "======================================\n",
      "Epoch 698, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 698 is 25.1655 sec\n",
      "======================================\n",
      "Epoch 699, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 699 is 24.6479 sec\n",
      "======================================\n",
      "Epoch 700, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 700 is 24.5501 sec\n",
      "======================================\n",
      "Epoch 701, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 701 is 24.4978 sec\n",
      "======================================\n",
      "Epoch 702, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 702 is 24.7574 sec\n",
      "======================================\n",
      "Epoch 703, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 703 is 24.5775 sec\n",
      "======================================\n",
      "Epoch 704, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 704 is 24.8251 sec\n",
      "======================================\n",
      "Epoch 705, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 705 is 25.2410 sec\n",
      "======================================\n",
      "Epoch 706, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 706 is 25.4500 sec\n",
      "======================================\n",
      "Epoch 707, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 707 is 24.8602 sec\n",
      "======================================\n",
      "Epoch 708, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 708 is 24.6555 sec\n",
      "======================================\n",
      "Epoch 709, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 709 is 24.2979 sec\n",
      "======================================\n",
      "Epoch 710, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 710 is 24.6338 sec\n",
      "======================================\n",
      "Epoch 711, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 711 is 24.7128 sec\n",
      "======================================\n",
      "Epoch 712, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 712 is 25.2295 sec\n",
      "======================================\n",
      "Epoch 713, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 713 is 24.4647 sec\n",
      "======================================\n",
      "Epoch 714, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 714 is 25.2666 sec\n",
      "======================================\n",
      "Epoch 715, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 715 is 24.2711 sec\n",
      "======================================\n",
      "Epoch 716, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 716 is 24.6509 sec\n",
      "======================================\n",
      "Epoch 717, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 717 is 24.8210 sec\n",
      "======================================\n",
      "Epoch 718, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 718 is 24.4440 sec\n",
      "======================================\n",
      "Epoch 719, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 719 is 24.4949 sec\n",
      "======================================\n",
      "Epoch 720, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 720 is 24.5069 sec\n",
      "======================================\n",
      "Epoch 721, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 721 is 24.5455 sec\n",
      "======================================\n",
      "Epoch 722, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 722 is 24.9172 sec\n",
      "======================================\n",
      "Epoch 723, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 723 is 25.3229 sec\n",
      "======================================\n",
      "Epoch 724, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 724 is 25.2125 sec\n",
      "======================================\n",
      "Epoch 725, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 725 is 24.5481 sec\n",
      "======================================\n",
      "Epoch 726, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 726 is 24.5767 sec\n",
      "======================================\n",
      "Epoch 727, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 727 is 24.7310 sec\n",
      "======================================\n",
      "Epoch 728, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 728 is 24.5871 sec\n",
      "======================================\n",
      "Epoch 729, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 729 is 24.6238 sec\n",
      "======================================\n",
      "Epoch 730, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 730 is 24.5568 sec\n",
      "======================================\n",
      "Epoch 731, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 731 is 25.2386 sec\n",
      "======================================\n",
      "Epoch 732, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 732 is 24.3417 sec\n",
      "======================================\n",
      "Epoch 733, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 733 is 25.5203 sec\n",
      "======================================\n",
      "Epoch 734, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 734 is 24.8695 sec\n",
      "======================================\n",
      "Epoch 735, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 735 is 24.7515 sec\n",
      "======================================\n",
      "Epoch 736, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 736 is 24.8803 sec\n",
      "======================================\n",
      "Epoch 737, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 737 is 24.2565 sec\n",
      "======================================\n",
      "Epoch 738, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 738 is 24.4445 sec\n",
      "======================================\n",
      "Epoch 739, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 739 is 25.5399 sec\n",
      "======================================\n",
      "Epoch 740, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 740 is 25.1815 sec\n",
      "======================================\n",
      "Epoch 741, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 741 is 24.7077 sec\n",
      "======================================\n",
      "Epoch 742, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 742 is 25.4746 sec\n",
      "======================================\n",
      "Epoch 743, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 743 is 25.1016 sec\n",
      "======================================\n",
      "Epoch 744, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 744 is 25.8262 sec\n",
      "======================================\n",
      "Epoch 745, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 745 is 24.4396 sec\n",
      "======================================\n",
      "Epoch 746, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 746 is 25.5827 sec\n",
      "======================================\n",
      "Epoch 747, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 747 is 24.3000 sec\n",
      "======================================\n",
      "Epoch 748, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 748 is 24.1319 sec\n",
      "======================================\n",
      "Epoch 749, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 749 is 24.3038 sec\n",
      "======================================\n",
      "Epoch 750, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 750 is 24.4452 sec\n",
      "======================================\n",
      "Epoch 751, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 751 is 24.3213 sec\n",
      "======================================\n",
      "Epoch 752, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 752 is 24.1510 sec\n",
      "======================================\n",
      "Epoch 753, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 753 is 24.3292 sec\n",
      "======================================\n",
      "Epoch 754, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 754 is 24.1679 sec\n",
      "======================================\n",
      "Epoch 755, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 755 is 24.4266 sec\n",
      "======================================\n",
      "Epoch 756, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 756 is 24.4051 sec\n",
      "======================================\n",
      "Epoch 757, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 757 is 24.2680 sec\n",
      "======================================\n",
      "Epoch 758, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 758 is 24.3041 sec\n",
      "======================================\n",
      "Epoch 759, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 759 is 24.4401 sec\n",
      "======================================\n",
      "Epoch 760, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 760 is 24.4454 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 761, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 761 is 24.6132 sec\n",
      "======================================\n",
      "Epoch 762, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 762 is 24.1999 sec\n",
      "======================================\n",
      "Epoch 763, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 763 is 24.3389 sec\n",
      "======================================\n",
      "Epoch 764, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 764 is 24.4272 sec\n",
      "======================================\n",
      "Epoch 765, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 765 is 24.3404 sec\n",
      "======================================\n",
      "Epoch 766, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 766 is 24.4181 sec\n",
      "======================================\n",
      "Epoch 767, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 767 is 24.5013 sec\n",
      "======================================\n",
      "Epoch 768, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 768 is 24.2288 sec\n",
      "======================================\n",
      "Epoch 769, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 769 is 24.4340 sec\n",
      "======================================\n",
      "Epoch 770, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 770 is 24.3284 sec\n",
      "======================================\n",
      "Epoch 771, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 771 is 24.7258 sec\n",
      "======================================\n",
      "Epoch 772, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 772 is 24.4663 sec\n",
      "======================================\n",
      "Epoch 773, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 773 is 24.3028 sec\n",
      "======================================\n",
      "Epoch 774, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 774 is 24.9512 sec\n",
      "======================================\n",
      "Epoch 775, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 775 is 24.3180 sec\n",
      "======================================\n",
      "Epoch 776, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 776 is 24.2063 sec\n",
      "======================================\n",
      "Epoch 777, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 777 is 24.4600 sec\n",
      "======================================\n",
      "Epoch 778, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 778 is 24.2170 sec\n",
      "======================================\n",
      "Epoch 779, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 779 is 24.2168 sec\n",
      "======================================\n",
      "Epoch 780, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 780 is 24.1788 sec\n",
      "======================================\n",
      "Epoch 781, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 781 is 24.1211 sec\n",
      "======================================\n",
      "Epoch 782, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 782 is 24.6251 sec\n",
      "======================================\n",
      "Epoch 783, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 783 is 24.5030 sec\n",
      "======================================\n",
      "Epoch 784, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 784 is 24.4906 sec\n",
      "======================================\n",
      "Epoch 785, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 785 is 24.3243 sec\n",
      "======================================\n",
      "Epoch 786, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 786 is 24.0860 sec\n",
      "======================================\n",
      "Epoch 787, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 787 is 24.7712 sec\n",
      "======================================\n",
      "Epoch 788, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 788 is 24.3451 sec\n",
      "======================================\n",
      "Epoch 789, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 789 is 24.4562 sec\n",
      "======================================\n",
      "Epoch 790, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 790 is 24.3795 sec\n",
      "======================================\n",
      "Epoch 791, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 791 is 24.7020 sec\n",
      "======================================\n",
      "Epoch 792, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 792 is 24.1843 sec\n",
      "======================================\n",
      "Epoch 793, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 793 is 24.1759 sec\n",
      "======================================\n",
      "Epoch 794, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 794 is 24.6651 sec\n",
      "======================================\n",
      "Epoch 795, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 795 is 24.4583 sec\n",
      "======================================\n",
      "Epoch 796, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 796 is 24.7218 sec\n",
      "======================================\n",
      "Epoch 797, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 797 is 24.2478 sec\n",
      "======================================\n",
      "Epoch 798, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 798 is 24.5258 sec\n",
      "======================================\n",
      "Epoch 799, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 799 is 24.1509 sec\n",
      "======================================\n",
      "Epoch 800, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 800 is 24.4326 sec\n",
      "======================================\n",
      "Epoch 801, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 801 is 24.6867 sec\n",
      "======================================\n",
      "Epoch 802, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 802 is 24.2681 sec\n",
      "======================================\n",
      "Epoch 803, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 803 is 24.4699 sec\n",
      "======================================\n",
      "Epoch 804, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 804 is 24.6180 sec\n",
      "======================================\n",
      "Epoch 805, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 805 is 24.4701 sec\n",
      "======================================\n",
      "Epoch 806, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 806 is 24.2889 sec\n",
      "======================================\n",
      "Epoch 807, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 807 is 24.4799 sec\n",
      "======================================\n",
      "Epoch 808, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 808 is 24.2892 sec\n",
      "======================================\n",
      "Epoch 809, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 809 is 24.5425 sec\n",
      "======================================\n",
      "Epoch 810, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 810 is 24.5429 sec\n",
      "======================================\n",
      "Epoch 811, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 811 is 24.2993 sec\n",
      "======================================\n",
      "Epoch 812, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 812 is 24.2666 sec\n",
      "======================================\n",
      "Epoch 813, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 813 is 24.1378 sec\n",
      "======================================\n",
      "Epoch 814, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 814 is 24.3237 sec\n",
      "======================================\n",
      "Epoch 815, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 815 is 24.2254 sec\n",
      "======================================\n",
      "Epoch 816, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 816 is 24.5228 sec\n",
      "======================================\n",
      "Epoch 817, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 817 is 24.3739 sec\n",
      "======================================\n",
      "Epoch 818, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 818 is 24.4913 sec\n",
      "======================================\n",
      "Epoch 819, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 819 is 24.3972 sec\n",
      "======================================\n",
      "Epoch 820, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 820 is 24.3817 sec\n",
      "======================================\n",
      "Epoch 821, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 821 is 24.3625 sec\n",
      "======================================\n",
      "Epoch 822, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 822 is 24.4301 sec\n",
      "======================================\n",
      "Epoch 823, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 823 is 24.2580 sec\n",
      "======================================\n",
      "Epoch 824, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 824 is 24.3240 sec\n",
      "======================================\n",
      "Epoch 825, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 825 is 24.4429 sec\n",
      "======================================\n",
      "Epoch 826, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 826 is 24.1677 sec\n",
      "======================================\n",
      "Epoch 827, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 827 is 24.6176 sec\n",
      "======================================\n",
      "Epoch 828, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 828 is 24.4803 sec\n",
      "======================================\n",
      "Epoch 829, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 829 is 24.2278 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 830, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 830 is 24.2629 sec\n",
      "======================================\n",
      "Epoch 831, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 831 is 24.1672 sec\n",
      "======================================\n",
      "Epoch 832, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 832 is 24.4127 sec\n",
      "======================================\n",
      "Epoch 833, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 833 is 24.3922 sec\n",
      "======================================\n",
      "Epoch 834, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 834 is 24.6042 sec\n",
      "======================================\n",
      "Epoch 835, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 835 is 24.5469 sec\n",
      "======================================\n",
      "Epoch 836, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 836 is 24.4034 sec\n",
      "======================================\n",
      "Epoch 837, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 837 is 24.2638 sec\n",
      "======================================\n",
      "Epoch 838, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 838 is 24.1900 sec\n",
      "======================================\n",
      "Epoch 839, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 839 is 24.2345 sec\n",
      "======================================\n",
      "Epoch 840, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 840 is 24.2879 sec\n",
      "======================================\n",
      "Epoch 841, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 841 is 24.4465 sec\n",
      "======================================\n",
      "Epoch 842, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 842 is 24.2539 sec\n",
      "======================================\n",
      "Epoch 843, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 843 is 24.3314 sec\n",
      "======================================\n",
      "Epoch 844, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 844 is 24.1273 sec\n",
      "======================================\n",
      "Epoch 845, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 845 is 24.1270 sec\n",
      "======================================\n",
      "Epoch 846, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 846 is 24.3523 sec\n",
      "======================================\n",
      "Epoch 847, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 847 is 24.3641 sec\n",
      "======================================\n",
      "Epoch 848, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 848 is 24.8426 sec\n",
      "======================================\n",
      "Epoch 849, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 849 is 25.4802 sec\n",
      "======================================\n",
      "Epoch 850, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 850 is 25.1569 sec\n",
      "======================================\n",
      "Epoch 851, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 851 is 24.8872 sec\n",
      "======================================\n",
      "Epoch 852, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 852 is 24.8153 sec\n",
      "======================================\n",
      "Epoch 853, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 853 is 24.6507 sec\n",
      "======================================\n",
      "Epoch 854, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 854 is 26.0582 sec\n",
      "======================================\n",
      "Epoch 855, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 855 is 24.6582 sec\n",
      "======================================\n",
      "Epoch 856, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 856 is 25.3801 sec\n",
      "======================================\n",
      "Epoch 857, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 857 is 25.7894 sec\n",
      "======================================\n",
      "Epoch 858, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 858 is 24.5398 sec\n",
      "======================================\n",
      "Epoch 859, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 859 is 25.1680 sec\n",
      "======================================\n",
      "Epoch 860, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 860 is 24.4558 sec\n",
      "======================================\n",
      "Epoch 861, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 861 is 24.2311 sec\n",
      "======================================\n",
      "Epoch 862, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 862 is 24.0237 sec\n",
      "======================================\n",
      "Epoch 863, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 863 is 24.5331 sec\n",
      "======================================\n",
      "Epoch 864, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 864 is 24.1311 sec\n",
      "======================================\n",
      "Epoch 865, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 865 is 24.3179 sec\n",
      "======================================\n",
      "Epoch 866, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 866 is 24.4951 sec\n",
      "======================================\n",
      "Epoch 867, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 867 is 24.2820 sec\n",
      "======================================\n",
      "Epoch 868, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 868 is 24.3851 sec\n",
      "======================================\n",
      "Epoch 869, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 869 is 25.1657 sec\n",
      "======================================\n",
      "Epoch 870, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 870 is 25.1970 sec\n",
      "======================================\n",
      "Epoch 871, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 871 is 24.9064 sec\n",
      "======================================\n",
      "Epoch 872, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 872 is 24.7942 sec\n",
      "======================================\n",
      "Epoch 873, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 873 is 25.1441 sec\n",
      "======================================\n",
      "Epoch 874, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 874 is 24.6519 sec\n",
      "======================================\n",
      "Epoch 875, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 875 is 24.4195 sec\n",
      "======================================\n",
      "Epoch 876, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 876 is 24.3500 sec\n",
      "======================================\n",
      "Epoch 877, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 877 is 24.4080 sec\n",
      "======================================\n",
      "Epoch 878, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 878 is 24.4347 sec\n",
      "======================================\n",
      "Epoch 879, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 879 is 24.0873 sec\n",
      "======================================\n",
      "Epoch 880, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 880 is 24.3561 sec\n",
      "======================================\n",
      "Epoch 881, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 881 is 24.1802 sec\n",
      "======================================\n",
      "Epoch 882, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 882 is 24.3251 sec\n",
      "======================================\n",
      "Epoch 883, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 883 is 27.0216 sec\n",
      "======================================\n",
      "Epoch 884, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 884 is 24.1631 sec\n",
      "======================================\n",
      "Epoch 885, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 885 is 24.4037 sec\n",
      "======================================\n",
      "Epoch 886, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 886 is 24.4399 sec\n",
      "======================================\n",
      "Epoch 887, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 887 is 24.6199 sec\n",
      "======================================\n",
      "Epoch 888, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 888 is 24.6228 sec\n",
      "======================================\n",
      "Epoch 889, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 889 is 24.3290 sec\n",
      "======================================\n",
      "Epoch 890, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 890 is 24.1789 sec\n",
      "======================================\n",
      "Epoch 891, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 891 is 24.2971 sec\n",
      "======================================\n",
      "Epoch 892, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 892 is 24.3610 sec\n",
      "======================================\n",
      "Epoch 893, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 893 is 24.2628 sec\n",
      "======================================\n",
      "Epoch 894, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 894 is 24.3011 sec\n",
      "======================================\n",
      "Epoch 895, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 895 is 24.6028 sec\n",
      "======================================\n",
      "Epoch 896, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 896 is 24.6461 sec\n",
      "======================================\n",
      "Epoch 897, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 897 is 24.2197 sec\n",
      "======================================\n",
      "Epoch 898, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 898 is 24.4900 sec\n",
      "======================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 899, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 899 is 24.2511 sec\n",
      "======================================\n",
      "Epoch 900, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 900 is 24.5892 sec\n",
      "======================================\n",
      "Epoch 901, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 901 is 24.7122 sec\n",
      "======================================\n",
      "Epoch 902, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 902 is 24.5102 sec\n",
      "======================================\n",
      "Epoch 903, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 903 is 24.2473 sec\n",
      "======================================\n",
      "Epoch 904, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 904 is 24.8383 sec\n",
      "======================================\n",
      "Epoch 905, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 905 is 24.3196 sec\n",
      "======================================\n",
      "Epoch 906, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 906 is 24.3981 sec\n",
      "======================================\n",
      "Epoch 907, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 907 is 24.2156 sec\n",
      "======================================\n",
      "Epoch 908, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 908 is 24.6339 sec\n",
      "======================================\n",
      "Epoch 909, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 909 is 24.7737 sec\n",
      "======================================\n",
      "Epoch 910, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 910 is 24.2436 sec\n",
      "======================================\n",
      "Epoch 911, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 911 is 24.2950 sec\n",
      "======================================\n",
      "Epoch 912, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 912 is 24.5307 sec\n",
      "======================================\n",
      "Epoch 913, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 913 is 24.4060 sec\n",
      "======================================\n",
      "Epoch 914, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 914 is 24.1455 sec\n",
      "======================================\n",
      "Epoch 915, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 915 is 24.2940 sec\n",
      "======================================\n",
      "Epoch 916, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 916 is 24.2001 sec\n",
      "======================================\n",
      "Epoch 917, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 917 is 24.0452 sec\n",
      "======================================\n",
      "Epoch 918, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 918 is 24.2879 sec\n",
      "======================================\n",
      "Epoch 919, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 919 is 24.2340 sec\n",
      "======================================\n",
      "Epoch 920, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 920 is 24.5353 sec\n",
      "======================================\n",
      "Epoch 921, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 921 is 24.3168 sec\n",
      "======================================\n",
      "Epoch 922, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 922 is 24.3610 sec\n",
      "======================================\n",
      "Epoch 923, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 923 is 24.6886 sec\n",
      "======================================\n",
      "Epoch 924, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 924 is 24.1920 sec\n",
      "======================================\n",
      "Epoch 925, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 925 is 24.1934 sec\n",
      "======================================\n",
      "Epoch 926, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 926 is 24.3638 sec\n",
      "======================================\n",
      "Epoch 927, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 927 is 24.5194 sec\n",
      "======================================\n",
      "Epoch 928, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 928 is 24.3221 sec\n",
      "======================================\n",
      "Epoch 929, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 929 is 24.3111 sec\n",
      "======================================\n",
      "Epoch 930, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 930 is 24.2368 sec\n",
      "======================================\n",
      "Epoch 931, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 931 is 24.2913 sec\n",
      "======================================\n",
      "Epoch 932, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 932 is 24.5000 sec\n",
      "======================================\n",
      "Epoch 933, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 933 is 24.1880 sec\n",
      "======================================\n",
      "Epoch 934, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 934 is 24.1550 sec\n",
      "======================================\n",
      "Epoch 935, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 935 is 24.3308 sec\n",
      "======================================\n",
      "Epoch 936, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 936 is 24.2929 sec\n",
      "======================================\n",
      "Epoch 937, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 937 is 24.3788 sec\n",
      "======================================\n",
      "Epoch 938, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 938 is 24.3773 sec\n",
      "======================================\n",
      "Epoch 939, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 939 is 24.3587 sec\n",
      "======================================\n",
      "Epoch 940, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 940 is 24.1166 sec\n",
      "======================================\n",
      "Epoch 941, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 941 is 24.4031 sec\n",
      "======================================\n",
      "Epoch 942, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 942 is 24.6094 sec\n",
      "======================================\n",
      "Epoch 943, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 943 is 24.3685 sec\n",
      "======================================\n",
      "Epoch 944, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 944 is 24.2636 sec\n",
      "======================================\n",
      "Epoch 945, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 945 is 24.1868 sec\n",
      "======================================\n",
      "Epoch 946, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 946 is 24.2747 sec\n",
      "======================================\n",
      "Epoch 947, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 947 is 24.8088 sec\n",
      "======================================\n",
      "Epoch 948, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 948 is 24.4181 sec\n",
      "======================================\n",
      "Epoch 949, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 949 is 24.3530 sec\n",
      "======================================\n",
      "Epoch 950, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 950 is 24.2218 sec\n",
      "======================================\n",
      "Epoch 951, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 951 is 24.3457 sec\n",
      "======================================\n",
      "Epoch 952, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 952 is 24.3949 sec\n",
      "======================================\n",
      "Epoch 953, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 953 is 24.5226 sec\n",
      "======================================\n",
      "Epoch 954, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 954 is 24.3891 sec\n",
      "======================================\n",
      "Epoch 955, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 955 is 24.3070 sec\n",
      "======================================\n",
      "Epoch 956, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 956 is 24.2978 sec\n",
      "======================================\n",
      "Epoch 957, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 957 is 24.0240 sec\n",
      "======================================\n",
      "Epoch 958, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 958 is 24.8748 sec\n",
      "======================================\n",
      "Epoch 959, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 959 is 25.2990 sec\n",
      "======================================\n",
      "Epoch 960, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 960 is 25.3408 sec\n",
      "======================================\n",
      "Epoch 961, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 961 is 25.8640 sec\n",
      "======================================\n",
      "Epoch 962, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 962 is 26.7604 sec\n",
      "======================================\n",
      "Epoch 963, gen_loss: 0.6931, disc_loss: 0.0000\n",
      "Time for epoch 963 is 25.4691 sec\n",
      "======================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19132\\32322937.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'N_EPOCH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19132\\371661158.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mimshow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaption\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimshow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mimshow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    750\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3011\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m   3012\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNext\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3013\u001b[1;33m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   3014\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3015\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "a634d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_caption2string(ls):\n",
    "    return \" \".join([id2word_dict[idx] for idx in ls]).strip().split(' <PAD>')[0]\n",
    "\n",
    "def testing_data_generator(caption, index, caption_type='id'):\n",
    "    if caption_type == 'id':\n",
    "        caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator, caption_type='id'):\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    \n",
    "    if caption_type == 'sentence':\n",
    "        data['Captions_string'] = data['Captions'].apply(test_caption2string)\n",
    "        captions = data['Captions_string'].values\n",
    "    elif caption_type == 'id':\n",
    "        captions = data['Captions'].values\n",
    "        \n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    if caption_type == 'id':\n",
    "        caption = caption.astype(np.int)\n",
    "        \n",
    "    datagen_func = lambda cap, img: data_generator(cap, img, caption_type=caption_type)\n",
    "        \n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(datagen_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "c7abd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator, caption_type=expSettings.caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "88cc621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (64,)\n",
      "Caption shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "# test the testing dataset\n",
    "for cap, img in testing_dataset.take(1):\n",
    "    print(\"Image shape:\", img.numpy().shape)\n",
    "    print(\"Caption shape:\", cap.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "8d07f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "8f624f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./inference/demo'):\n",
    "    os.makedirs('./inference/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "6d3d0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=0.1, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    print(sample_seed[0:3, :])\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "            if i == 0 and step == 1: \n",
    "                #print(captions)\n",
    "                text_embed_t, hidden_t = text_encoder(captions, hidden)\n",
    "                #print(text_embed_t)\n",
    "                print(fake_image[0:1, 0:5, 0:5, :])\n",
    "                img_logits, _, debug = generator(text_embed_t, sample_seed, debug_output=True)\n",
    "                print(debug[0][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[1][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[2][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[3][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[4][0:3, 0:5, 0:5, 0:5])\n",
    "                print(img_logits[0:3, 0:5, 0:5, :])\n",
    "                pred_logit, pred = discriminator(fake_image, text_embed_t)\n",
    "                #print(pred_logit)\n",
    "                \n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "d12a53a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./checkpoints/demo\\ckpt-963\n"
     ]
    }
   ],
   "source": [
    "#checkpoint.restore(checkpoint_dir + f'/ckpt-50')\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Restoring from {latest_checkpoint}\")\n",
    "    checkpoint.restore(latest_checkpoint)\n",
    "else:\n",
    "    print(\"No checkpoint found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "ff59c63a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25735587  0.02635855  0.07098211 ...  0.11278663  0.08181195\n",
      "   0.04097688]\n",
      " [-0.24220537 -0.10393385 -0.04679523 ... -0.13002335  0.01695522\n",
      "  -0.08721623]\n",
      " [-0.09068998 -0.0250848   0.10610484 ... -0.23031023  0.22233571\n",
      "  -0.11565182]]\n",
      "tf.Tensor(\n",
      "[[[[ 0.05371411  0.07407826  0.01101015]\n",
      "   [-0.23510526 -0.02537002 -0.23780264]\n",
      "   [ 0.7734425  -0.0782879  -0.09224059]\n",
      "   [-0.09640063 -0.11736421 -0.18235064]\n",
      "   [-0.3867997  -0.30161613 -0.31821412]]\n",
      "\n",
      "  [[-0.02428844 -0.00901427 -0.01478982]\n",
      "   [-0.8219737  -0.74098504 -0.751086  ]\n",
      "   [-0.92784774 -0.48945045 -0.78000706]\n",
      "   [-0.9865599  -0.8891686  -0.94828457]\n",
      "   [-0.9906214  -0.8981045  -0.8246935 ]]\n",
      "\n",
      "  [[-0.0325912  -0.01237387 -0.02101737]\n",
      "   [-0.79343396 -0.20068747 -0.65321845]\n",
      "   [-0.96634734 -0.7707148  -0.8599473 ]\n",
      "   [-0.9650226  -0.7101485  -0.8881296 ]\n",
      "   [-0.9939072  -0.91076356 -0.9596795 ]]\n",
      "\n",
      "  [[-0.04732414 -0.02123832 -0.02856526]\n",
      "   [-0.9788062  -0.84277856 -0.97331697]\n",
      "   [-0.99145055 -0.60533476 -0.93648994]\n",
      "   [-0.99985516 -0.99836844 -0.99959064]\n",
      "   [-0.9999841  -0.9934562  -0.99514556]]\n",
      "\n",
      "  [[-0.03563284 -0.02352159 -0.02210975]\n",
      "   [-0.897297   -0.1281277  -0.833106  ]\n",
      "   [-0.85504246 -0.8424125  -0.90716034]\n",
      "   [-0.9913347  -0.55733544 -0.97603303]\n",
      "   [-0.9998229  -0.99247193 -0.9972286 ]]]], shape=(1, 5, 5, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-0.13544112 -0.22869094 -0.11748415 -0.0558539  -0.1800295 ]\n",
      "   [-0.08462553 -0.0871674  -0.14017984 -0.1467551  -0.08566784]]\n",
      "\n",
      "  [[-0.03547836 -0.18772088 -0.04753652 -0.2260325  -0.23339093]\n",
      "   [-0.03594555  0.15835613 -0.01116289 -0.08357322 -0.03983687]]]\n",
      "\n",
      "\n",
      " [[[-0.11371904 -0.07376008 -0.078206   -0.15940988 -0.13736609]\n",
      "   [-0.02487661 -0.19445944 -0.06026432 -0.07291579 -0.14248604]]\n",
      "\n",
      "  [[-0.11532696 -0.09772105 -0.1963218  -0.1519921  -0.28094047]\n",
      "   [-0.15411921 -0.15321863  1.087596   -0.07692196 -0.03046727]]]\n",
      "\n",
      "\n",
      " [[[-0.13618521  0.06522537 -0.19782165 -0.1788718  -0.1169649 ]\n",
      "   [-0.19575919 -0.07286366 -0.26526573 -0.05590468 -0.12071057]]\n",
      "\n",
      "  [[-0.10934514 -0.22079003 -0.10321176 -0.18491666 -0.33954322]\n",
      "   [-0.12696218 -0.06261244  0.16377263 -0.05673087 -0.07957178]]]], shape=(3, 2, 2, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[ 4.78239870e+00  7.38610744e-01  8.90894294e-01 -6.19023219e-02\n",
      "     1.56024158e+00]\n",
      "   [-1.36017790e-02  1.02086058e+01  1.00259438e+01 -2.43667915e-01\n",
      "     6.01155281e+00]\n",
      "   [ 8.45824146e+00  5.66814756e+00  5.05511475e+00 -9.43512917e-02\n",
      "     7.70184278e+00]\n",
      "   [-1.97202742e-01  3.29954815e+00  9.14264441e-01 -2.31375813e-01\n",
      "     8.32395479e-02]]\n",
      "\n",
      "  [[-1.16135016e-01 -5.27420342e-02 -8.60460475e-02 -2.38662124e-01\n",
      "     1.12358916e+00]\n",
      "   [-9.80735779e-01  4.15317917e+00  8.20704639e-01 -5.08394122e-01\n",
      "     1.40466185e+01]\n",
      "   [-2.27531344e-01 -1.01863742e-01 -3.25586885e-01 -7.43218601e-01\n",
      "     1.00054989e+01]\n",
      "   [-3.44882458e-01 -2.95426190e-01 -1.39608845e-01 -3.04897040e-01\n",
      "     7.65563917e+00]]\n",
      "\n",
      "  [[-7.44458660e-02 -1.23247817e-01  1.44014037e+00 -9.31334496e-02\n",
      "    -2.15760261e-01]\n",
      "   [-8.04468751e-01 -3.57252181e-01  1.41700954e+01 -5.76590300e-01\n",
      "    -7.38976002e-01]\n",
      "   [-1.07764006e-01 -3.66165727e-01  1.53337717e+01 -8.86574686e-02\n",
      "    -5.66606939e-01]\n",
      "   [-1.19218528e-01 -3.95429105e-01 -1.85001656e-01  8.52361768e-02\n",
      "    -2.95896739e-01]]\n",
      "\n",
      "  [[-3.37608345e-02 -4.75140810e-02  8.22777271e-01 -2.93974429e-02\n",
      "    -3.28736156e-02]\n",
      "   [ 4.95393723e-01 -1.46286920e-01 -8.73893723e-02 -6.41176030e-02\n",
      "    -2.42531702e-01]\n",
      "   [ 5.54868519e-01 -4.78589572e-02 -1.12992696e-01 -7.13277329e-03\n",
      "    -1.82754576e-01]\n",
      "   [ 6.31559706e+00 -4.10773270e-02 -1.01088800e-01 -1.04720704e-02\n",
      "    -2.25942180e-01]]]\n",
      "\n",
      "\n",
      " [[[ 5.69194794e+00  9.12725508e-01  1.15057743e+00 -5.95371835e-02\n",
      "     1.86647761e+00]\n",
      "   [ 2.46693566e-01  1.13604469e+01  1.15701351e+01 -2.33875200e-01\n",
      "     7.25199747e+00]\n",
      "   [ 8.68352699e+00  6.98220587e+00  5.33981562e+00 -9.72435102e-02\n",
      "     9.00302887e+00]\n",
      "   [-1.91324040e-01  3.48160768e+00  8.05580914e-01 -2.17215806e-01\n",
      "     5.48446476e-01]]\n",
      "\n",
      "  [[-1.10188998e-01 -5.46787493e-02 -1.86081514e-01 -2.88122743e-01\n",
      "     1.48423719e+00]\n",
      "   [-1.09168148e+00  3.97168779e+00  5.12605488e-01 -5.83837390e-01\n",
      "     1.53278608e+01]\n",
      "   [-2.59844184e-01 -1.19072475e-01 -5.30447125e-01 -8.43401432e-01\n",
      "     1.06755409e+01]\n",
      "   [-3.12739879e-01 -2.64554381e-01 -1.97866574e-01 -3.30913365e-01\n",
      "     8.07895184e+00]]\n",
      "\n",
      "  [[-4.09899503e-02 -1.39335185e-01  1.47096312e+00 -1.14101686e-01\n",
      "    -2.57548034e-01]\n",
      "   [-9.03815210e-01 -4.16007429e-01  1.47651196e+01 -5.86951196e-01\n",
      "    -8.01556766e-01]\n",
      "   [-4.82716374e-02 -4.40124094e-01  1.59528637e+01 -9.80777219e-02\n",
      "    -6.59770727e-01]\n",
      "   [-1.24822654e-01 -4.51404005e-01 -2.69613773e-01  6.51371956e-01\n",
      "    -2.80346692e-01]]\n",
      "\n",
      "  [[-4.17838655e-02 -7.05200359e-02  1.07381797e+00 -2.90373415e-02\n",
      "    -5.02369814e-02]\n",
      "   [ 1.41335949e-01 -1.83429122e-01 -9.75806415e-02 -7.08280429e-02\n",
      "    -2.54756302e-01]\n",
      "   [ 2.94147491e-01 -7.59842098e-02 -1.09078936e-01 -3.59916948e-02\n",
      "    -1.97365642e-01]\n",
      "   [ 6.22725344e+00 -5.29642962e-02 -8.19999352e-02 -6.10273182e-02\n",
      "    -2.81306982e-01]]]\n",
      "\n",
      "\n",
      " [[[ 6.01264954e+00  1.14507627e+00  8.68055761e-01 -5.85330427e-02\n",
      "     2.30348468e+00]\n",
      "   [-3.82994823e-02  1.17471437e+01  1.19153643e+01 -2.78621703e-01\n",
      "     6.98529577e+00]\n",
      "   [ 9.57143593e+00  6.61248779e+00  5.98564482e+00 -1.11259758e-01\n",
      "     8.98786831e+00]\n",
      "   [-2.28217155e-01  3.64999485e+00  9.20939982e-01 -2.50255704e-01\n",
      "     2.30013236e-01]]\n",
      "\n",
      "  [[-1.01341940e-01 -7.38209561e-02 -1.38981983e-01 -2.95455843e-01\n",
      "     1.37731385e+00]\n",
      "   [-1.17200303e+00  4.00153589e+00  8.85460377e-01 -6.16463602e-01\n",
      "     1.61217003e+01]\n",
      "   [-2.37056956e-01 -1.02941491e-01 -4.71607119e-01 -8.51697922e-01\n",
      "     1.14135227e+01]\n",
      "   [-2.85635352e-01 -2.81320333e-01 -1.54010281e-01 -3.84843171e-01\n",
      "     8.19455624e+00]]\n",
      "\n",
      "  [[-7.59616122e-02 -9.44213644e-02  1.80353868e+00 -9.26441252e-02\n",
      "    -2.77664483e-01]\n",
      "   [-9.04948711e-01 -4.40087706e-01  1.54548025e+01 -6.50485933e-01\n",
      "    -7.92592824e-01]\n",
      "   [-1.34155393e-01 -4.17518467e-01  1.61027203e+01 -1.03579104e-01\n",
      "    -6.77526057e-01]\n",
      "   [-1.38923377e-01 -4.82945442e-01 -2.65803158e-01  1.02781296e-01\n",
      "    -3.31719935e-01]]\n",
      "\n",
      "  [[-4.47566323e-02 -4.19483595e-02  8.69324446e-01 -2.93832552e-02\n",
      "    -2.64337752e-02]\n",
      "   [ 4.31631282e-02 -1.87395155e-01 -1.01960197e-01 -4.56503332e-02\n",
      "    -2.47032166e-01]\n",
      "   [ 6.19950175e-01 -5.41433692e-02 -1.53133884e-01 -1.45432977e-02\n",
      "    -1.96062580e-01]\n",
      "   [ 6.89851522e+00 -8.88875965e-03 -1.17914535e-01 -4.74976711e-02\n",
      "    -2.78889120e-01]]]], shape=(3, 4, 4, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-1.84168518e-01  2.88473463e+00 -6.81601986e-02 -4.32402007e-02\n",
      "    -4.34863530e-02]\n",
      "   [-1.20528102e+00 -8.19348514e-01 -4.91692126e-01  7.20514536e+00\n",
      "    -5.58867037e-01]\n",
      "   [-6.98901832e-01  2.20524960e+01  1.27534103e+00 -4.25445251e-02\n",
      "    -6.01565957e-01]\n",
      "   [-1.82359052e+00 -1.37563741e+00 -1.33601224e+00  9.82530177e-01\n",
      "    -7.78702676e-01]\n",
      "   [-1.30095768e+00  3.89233170e+01 -7.31453836e-01  4.50376892e+00\n",
      "    -1.06856608e+00]]\n",
      "\n",
      "  [[-9.03609619e-02  5.97725964e+00 -2.46275783e-01  1.15308449e-01\n",
      "    -9.82638374e-02]\n",
      "   [-1.54232621e+00 -2.55979776e+00 -1.74305177e+00 -1.40036523e+00\n",
      "    -3.35204184e-01]\n",
      "   [-9.89472032e-01  6.79919662e+01 -1.46247029e+00  1.20001411e+01\n",
      "    -1.82826257e+00]\n",
      "   [-2.72257614e+00 -2.93368554e+00 -5.02289534e+00 -1.36341119e+00\n",
      "    -1.88251436e+00]\n",
      "   [-1.05091059e+00  1.13825294e+02 -2.26919413e+00  9.16078949e+00\n",
      "    -9.26015794e-01]]\n",
      "\n",
      "  [[-3.97266857e-02  5.23734426e+00 -1.91258430e-01 -8.49397108e-02\n",
      "    -1.99066713e-01]\n",
      "   [-1.21091044e+00 -8.40234101e-01  1.13286495e+01  7.08123493e+00\n",
      "    -6.10367596e-01]\n",
      "   [-6.46523714e-01  5.26469231e+01 -1.44702423e+00  2.19878531e+00\n",
      "    -1.16274071e+00]\n",
      "   [-1.15857542e+00 -1.71182692e+00 -1.51911771e+00  1.04671469e+01\n",
      "    -1.85051823e+00]\n",
      "   [-9.70891297e-01  8.24564743e+01 -1.59267092e+00  4.77260542e+00\n",
      "    -1.55381536e+00]]\n",
      "\n",
      "  [[-9.10886656e-03  4.32437563e+00 -3.99447158e-02  7.95523524e-01\n",
      "    -1.28133431e-01]\n",
      "   [-2.89528513e+00 -3.61806840e-01  6.40704453e-01 -2.79307055e+00\n",
      "     5.02346134e+00]\n",
      "   [-6.40509784e-01  6.56901779e+01  2.66823196e+00  1.22962456e+01\n",
      "    -6.30580783e-01]\n",
      "   [-2.37124181e+00 -2.49297547e+00 -4.38167381e+00 -1.69460583e+00\n",
      "    -1.25804484e-01]\n",
      "   [-3.20055813e-01  1.17374771e+02 -1.94802988e+00  2.00311794e+01\n",
      "     3.73057842e+00]]\n",
      "\n",
      "  [[-1.07699670e-01  2.21864223e+00 -1.42723188e-01 -1.88870411e-02\n",
      "    -1.69571321e-02]\n",
      "   [ 1.54805034e-01 -1.71148992e+00  3.66405602e+01  2.44794254e+01\n",
      "    -1.07902467e+00]\n",
      "   [-1.58953714e+00  3.62198410e+01 -1.34190476e+00  5.93541336e+00\n",
      "    -3.41353804e-01]\n",
      "   [-1.76351833e+00 -2.12061429e+00 -9.60445106e-01  2.73689995e+01\n",
      "    -2.01404810e+00]\n",
      "   [-2.16146374e+00  7.34081116e+01 -3.70481849e+00  8.18678093e+00\n",
      "    -2.09978849e-01]]]\n",
      "\n",
      "\n",
      " [[[-2.14711651e-01  3.60207534e+00 -7.31975958e-02 -5.24805076e-02\n",
      "    -5.02352901e-02]\n",
      "   [-1.37810075e+00 -9.05378938e-01 -5.79978943e-01  8.01621532e+00\n",
      "    -6.22704923e-01]\n",
      "   [-8.28706741e-01  2.51194515e+01  1.58542657e+00 -1.76309701e-02\n",
      "    -6.54816508e-01]\n",
      "   [-2.02574420e+00 -1.62494564e+00 -1.44068718e+00  1.15821993e+00\n",
      "    -8.74003410e-01]\n",
      "   [-1.45214820e+00  4.34509506e+01 -8.37008476e-01  5.45334005e+00\n",
      "    -1.23451650e+00]]\n",
      "\n",
      "  [[-1.10860325e-01  7.25777149e+00 -3.16293508e-01  3.32329065e-01\n",
      "    -1.47004843e-01]\n",
      "   [-1.78421533e+00 -2.85530949e+00 -2.07528090e+00 -1.57624853e+00\n",
      "    -3.40980798e-01]\n",
      "   [-1.12192595e+00  7.69186249e+01 -1.65584183e+00  1.42332125e+01\n",
      "    -2.00538468e+00]\n",
      "   [-3.08077788e+00 -3.31958747e+00 -5.60496855e+00 -1.56949890e+00\n",
      "    -2.08090973e+00]\n",
      "   [-1.18543959e+00  1.26362511e+02 -2.52134109e+00  9.92631435e+00\n",
      "    -1.04123676e+00]]\n",
      "\n",
      "  [[-5.87488972e-02  5.99757719e+00 -2.41287231e-01 -9.56043154e-02\n",
      "    -2.46236876e-01]\n",
      "   [-1.26246488e+00 -9.07472789e-01  1.20916939e+01  8.12821865e+00\n",
      "    -6.85665071e-01]\n",
      "   [-7.21429348e-01  5.92133522e+01 -1.65014303e+00  2.45298862e+00\n",
      "    -1.24900734e+00]\n",
      "   [-1.24609232e+00 -1.86049020e+00 -1.70944822e+00  1.23642578e+01\n",
      "    -2.09186769e+00]\n",
      "   [-1.13828814e+00  9.07456131e+01 -1.82906365e+00  4.82671881e+00\n",
      "    -1.78352010e+00]]\n",
      "\n",
      "  [[-1.93299185e-02  5.31806946e+00 -3.05191334e-02  8.86667609e-01\n",
      "    -1.64010227e-01]\n",
      "   [-3.24141049e+00 -3.99049938e-01  4.89240527e-01 -3.08488107e+00\n",
      "     5.59197283e+00]\n",
      "   [-6.76308215e-01  7.31730652e+01  3.27791500e+00  1.36821804e+01\n",
      "    -6.74486816e-01]\n",
      "   [-2.62932801e+00 -2.67406321e+00 -4.78231716e+00 -1.90841138e+00\n",
      "    -5.73239513e-02]\n",
      "   [-3.46692443e-01  1.29117355e+02 -2.00952744e+00  2.19526329e+01\n",
      "     3.97245097e+00]]\n",
      "\n",
      "  [[-1.40715048e-01  2.68698072e+00 -1.79984033e-01 -2.28308197e-02\n",
      "    -2.29126401e-02]\n",
      "   [ 1.47362843e-01 -1.87731421e+00  3.98848877e+01  2.70257931e+01\n",
      "    -1.22424030e+00]\n",
      "   [-1.76364827e+00  4.00219574e+01 -1.48647916e+00  6.76592398e+00\n",
      "    -3.23432624e-01]\n",
      "   [-1.94756186e+00 -2.34607863e+00 -9.63091195e-01  3.00790215e+01\n",
      "    -2.23594260e+00]\n",
      "   [-2.47705269e+00  8.10108109e+01 -4.10131502e+00  8.98608112e+00\n",
      "    -2.82747865e-01]]]\n",
      "\n",
      "\n",
      " [[[-2.26282835e-01  3.76280236e+00 -8.78728181e-02 -4.93973009e-02\n",
      "    -6.34852573e-02]\n",
      "   [-1.44210899e+00 -9.75561798e-01 -6.02229118e-01  8.36148071e+00\n",
      "    -6.61256731e-01]\n",
      "   [-8.51942241e-01  2.61147060e+01  1.45718026e+00 -3.59411836e-02\n",
      "    -7.20444679e-01]\n",
      "   [-2.10923266e+00 -1.63957560e+00 -1.53863549e+00  1.49579072e+00\n",
      "    -9.08948839e-01]\n",
      "   [-1.53149140e+00  4.54392242e+01 -8.67174804e-01  5.50059605e+00\n",
      "    -1.25417018e+00]]\n",
      "\n",
      "  [[-1.12497382e-01  7.82845354e+00 -3.12964916e-01 -6.18538237e-04\n",
      "    -1.32582545e-01]\n",
      "   [-1.79526925e+00 -2.91130996e+00 -2.13500500e+00 -1.65151525e+00\n",
      "    -3.28077555e-01]\n",
      "   [-1.11172450e+00  7.96039734e+01 -1.64495313e+00  1.44200001e+01\n",
      "    -2.02874231e+00]\n",
      "   [-3.12523246e+00 -3.41882372e+00 -5.72180223e+00 -1.61092949e+00\n",
      "    -2.08479643e+00]\n",
      "   [-1.17295349e+00  1.30197388e+02 -2.56988978e+00  1.04660158e+01\n",
      "    -1.00866973e+00]]\n",
      "\n",
      "  [[-6.37915283e-02  6.24253273e+00 -2.48345375e-01 -1.11688510e-01\n",
      "    -2.46911988e-01]\n",
      "   [-1.30960965e+00 -9.29444253e-01  1.27504597e+01  8.31898594e+00\n",
      "    -6.86912060e-01]\n",
      "   [-7.40546882e-01  6.12491837e+01 -1.68155062e+00  2.88665509e+00\n",
      "    -1.27081859e+00]\n",
      "   [-1.29048336e+00 -1.88425767e+00 -1.77465117e+00  1.27417059e+01\n",
      "    -2.08659267e+00]\n",
      "   [-1.04982138e+00  9.35704498e+01 -1.85850430e+00  5.18290710e+00\n",
      "    -1.74585080e+00]]\n",
      "\n",
      "  [[ 2.29113474e-02  5.69216061e+00 -1.57942381e-02  1.13850915e+00\n",
      "    -1.41174465e-01]\n",
      "   [-3.26473927e+00 -4.17183012e-01  9.01459515e-01 -3.12079859e+00\n",
      "     5.24725819e+00]\n",
      "   [-7.10240126e-01  7.58732681e+01  3.08831167e+00  1.42440586e+01\n",
      "    -6.93632245e-01]\n",
      "   [-2.67338943e+00 -2.75596213e+00 -4.90671492e+00 -1.88600409e+00\n",
      "    -2.93496791e-02]\n",
      "   [-3.18973035e-01  1.32974701e+02 -2.07129240e+00  2.26270676e+01\n",
      "     4.05257320e+00]]\n",
      "\n",
      "  [[-1.48866743e-01  3.06523085e+00 -1.63371816e-01 -1.61834117e-02\n",
      "    -8.67498852e-03]\n",
      "   [ 4.58428681e-01 -1.91431367e+00  4.11788788e+01  2.82869263e+01\n",
      "    -1.24468410e+00]\n",
      "   [-1.80243587e+00  4.19804001e+01 -1.50438499e+00  6.93998814e+00\n",
      "    -3.80669892e-01]\n",
      "   [-1.92766893e+00 -2.37681293e+00 -9.57574368e-01  3.15150108e+01\n",
      "    -2.26883626e+00]\n",
      "   [-2.44943213e+00  8.33246765e+01 -4.22015285e+00  9.61482239e+00\n",
      "    -2.86149949e-01]]]], shape=(3, 5, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-6.59006983e-02 -2.58060843e-01  1.67156744e+00  3.22404839e-02\n",
      "    -5.07203229e-02]\n",
      "   [-1.46771801e+00 -1.59181744e-01 -1.65427208e+00  1.87273750e+01\n",
      "     1.00174150e+01]\n",
      "   [-1.12557507e+00 -1.57166994e+00 -1.47716737e+00  6.48385572e+00\n",
      "    -1.00935213e-01]\n",
      "   [-4.51049852e+00 -2.47063026e-01 -1.61254871e+00  2.33627014e+01\n",
      "     1.08975315e+01]\n",
      "   [-1.19772208e+00 -1.66813505e+00  4.25048018e+00  2.62442732e+00\n",
      "    -1.80054033e+00]]\n",
      "\n",
      "  [[-1.43349990e-01 -8.36265385e-02  4.11062002e-01  1.03029680e+00\n",
      "     1.24172306e+00]\n",
      "   [-3.43193316e+00  1.46939049e+01 -3.59754443e+00  8.53072815e+01\n",
      "    -1.08539164e-01]\n",
      "   [-8.62648308e-01  3.06694722e+00 -3.72281909e+00  9.60744629e+01\n",
      "     1.10036116e+01]\n",
      "   [ 2.73833704e+00  2.12041550e+01 -7.84725332e+00  6.77796021e+01\n",
      "     1.48927498e+01]\n",
      "   [-4.15470457e+00  1.27490640e+00  9.51387596e+00  1.06544275e+01\n",
      "    -1.38302553e+00]]\n",
      "\n",
      "  [[-8.95617977e-02 -1.82764724e-01  1.12770128e+00 -1.57735676e-01\n",
      "    -5.06086648e-02]\n",
      "   [ 1.60672283e+01 -1.05230141e+00  5.17369413e+00  5.17913857e+01\n",
      "    -9.64587688e-01]\n",
      "   [ 4.94714975e+00 -1.30828965e+00 -3.52834058e+00  1.33756828e+01\n",
      "    -1.29940283e+00]\n",
      "   [-3.56091857e+00 -2.46802807e+00  5.66886139e+00  9.57975388e+01\n",
      "    -1.59639180e-01]\n",
      "   [-2.10496545e+00 -3.87314081e-01  2.92914619e+01 -3.91765666e+00\n",
      "    -7.52042174e-01]]\n",
      "\n",
      "  [[-1.92562684e-01 -3.03358436e-01 -1.29908890e-01  1.74752164e+00\n",
      "    -2.27128610e-01]\n",
      "   [-5.96109629e+00  3.53912201e+01 -4.44151449e+00  1.72853546e+02\n",
      "    -5.38394403e+00]\n",
      "   [-1.81507516e+00  5.40152550e+01 -6.10801077e+00  1.23743301e+02\n",
      "    -2.31411076e+00]\n",
      "   [-6.87749863e+00  4.37972794e+01 -1.14252443e+01  1.15797684e+02\n",
      "    -3.45146680e+00]\n",
      "   [-4.83400774e+00  1.82747211e+01  2.46945343e+01  7.42085571e+01\n",
      "    -3.77905536e+00]]\n",
      "\n",
      "  [[-2.61895746e-01 -2.55852669e-01  1.15948331e+00 -7.53228590e-02\n",
      "    -6.59500286e-02]\n",
      "   [-4.62598705e+00  6.86088979e-01 -2.67728424e+00  8.76071320e+01\n",
      "     2.67459126e+01]\n",
      "   [-1.22646868e+00 -3.54526067e+00 -5.30421925e+00  4.48581657e+01\n",
      "     6.25234079e+00]\n",
      "   [-1.29870558e+01  5.92622328e+00 -4.08416986e+00  1.13924202e+02\n",
      "     3.32772522e+01]\n",
      "   [-4.81121445e+00 -2.35748935e+00  1.44737415e+01 -2.66659021e+00\n",
      "    -4.60512877e+00]]]\n",
      "\n",
      "\n",
      " [[[-8.18804726e-02 -3.09935421e-01  1.86215079e+00  1.03686370e-01\n",
      "    -6.37215599e-02]\n",
      "   [-1.69652784e+00 -2.13090703e-01 -1.82986343e+00  2.10088692e+01\n",
      "     1.11047077e+01]\n",
      "   [-1.26653564e+00 -1.77713203e+00 -1.65342200e+00  7.17695045e+00\n",
      "    -1.10848762e-01]\n",
      "   [-5.08657980e+00 -2.99159497e-01 -1.79563797e+00  2.60297585e+01\n",
      "     1.23639507e+01]\n",
      "   [-1.34686708e+00 -1.90083492e+00  4.76321411e+00  2.80732226e+00\n",
      "    -2.00268984e+00]]\n",
      "\n",
      "  [[-1.78449854e-01 -9.88993868e-02  4.52287108e-01  1.29104853e+00\n",
      "     1.45551634e+00]\n",
      "   [-3.83641219e+00  1.59731703e+01 -4.02015257e+00  9.51925583e+01\n",
      "    -6.51278943e-02]\n",
      "   [-1.03705919e+00  3.10674715e+00 -4.11348009e+00  1.07606018e+02\n",
      "     1.27160568e+01]\n",
      "   [ 2.87996340e+00  2.33437481e+01 -8.75478268e+00  7.56898575e+01\n",
      "     1.67867432e+01]\n",
      "   [-4.65074539e+00  1.52088940e+00  1.05128183e+01  1.21791945e+01\n",
      "    -1.51164639e+00]]\n",
      "\n",
      "  [[-1.23057961e-01 -2.15443239e-01  1.36491334e+00 -1.85946435e-01\n",
      "    -6.65946230e-02]\n",
      "   [ 1.75946465e+01 -1.20438945e+00  5.86401224e+00  5.78550873e+01\n",
      "    -1.08973145e+00]\n",
      "   [ 5.44819689e+00 -1.50269687e+00 -3.96009374e+00  1.47839985e+01\n",
      "    -1.47805429e+00]\n",
      "   [-4.08751488e+00 -2.74075270e+00  5.33175087e+00  1.07165939e+02\n",
      "    -1.57279328e-01]\n",
      "   [-2.38734126e+00 -4.22677845e-01  3.26450691e+01 -4.37647200e+00\n",
      "    -8.45171154e-01]]\n",
      "\n",
      "  [[-2.46392652e-01 -3.69219750e-01 -1.62264496e-01  2.15835500e+00\n",
      "    -2.86088705e-01]\n",
      "   [-6.61909580e+00  3.93907738e+01 -4.92009068e+00  1.91987885e+02\n",
      "    -6.00160933e+00]\n",
      "   [-2.13765788e+00  5.97294540e+01 -6.79853392e+00  1.38273621e+02\n",
      "    -2.63284183e+00]\n",
      "   [-7.68944263e+00  4.83520622e+01 -1.27476225e+01  1.28464142e+02\n",
      "    -3.84135294e+00]\n",
      "   [-5.43848276e+00  1.98794670e+01  2.72111073e+01  8.19536743e+01\n",
      "    -4.22848272e+00]]\n",
      "\n",
      "  [[-3.25007349e-01 -3.10747117e-01  1.32863092e+00 -9.14657637e-02\n",
      "    -8.20469186e-02]\n",
      "   [-5.32951832e+00  4.07786101e-01 -2.99337387e+00  9.68769608e+01\n",
      "     3.01154079e+01]\n",
      "   [-1.40235472e+00 -3.96883440e+00 -5.91612911e+00  4.92012138e+01\n",
      "     7.24217463e+00]\n",
      "   [-1.45500593e+01  6.29351425e+00 -4.64433813e+00  1.26240852e+02\n",
      "     3.78638382e+01]\n",
      "   [-5.39638042e+00 -2.69931674e+00  1.56347713e+01 -2.96672845e+00\n",
      "    -5.10468149e+00]]]\n",
      "\n",
      "\n",
      " [[[-8.39794353e-02 -3.29021156e-01  2.03567147e+00  8.47334117e-02\n",
      "    -6.54304028e-02]\n",
      "   [-1.77605140e+00 -2.09736779e-01 -1.92793524e+00  2.17807980e+01\n",
      "     1.16065617e+01]\n",
      "   [-1.31817341e+00 -1.85135591e+00 -1.72042406e+00  7.46046448e+00\n",
      "    -1.22107044e-01]\n",
      "   [-5.33041239e+00 -3.18614513e-01 -1.88420868e+00  2.74027767e+01\n",
      "     1.29927883e+01]\n",
      "   [-1.40110874e+00 -1.96428871e+00  4.87856960e+00  2.95229983e+00\n",
      "    -2.07762337e+00]]\n",
      "\n",
      "  [[-1.79353938e-01 -1.01256885e-01  5.13837636e-01  1.49048197e+00\n",
      "     1.45317066e+00]\n",
      "   [-3.99792361e+00  1.67466564e+01 -4.18582869e+00  9.85239716e+01\n",
      "    -8.89048278e-02]\n",
      "   [-1.08778524e+00  3.58555436e+00 -4.29055119e+00  1.11976616e+02\n",
      "     1.31938467e+01]\n",
      "   [ 2.81385136e+00  2.45954208e+01 -9.13290215e+00  7.90706558e+01\n",
      "     1.70957031e+01]\n",
      "   [-4.84943914e+00  1.73615205e+00  1.06418018e+01  1.29479122e+01\n",
      "    -1.55411911e+00]]\n",
      "\n",
      "  [[-1.31439328e-01 -2.17969134e-01  1.51255929e+00 -1.93851456e-01\n",
      "    -7.49539137e-02]\n",
      "   [ 1.80595074e+01 -1.22298765e+00  6.39097404e+00  6.00630760e+01\n",
      "    -1.13751602e+00]\n",
      "   [ 5.89011049e+00 -1.53057802e+00 -4.13497448e+00  1.55079479e+01\n",
      "    -1.52263689e+00]\n",
      "   [-4.20246887e+00 -2.83388925e+00  5.67149067e+00  1.12309334e+02\n",
      "    -1.50578395e-01]\n",
      "   [-2.47606373e+00 -3.85245353e-01  3.36247520e+01 -4.47294569e+00\n",
      "    -8.87083054e-01]]\n",
      "\n",
      "  [[-2.53997266e-01 -3.64269286e-01 -1.71480343e-01  2.34291434e+00\n",
      "    -3.03535491e-01]\n",
      "   [-6.84554052e+00  4.05265198e+01 -5.09322071e+00  1.97958420e+02\n",
      "    -6.17265797e+00]\n",
      "   [-2.23212004e+00  6.16309395e+01 -7.06280375e+00  1.43153198e+02\n",
      "    -2.70063281e+00]\n",
      "   [-7.95120716e+00  4.97576714e+01 -1.32143888e+01  1.33554871e+02\n",
      "    -3.94706964e+00]\n",
      "   [-5.64060354e+00  2.12611160e+01  2.75330849e+01  8.54886246e+01\n",
      "    -4.31675339e+00]]\n",
      "\n",
      "  [[-3.53016824e-01 -3.24433059e-01  1.25557101e+00 -9.41482559e-02\n",
      "    -8.46771076e-02]\n",
      "   [-5.52306604e+00  3.42995720e-03 -3.03612876e+00  9.99446869e+01\n",
      "     3.11983719e+01]\n",
      "   [-1.39908731e+00 -4.09272432e+00 -6.15833569e+00  5.12708740e+01\n",
      "     7.32659101e+00]\n",
      "   [-1.49761810e+01  6.34427357e+00 -4.79753590e+00  1.30631714e+02\n",
      "     3.91176033e+01]\n",
      "   [-5.56697226e+00 -2.75593162e+00  1.57267647e+01 -3.07258296e+00\n",
      "    -5.25330305e+00]]]], shape=(3, 5, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-5.22097424e-02  1.12353712e-02 -6.68548644e-02 -7.02728182e-02\n",
      "    -3.77779757e-03]\n",
      "   [-9.41805184e-01 -9.91023839e-01 -1.06460762e+00 -1.49371445e+00\n",
      "    -5.79697788e-01]\n",
      "   [-2.21053338e+00 -7.10209846e-01 -2.24337864e+00 -1.07835078e+00\n",
      "     5.32354403e+00]\n",
      "   [-1.07693136e+00 -1.55750740e+00 -3.43223643e+00 -4.64303684e+00\n",
      "     2.49875832e+00]\n",
      "   [-9.77781951e-01  3.84718871e+00 -3.18404555e+00 -3.90816736e+00\n",
      "    -7.86105394e-02]]\n",
      "\n",
      "  [[-5.89245223e-02 -5.94953708e-02 -1.53092727e-01 -1.59566492e-01\n",
      "    -8.27608556e-02]\n",
      "   [-1.33857751e+00 -2.57028461e+00 -2.23797584e+00 -7.56600666e+00\n",
      "    -2.05093312e+00]\n",
      "   [-2.06887031e+00 -2.81082320e+00 -6.57951307e+00 -8.16923714e+00\n",
      "    -2.69267702e+00]\n",
      "   [-5.09508324e+00 -7.74884892e+00 -4.97548056e+00 -7.49783325e+00\n",
      "    -6.76234674e+00]\n",
      "   [-4.91353130e+00 -3.69480181e+00 -1.24880629e+01 -1.11942053e+01\n",
      "    -4.55526114e+00]]\n",
      "\n",
      "  [[-1.29384503e-01 -1.08388007e-01 -2.08459884e-01 -1.05819277e-01\n",
      "     7.54909396e-01]\n",
      "   [-2.41128373e+00 -6.34236097e+00 -4.21714592e+00 -7.16966629e+00\n",
      "    -3.22764015e+00]\n",
      "   [-6.52056742e+00 -1.64727044e+00 -8.91827488e+00 -5.73056889e+00\n",
      "     7.07230473e+00]\n",
      "   [-3.48090053e+00 -1.02628870e+01 -9.76341724e+00 -1.38348713e+01\n",
      "    -2.31940532e+00]\n",
      "   [-8.38354778e+00 -5.28286600e+00 -1.51449041e+01 -9.97095585e+00\n",
      "     9.29749489e+00]]\n",
      "\n",
      "  [[ 1.84023902e-02 -5.55319004e-02 -8.55157599e-02 -2.41962239e-01\n",
      "     1.45436421e-01]\n",
      "   [-1.33648145e+00 -9.83365476e-01 -3.91560602e+00 -4.94949293e+00\n",
      "    -4.41685057e+00]\n",
      "   [-2.52998829e+00 -3.82207155e+00 -8.66215897e+00 -6.76377583e+00\n",
      "    -1.51092398e+00]\n",
      "   [-6.74330091e+00 -2.97041154e+00 -9.58806515e+00 -8.58834171e+00\n",
      "    -5.77477217e+00]\n",
      "   [-3.87431312e+00 -7.31710291e+00 -1.56069899e+01 -1.31711655e+01\n",
      "    -1.08794582e+00]]\n",
      "\n",
      "  [[-7.78602138e-02 -7.26011842e-02 -2.65559524e-01 -1.03746377e-01\n",
      "    -2.98930556e-02]\n",
      "   [-2.52417874e+00 -6.23019314e+00 -5.90248871e+00 -6.28485441e+00\n",
      "    -2.80696416e+00]\n",
      "   [-2.81410122e+00 -1.81839812e+00 -1.10558500e+01 -7.07624912e+00\n",
      "     8.48789692e+00]\n",
      "   [-6.03288031e+00 -6.65562010e+00 -9.64623547e+00 -1.52872972e+01\n",
      "    -1.24633169e+00]\n",
      "   [-5.17607069e+00 -8.50643921e+00 -2.00380421e+01 -1.19216385e+01\n",
      "    -2.41217542e+00]]]\n",
      "\n",
      "\n",
      " [[[-6.39951974e-02 -4.48658504e-03 -8.68927911e-02 -8.97693858e-02\n",
      "    -6.28917827e-04]\n",
      "   [-1.05825281e+00 -1.12703967e+00 -1.19601977e+00 -1.71355939e+00\n",
      "    -6.55957341e-01]\n",
      "   [-2.49442840e+00 -8.12343717e-01 -2.53471303e+00 -1.23285472e+00\n",
      "     6.05354738e+00]\n",
      "   [-1.21493709e+00 -1.75664139e+00 -3.86749125e+00 -5.25581551e+00\n",
      "     2.80862737e+00]\n",
      "   [-1.09458125e+00  4.19691133e+00 -3.58690763e+00 -4.41335773e+00\n",
      "    -7.22516850e-02]]\n",
      "\n",
      "  [[-7.32452795e-02 -8.16855282e-02 -1.95819631e-01 -2.14552566e-01\n",
      "    -1.06059566e-01]\n",
      "   [-1.53323555e+00 -2.88421392e+00 -2.50822806e+00 -8.47465324e+00\n",
      "    -2.31940722e+00]\n",
      "   [-2.34793258e+00 -3.17437696e+00 -7.38525152e+00 -9.17891598e+00\n",
      "    -3.03120208e+00]\n",
      "   [-5.72479200e+00 -8.66080761e+00 -5.54538298e+00 -8.37940311e+00\n",
      "    -7.58558416e+00]\n",
      "   [-5.47339392e+00 -4.11582136e+00 -1.40201159e+01 -1.25470333e+01\n",
      "    -5.11284781e+00]]\n",
      "\n",
      "  [[-1.66019067e-01 -1.44045278e-01 -2.69915968e-01 -1.35576114e-01\n",
      "     9.68457043e-01]\n",
      "   [-2.70736647e+00 -7.11272049e+00 -4.73616743e+00 -8.06466293e+00\n",
      "    -3.62575030e+00]\n",
      "   [-7.28834105e+00 -1.89272368e+00 -9.99538612e+00 -6.42962790e+00\n",
      "     7.99395561e+00]\n",
      "   [-3.87030411e+00 -1.15133114e+01 -1.09349260e+01 -1.54971943e+01\n",
      "    -2.61551881e+00]\n",
      "   [-9.40975285e+00 -5.95365906e+00 -1.69887371e+01 -1.12053347e+01\n",
      "     1.04228973e+01]]\n",
      "\n",
      "  [[ 2.03543715e-03 -7.44469687e-02 -1.15099363e-01 -3.18459123e-01\n",
      "     1.62037954e-01]\n",
      "   [-1.52057660e+00 -1.15326095e+00 -4.38559866e+00 -5.56863022e+00\n",
      "    -4.92035389e+00]\n",
      "   [-2.80858707e+00 -4.30131292e+00 -9.71790409e+00 -7.59038544e+00\n",
      "    -1.69775546e+00]\n",
      "   [-7.51289988e+00 -3.32745099e+00 -1.07307549e+01 -9.60369015e+00\n",
      "    -6.49128675e+00]\n",
      "   [-4.34288549e+00 -8.16887188e+00 -1.74488754e+01 -1.48022604e+01\n",
      "    -1.20981002e+00]]\n",
      "\n",
      "  [[-9.71851796e-02 -9.72235426e-02 -3.46197039e-01 -1.43319979e-01\n",
      "    -3.79193015e-02]\n",
      "   [-2.81691432e+00 -6.99790812e+00 -6.61197901e+00 -7.07551527e+00\n",
      "    -3.14761233e+00]\n",
      "   [-3.18764281e+00 -2.07018256e+00 -1.23831329e+01 -7.90694189e+00\n",
      "     9.51103687e+00]\n",
      "   [-6.74001169e+00 -7.50463057e+00 -1.07858467e+01 -1.71392193e+01\n",
      "    -1.38718939e+00]\n",
      "   [-5.78550673e+00 -9.56496716e+00 -2.24250698e+01 -1.33536882e+01\n",
      "    -2.69874930e+00]]]\n",
      "\n",
      "\n",
      " [[[-6.52840957e-02 -2.96173990e-03 -8.88563842e-02 -9.32419151e-02\n",
      "    -1.01534987e-03]\n",
      "   [-1.10904074e+00 -1.17267501e+00 -1.24926162e+00 -1.78368509e+00\n",
      "    -6.83857143e-01]\n",
      "   [-2.60427880e+00 -8.44060421e-01 -2.64371395e+00 -1.28741157e+00\n",
      "     6.31100416e+00]\n",
      "   [-1.26524067e+00 -1.83979571e+00 -4.02944660e+00 -5.48317957e+00\n",
      "     2.90919256e+00]\n",
      "   [-1.14840221e+00  4.35446405e+00 -3.75280356e+00 -4.59892035e+00\n",
      "    -7.58684501e-02]]\n",
      "\n",
      "  [[-7.39094764e-02 -8.55615363e-02 -2.04245716e-01 -2.26373151e-01\n",
      "    -1.11703947e-01]\n",
      "   [-1.59231031e+00 -3.01309824e+00 -2.60714912e+00 -8.83451653e+00\n",
      "    -2.41094518e+00]\n",
      "   [-2.44372058e+00 -3.31398392e+00 -7.69253111e+00 -9.57151699e+00\n",
      "    -3.14299059e+00]\n",
      "   [-5.95598745e+00 -9.03069019e+00 -5.77970648e+00 -8.73299694e+00\n",
      "    -7.90875769e+00]\n",
      "   [-5.71729612e+00 -4.28387928e+00 -1.46229296e+01 -1.30811529e+01\n",
      "    -5.32691383e+00]]\n",
      "\n",
      "  [[-1.75547838e-01 -1.49349198e-01 -2.83044010e-01 -1.42192975e-01\n",
      "     1.03065050e+00]\n",
      "   [-2.82593751e+00 -7.40703535e+00 -4.93137646e+00 -8.41499138e+00\n",
      "    -3.77795219e+00]\n",
      "   [-7.59014130e+00 -1.97227514e+00 -1.04083052e+01 -6.71185446e+00\n",
      "     8.38330936e+00]\n",
      "   [-4.05958509e+00 -1.19701967e+01 -1.14008875e+01 -1.61306629e+01\n",
      "    -2.70878744e+00]\n",
      "   [-9.80212116e+00 -6.20289373e+00 -1.77106285e+01 -1.16673880e+01\n",
      "     1.07347832e+01]]\n",
      "\n",
      "  [[ 2.70040724e-02 -7.71634132e-02 -1.25694677e-01 -3.37404281e-01\n",
      "     2.00072840e-01]\n",
      "   [-1.58792770e+00 -1.20353007e+00 -4.55664587e+00 -5.79100990e+00\n",
      "    -5.12284374e+00]\n",
      "   [-2.92833114e+00 -4.47533703e+00 -1.01028070e+01 -7.91650867e+00\n",
      "    -1.76394558e+00]\n",
      "   [-7.82448530e+00 -3.43981051e+00 -1.11616945e+01 -1.00005770e+01\n",
      "    -6.75295973e+00]\n",
      "   [-4.53548908e+00 -8.52883530e+00 -1.81799145e+01 -1.54074984e+01\n",
      "    -1.26646209e+00]]\n",
      "\n",
      "  [[-1.05664209e-01 -1.04938224e-01 -3.67439806e-01 -1.54707476e-01\n",
      "    -3.98709141e-02]\n",
      "   [-2.93139887e+00 -7.28247166e+00 -6.88481140e+00 -7.35567427e+00\n",
      "    -3.28720355e+00]\n",
      "   [-3.30359244e+00 -2.15913272e+00 -1.28875418e+01 -8.23927116e+00\n",
      "     9.99759579e+00]\n",
      "   [-7.03285933e+00 -7.78545713e+00 -1.12538929e+01 -1.78096390e+01\n",
      "    -1.45702827e+00]\n",
      "   [-6.04229927e+00 -9.95066452e+00 -2.33302517e+01 -1.38860626e+01\n",
      "    -2.81443667e+00]]]], shape=(3, 5, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[ 5.37658483e-02  7.42142126e-02  1.10105872e-02]\n",
      "   [-2.39586636e-01 -2.53754742e-02 -2.42443785e-01]\n",
      "   [ 1.02883959e+00 -7.84484595e-02 -9.25035477e-02]\n",
      "   [-9.67009664e-02 -1.17907584e-01 -1.84413135e-01]\n",
      "   [-4.08031315e-01 -3.11296582e-01 -3.29658717e-01]]\n",
      "\n",
      "  [[-2.42932197e-02 -9.01451614e-03 -1.47909001e-02]\n",
      "   [-1.16287220e+00 -9.52660024e-01 -9.75441873e-01]\n",
      "   [-1.64268994e+00 -5.35337389e-01 -1.04538858e+00]\n",
      "   [-2.49795914e+00 -1.41794097e+00 -1.81447434e+00]\n",
      "   [-2.67888546e+00 -1.46233118e+00 -1.17131579e+00]]\n",
      "\n",
      "  [[-3.26027423e-02 -1.23745045e-02 -2.10204646e-02]\n",
      "   [-1.08063364e+00 -2.03448817e-01 -7.80892074e-01]\n",
      "   [-2.03392005e+00 -1.02208602e+00 -1.29314220e+00]\n",
      "   [-2.01428151e+00 -8.87483120e-01 -1.41300046e+00]\n",
      "   [-2.89537334e+00 -1.53198421e+00 -1.94183767e+00]]\n",
      "\n",
      "  [[-4.73595150e-02 -2.12415159e-02 -2.85730362e-02]\n",
      "   [-2.26827049e+00 -1.23068690e+00 -2.15172410e+00]\n",
      "   [-2.72537589e+00 -7.01524794e-01 -1.70871663e+00]\n",
      "   [-4.76657629e+00 -3.55528617e+00 -4.24688482e+00]\n",
      "   [-5.87093163e+00 -2.85954738e+00 -3.00927281e+00]]\n",
      "\n",
      "  [[-3.56479324e-02 -2.35259365e-02 -2.21133549e-02]\n",
      "   [-1.45817232e+00 -1.28835887e-01 -1.19820428e+00]\n",
      "   [-1.27461112e+00 -1.22942483e+00 -1.51124823e+00]\n",
      "   [-2.71861053e+00 -6.28959417e-01 -2.20608616e+00]\n",
      "   [-4.66595840e+00 -2.78924656e+00 -3.29008484e+00]]]\n",
      "\n",
      "\n",
      " [[[ 5.91704110e-03  3.59308384e-02 -1.65594497e-03]\n",
      "   [-2.73286670e-01 -3.23098749e-02 -2.76290625e-01]\n",
      "   [ 1.14875972e+00 -9.30209458e-02 -1.04787908e-01]\n",
      "   [-1.11039422e-01 -1.34751201e-01 -2.08487183e-01]\n",
      "   [-4.66623396e-01 -3.57225865e-01 -3.75902981e-01]]\n",
      "\n",
      "  [[-3.86877470e-02 -1.51162492e-02 -2.35073138e-02]\n",
      "   [-1.31953895e+00 -1.08304286e+00 -1.10497987e+00]\n",
      "   [-1.85450387e+00 -6.11030340e-01 -1.17732799e+00]\n",
      "   [-2.81292486e+00 -1.58350337e+00 -2.03906083e+00]\n",
      "   [-3.00950408e+00 -1.64450741e+00 -1.31603360e+00]]\n",
      "\n",
      "  [[-4.91026416e-02 -2.04877183e-02 -3.09428554e-02]\n",
      "   [-1.22252023e+00 -2.31553391e-01 -8.82826328e-01]\n",
      "   [-2.28514457e+00 -1.15111065e+00 -1.45789266e+00]\n",
      "   [-2.26401019e+00 -1.00631368e+00 -1.58864951e+00]\n",
      "   [-3.24584174e+00 -1.72355640e+00 -2.17856145e+00]]\n",
      "\n",
      "  [[-6.86016157e-02 -3.21490057e-02 -4.01115902e-02]\n",
      "   [-2.55751514e+00 -1.39144123e+00 -2.42174721e+00]\n",
      "   [-3.06960011e+00 -7.98294544e-01 -1.92242849e+00]\n",
      "   [-5.35090780e+00 -3.99427295e+00 -4.75692511e+00]\n",
      "   [-6.58103418e+00 -3.20823455e+00 -3.37254596e+00]]\n",
      "\n",
      "  [[-5.07298186e-02 -3.49800438e-02 -3.11414003e-02]\n",
      "   [-1.64793146e+00 -1.48846433e-01 -1.35335088e+00]\n",
      "   [-1.44902933e+00 -1.38748038e+00 -1.70163274e+00]\n",
      "   [-3.05898166e+00 -7.09291577e-01 -2.47673845e+00]\n",
      "   [-5.23803329e+00 -3.13335109e+00 -3.69251251e+00]]]\n",
      "\n",
      "\n",
      " [[[ 1.72005240e-02  3.86476815e-02 -1.19830656e-03]\n",
      "   [-2.83659637e-01 -3.43564227e-02 -2.87914962e-01]\n",
      "   [ 1.22246933e+00 -9.66974944e-02 -1.09060980e-01]\n",
      "   [-1.14194535e-01 -1.41752899e-01 -2.17246801e-01]\n",
      "   [-4.86327469e-01 -3.73346657e-01 -3.92371267e-01]]\n",
      "\n",
      "  [[-3.84464487e-02 -1.51193915e-02 -2.33442783e-02]\n",
      "   [-1.36982846e+00 -1.12568963e+00 -1.14904821e+00]\n",
      "   [-1.92765892e+00 -6.32633805e-01 -1.22534835e+00]\n",
      "   [-2.93097401e+00 -1.65029812e+00 -2.12583232e+00]\n",
      "   [-3.13541436e+00 -1.71346974e+00 -1.37265468e+00]]\n",
      "\n",
      "  [[-4.93638478e-02 -2.06718128e-02 -3.13285701e-02]\n",
      "   [-1.27047622e+00 -2.41722330e-01 -9.18225765e-01]\n",
      "   [-2.37735820e+00 -1.19870424e+00 -1.51548088e+00]\n",
      "   [-2.35751152e+00 -1.04495537e+00 -1.65548635e+00]\n",
      "   [-3.38321495e+00 -1.79725158e+00 -2.27046537e+00]]\n",
      "\n",
      "  [[-7.14291781e-02 -3.39696556e-02 -4.16567065e-02]\n",
      "   [-2.66268182e+00 -1.44591355e+00 -2.52026677e+00]\n",
      "   [-3.19349575e+00 -8.30210030e-01 -2.00014019e+00]\n",
      "   [-5.57272577e+00 -4.15930223e+00 -4.95524168e+00]\n",
      "   [-6.85794783e+00 -3.34198356e+00 -3.51434398e+00]]\n",
      "\n",
      "  [[-5.37495613e-02 -3.69603187e-02 -3.28770541e-02]\n",
      "   [-1.71574938e+00 -1.55407548e-01 -1.40849614e+00]\n",
      "   [-1.50507164e+00 -1.44409311e+00 -1.77015555e+00]\n",
      "   [-3.18543696e+00 -7.37381518e-01 -2.57954335e+00]\n",
      "   [-5.45714712e+00 -3.26396441e+00 -3.84646225e+00]]]], shape=(3, 5, 5, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 4.2369 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "cf8cda21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator_14/dense_block_14/dense_90/kernel:0 [[-0.01977373  0.0257942   0.02203238 ...  0.04614797  0.03235683\n",
      "  -0.03627039]\n",
      " [ 0.02929055 -0.05542327 -0.03276846 ... -0.00220141 -0.04890126\n",
      "  -0.03045244]\n",
      " [ 0.01022856 -0.05964788 -0.0528914  ... -0.00563824  0.03960341\n",
      "  -0.03896577]\n",
      " ...\n",
      " [ 0.03309955 -0.00184861  0.01047124 ...  0.04798859  0.08177074\n",
      "  -0.05541556]\n",
      " [-0.03271682 -0.02365159 -0.01691599 ... -0.02826412 -0.02848577\n",
      "   0.00127587]\n",
      " [-0.03394162 -0.02770595  0.04155768 ... -0.00242701 -0.03895332\n",
      "   0.02669442]]\n",
      "generator_14/dense_block_14/dense_90/bias:0 [-0.00803091 -0.00534232 -0.00174159 ... -0.01185318  0.00154424\n",
      " -0.0158552 ]\n",
      "generator_14/dense_block_14/batch_normalization_153/gamma:0 [0.950655   0.9673666  0.9671377  ... 0.92883193 0.98732436 0.94109476]\n",
      "generator_14/dense_block_14/batch_normalization_153/beta:0 [-0.00784559 -0.00570957 -0.00161753 ... -0.01253364  0.00215867\n",
      " -0.01634489]\n",
      "generator_14/dense_block_14/batch_normalization_153/moving_mean:0 [0. 0. 0. ... 0. 0. 0.]\n",
      "generator_14/dense_block_14/batch_normalization_153/moving_variance:0 [1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "for var in generator.starter.variables:\n",
    "    print(var.name, var.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "1ca7c064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Courses\\24aut_deep_learning\\24aut-deep-learning\\deep-learning-comp3\\testing\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "--------------Evaluation Success-----------------\n",
      "C:\\Users\\User\\Courses\\24aut_deep_learning\\24aut-deep-learning\\deep-learning-comp3\n"
     ]
    }
   ],
   "source": [
    "%cd ./testing\n",
    "!python inception_score.py ../inference/demo output.csv 21\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "fa35e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_encoder_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    multiple                  0 (unused)\n",
      "                                                                 \n",
      " gru_14 (GRU)                multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "efcd9b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_28 (Flatten)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_88 (Dense)            multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_89 (Dense)            multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_block_14 (dense_block  multiple                 1053696   \n",
      " )                                                               \n",
      "                                                                 \n",
      " deconv_block_70 (deconv_blo  multiple                 1049856   \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " deconv_block_71 (deconv_blo  multiple                 524928    \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " deconv_block_72 (deconv_blo  multiple                 131392    \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " deconv_block_73 (deconv_blo  multiple                 32928     \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " deconv_block_74 (deconv_blo  multiple                 8272      \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " conv_block_76 (conv_block)  multiple                  63        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,801,135\n",
      "Trainable params: 2,798,089\n",
      "Non-trainable params: 3,046\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "7d6f6c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_block_77 (conv_block)  multiple                  8192      \n",
      "                                                                 \n",
      " conv_block_78 (conv_block)  multiple                  147776    \n",
      "                                                                 \n",
      " flatten_29 (Flatten)        multiple                  0         \n",
      "                                                                 \n",
      " dense_91 (Dense)            multiple                  65664     \n",
      "                                                                 \n",
      " dense_92 (Dense)            multiple                  33554560  \n",
      "                                                                 \n",
      " dense_93 (Dense)            multiple                  257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,776,449\n",
      "Trainable params: 33,775,809\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f10b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f537e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b128e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a11e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865cbfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d63bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a191dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
