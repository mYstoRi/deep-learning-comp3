{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a26ad83",
   "metadata": {},
   "source": [
    "# Competition 3: Team 21\n",
    "\n",
    "112062649 王俊皓\n",
    "\n",
    "112062650 廖士傑\n",
    "\n",
    "##  Reverse Image Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414f827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class experimental_settings:\n",
    "    def __init__(self,\n",
    "                 enc=True,\n",
    "                 gen=True,\n",
    "                 dis=True,\n",
    "                 enc_do_batchnorm=False,\n",
    "                 delete_checkpoint=False):\n",
    "        self.enc = enc\n",
    "        self.gen = gen\n",
    "        self.dis = dis\n",
    "        self.enc_do_batchnorm = enc_do_batchnorm\n",
    "        self.delete_checkpoint = delete_checkpoint # not implemented yet\n",
    "        \n",
    "        # ============================ #\n",
    "        # automatic\n",
    "        # ============================ #\n",
    "        \n",
    "        self.caption_type = 'sentence' if self.enc else 'id'\n",
    "\n",
    "\n",
    "expSettings = experimental_settings(enc=True,\n",
    "                                    gen=True,\n",
    "                                    dis=True,\n",
    "                                    delete_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32322ac2",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15576cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7b646",
   "metadata": {},
   "source": [
    "GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d992c613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71f7c6",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58104ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf3f5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14b0e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n",
      "tf.Tensor(b'the flower shown has yellow anther red pistil and bright red petals <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def id2Sent(ids):\n",
    "    return \" \".join([id2word_dict[idx] for idx in ids]).strip()\n",
    "\n",
    "#def batch_id2Sent(batch_ids):\n",
    "    #return [id2Sent(ids) for ids in batch_ids]\n",
    "    \n",
    "def batch_id2Sent(batch_ids):\n",
    "    def process_single(ids):\n",
    "        # Convert a single tensor of IDs to a sentence\n",
    "        ids = ids.numpy()  # Convert Tensor to NumPy\n",
    "        sentence = \" \".join([id2word_dict.get(idx, \"<UNK>\") for idx in ids])  # Handle unknown IDs\n",
    "        return sentence\n",
    "\n",
    "    # Use tf.py_function to apply Python function inside the TensorFlow graph\n",
    "    sentences = tf.map_fn(\n",
    "        lambda ids: tf.py_function(process_single, [ids], tf.string),\n",
    "        batch_ids,\n",
    "        fn_output_signature=tf.string\n",
    "    )\n",
    "    return sentences\n",
    "\n",
    "\n",
    "print(sent2IdList(text))\n",
    "print(id2Sent(sent2IdList(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23133915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "text2ImgData = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(text2ImgData)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c49264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption2string(cap):\n",
    "    output = []\n",
    "    for sen in cap:\n",
    "        s = \" \".join([id2word_dict[idx] for idx in sen]).strip()\n",
    "        output.append(s.split(' <PAD>')[0])\n",
    "    return output\n",
    "\n",
    "# adding caption as strings\n",
    "text2ImgData['Captions_string'] = text2ImgData['Captions'].apply(caption2string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf9314c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>Captions_string</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \\\n",
       "ID                                   \n",
       "6734  ./102flowers/image_06734.jpg   \n",
       "6736  ./102flowers/image_06736.jpg   \n",
       "6737  ./102flowers/image_06737.jpg   \n",
       "6738  ./102flowers/image_06738.jpg   \n",
       "6739  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                        Captions_string  \n",
       "ID                                                       \n",
       "6734  [the petals of the flower are pink in color an...  \n",
       "6736  [this flower has white petals and yellow pisti...  \n",
       "6737  [the petals on this flower are pink with white...  \n",
       "6738  [the flower has a smooth purple petal with whi...  \n",
       "6739  [this white flower has bright yellow stamen wi...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2ImgData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a6b09d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the petals of the flower are pink in color and have a yellow center',\n",
       "  'this flower is pink and white in color with petals that are multi colored',\n",
       "  'the purple petals have shades of white with white anther and filament',\n",
       "  'this flower has large pink petals and a white stigma in the center',\n",
       "  'this flower has petals that are pink and has a yellow stamen',\n",
       "  'a flower with short and wide petals that is light purple',\n",
       "  'this flower has small pink petals with a yellow center',\n",
       "  'this flower has large rounded pink petals with curved edges and purple veins',\n",
       "  'this flower has purple petals as well as a white stamen']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2ImgData['Captions_string'][:1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5efa616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_SIZE = 64\n",
    "IMAGE_HEIGHT = IMAGE_SIZE\n",
    "IMAGE_WIDTH = IMAGE_SIZE\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path, caption_type='id'):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    if caption_type == 'id':\n",
    "        caption = tf.cast(caption, tf.int32)\n",
    "    elif caption_type == 'sentence':\n",
    "        caption = tf.convert_to_tensor(caption, dtype=tf.string)\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator, caption_type='id'):\n",
    "    # load the training data into two NumPy arrays\n",
    "    if filenames != None:\n",
    "        df = pd.read_pickle(filenames)\n",
    "    else:\n",
    "        df = text2ImgData\n",
    "    \n",
    "    if caption_type == 'id':\n",
    "        captions = df['Captions'].values\n",
    "    elif caption_type == 'sentence':\n",
    "        captions = df['Captions_string'].values\n",
    "    else:\n",
    "        raise ValueError('for dataset_generator, caption_type= should be \\'id\\' or \\'sentence\\'.')\n",
    "        \n",
    "    caption = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose one of them randomly for training\n",
    "    \n",
    "    # ============================================ #\n",
    "    # TODO: augmentation\n",
    "    # idea 1 (difficulty: easy)\n",
    "    #     training data has multiple captions, right now it picks a random one.\n",
    "    #     we can make it so that every caption is an entry and multiple captions link to the same image.\n",
    "    # idea 2 (difficulty: medium)\n",
    "    #     after text embedding, use the average of 2 caption embeddings to generate a new caption.\n",
    "    #     the data does not need to have an image tied to it, it just have the label 0 (fake image).\n",
    "    # ============================================ #\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    if caption_type == 'id':\n",
    "        caption = caption.astype(np.int)\n",
    "        \n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    datagen_func = lambda cap, img: data_generator(cap, img, caption_type=caption_type)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(datagen_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "710669ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = dataset_generator(\n",
    "    #data_path + '/text2ImgData.pkl',\n",
    "    None,\n",
    "    BATCH_SIZE, \n",
    "    training_data_generator, \n",
    "    caption_type=expSettings.caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9640f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (64, 64, 64, 3)\n",
      "Caption shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "# dataset testing ground\n",
    "for img, cap in dataset.take(1):\n",
    "    print(\"Image shape:\", img.numpy().shape)\n",
    "    print(\"Caption shape:\", cap.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e941524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2DTranspose, Conv2D, BatchNormalization, LeakyReLU\n",
    "\n",
    "# custom layers\n",
    "class flattened_dense(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a dense layer that is made compatible with convolution layers\n",
    "    by flattening the input first and followed by a dense layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=64, kernel_initializer=\"glorot_uniform\"):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(channels, kernel_initializer=kernel_initializer)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        fl = self.flatten(inputs)\n",
    "        return self.dense(fl)\n",
    "    \n",
    "class conv_block(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a convolution layer with batch normalization and leaky relu activation\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=128, kernel_size=1, strides=1, kernel_initializer='glorot_uniform'):\n",
    "        super().__init__()\n",
    "        self.conv = Conv2D(filters=filters,\n",
    "                           kernel_size = (kernel_size, kernel_size),\n",
    "                           strides=(strides, strides),\n",
    "                           padding='same',\n",
    "                           kernel_initializer=kernel_initializer)\n",
    "        self.bn = BatchNormalization()\n",
    "        self.activation = LeakyReLU(alpha=0.1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.activation(self.bn(self.conv(inputs)))\n",
    "    \n",
    "class deconv_block(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a deconvolution layer with batch normalization and leaky relu activation\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=128, kernel_size=3, strides=2, kernel_initializer='glorot_uniform'):\n",
    "        super().__init__()\n",
    "        self.deconv = Conv2DTranspose(filters=filters,\n",
    "                                    kernel_size = (kernel_size, kernel_size),\n",
    "                                    strides=(strides, strides),\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=kernel_initializer)\n",
    "        self.bn = BatchNormalization()\n",
    "        self.activation = LeakyReLU(alpha=0.1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.activation(self.bn(self.deconv(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b79a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas, experimental=False, do_batchnorm=False):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.exp=experimental\n",
    "        self.do_batchnorm = do_batchnorm\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "        if self.exp:\n",
    "            self.embed = hub.load('./checkpoints/universal_sentence_encoder')\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        if self.exp:\n",
    "            with tf.device('/CPU:0'): # TODO if you find a way to use GPU, go for it.\n",
    "                output_last = self.embed(text)\n",
    "                \n",
    "            state = hidden # not updating state for compatibility reasons\n",
    "            \n",
    "        else:\n",
    "            text = self.embedding(text)\n",
    "            output, state = self.gru(text, initial_state = hidden)\n",
    "            output_last = output[:, -1, :]\n",
    "        \n",
    "        # normalization in-batch\n",
    "        if self.do_batchnorm:\n",
    "            mean = tf.reduce_mean(output_last, axis=0, keepdims=True)  # Mean across the batch\n",
    "            std = tf.math.reduce_std(output_last, axis=0, keepdims=True)  # Std across the batch\n",
    "            normalized = (output_last - mean) / (std + 1e-6)  # Avoid division by zero\n",
    "        else:\n",
    "            normalized = output_last\n",
    "        \n",
    "        return normalized, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7dcc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas, experimental=False):\n",
    "        super(Generator, self).__init__()\n",
    "        self.exp = experimental\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d2 = tf.keras.layers.Dense(64*64*3)\n",
    "        if self.exp:\n",
    "            self.deconv_depth = int(math.log(IMAGE_SIZE, 2)) - 1\n",
    "            self.dnoise = tf.keras.layers.Dense(256)\n",
    "            self.starter = tf.keras.layers.Dense(2*2*256)\n",
    "            self.deconv = [deconv_block(filters=256) for i in range(self.deconv_depth)]\n",
    "            self.conv_in_deconv = [conv_block(filters=128, kernel_size=3) for i in range(self.deconv_depth)]\n",
    "            self.head1 = conv_block(filters=128, kernel_size=3, strides=1)\n",
    "            self.head2 = conv_block(filters=64, kernel_size=3, strides=1)\n",
    "            self.head3 = conv_block(filters=16, kernel_size=1, strides=1)\n",
    "            self.headf = conv_block(filters=3, kernel_size=1, strides=1)\n",
    "            \n",
    "    def call(self, text, noise_z, debug_output=False):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        # deconvolution\n",
    "        if self.exp:\n",
    "            dnoise = self.dnoise(noise_z)\n",
    "            noisy_text = tf.concat([text, dnoise], axis=1)\n",
    "            img = self.starter(noisy_text)\n",
    "            img = tf.reshape(img, [-1, 2, 2, 256])\n",
    "            debug = []\n",
    "            for i in range(int(math.log(IMAGE_SIZE, 2))):\n",
    "                debug.append(img)\n",
    "                img = self.deconv[i](img)\n",
    "                img = self.conv_in_deconv[i](img)\n",
    "\n",
    "            img = self.head1(img)\n",
    "            img = self.head2(img)\n",
    "            img = self.head3(img)\n",
    "            img = self.headf(img)\n",
    "            #helper = self.d2(tf.concat([noise_z, text], axis=1))\n",
    "            #helper = tf.reshape(helper, [-1, 64, 64, 3])\n",
    "            logits = tf.reshape(img, [-1, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "            #logits = logits + helper\n",
    "            output = tf.nn.tanh(logits)\n",
    "        \n",
    "        # concatenate input text and random noise\n",
    "        else:\n",
    "            text_concat = tf.concat([noise_z, text], axis=1)\n",
    "            text_concat = self.d2(text_concat)\n",
    "        \n",
    "            logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "            output = tf.nn.tanh(logits)\n",
    "            debug_output = output\n",
    "        \n",
    "        if debug_output:\n",
    "            return logits, output, debug\n",
    "        else:\n",
    "            return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a78cfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas, experimental=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.exp = experimental\n",
    "        self.hparas = hparas\n",
    "        if self.exp:\n",
    "            self.resnet_base = ResNet50(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), weights='imagenet', include_top=False)\n",
    "            for layer in self.resnet_base.layers:\n",
    "                layer.trainable = False\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, img, text):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d_text(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        if self.exp:\n",
    "            img = self.resnet_base(img)\n",
    "        img = self.flatten(img)\n",
    "        img = self.d_img(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=1)\n",
    "        \n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac84cb",
   "metadata": {},
   "source": [
    "Parameters and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46263c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LR_GEN': 1e-3,\n",
    "    'LR_DIS': 1e-4,\n",
    "    'LR_DECAY': 0.5,                          # unused\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 50,                            # number of epoch for demo\n",
    "    'N_SAMPLE': num_training_sample,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 1                           # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3d61caf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(hparas, \n",
    "                           experimental=expSettings.enc,\n",
    "                           do_batchnorm=expSettings.enc_do_batchnorm)\n",
    "\n",
    "generator = Generator(hparas,\n",
    "                      experimental=expSettings.gen)\n",
    "\n",
    "discriminator = Discriminator(hparas,\n",
    "                              experimental=expSettings.dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90824abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (64, 64, 64, 3)\n",
      "Caption shape: (64,)\n",
      "Caption embed shape: (64, 512)\n"
     ]
    }
   ],
   "source": [
    "# test text encoder\n",
    "for img, cap in dataset.take(1):\n",
    "    print(\"Image shape:\", img.numpy().shape)\n",
    "    print(\"Caption shape:\", cap.shape)\n",
    "    with tf.device('/CPU:0'):\n",
    "        output, _ = text_encoder(cap, 0)\n",
    "        print(\"Caption embed shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "220a1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a45f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21f720d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR_GEN'], clipvalue=10.0)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR_DIS'], clipvalue=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24d5f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "if expSettings.delete_checkpoint:\n",
    "    # TODO remove old checkpoint files\n",
    "    pass\n",
    "\n",
    "# one benefit of tf.train.Checkpoint() API is we can save everything seperately\n",
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 text_encoder=text_encoder,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b35f60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, caption, hidden, imshow=False):\n",
    "    # random noise for generator\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=0.1)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        text_embed, hidden = text_encoder(caption, hidden)\n",
    "        _, fake_image = generator(text_embed, noise)\n",
    "        if imshow:\n",
    "            plt.imshow(fake_image[0])\n",
    "\n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "        g_loss = generator_loss(fake_logits)\n",
    "        d_loss = discriminator_loss(real_logits, fake_logits)\n",
    "\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grad_d = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_d, discriminator.trainable_variables))\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c25ce5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5d5af",
   "metadata": {},
   "source": [
    "Sample Debugging (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d68c0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09b260b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size, caption_type='id'):\n",
    "    if caption_type == 'sentence':\n",
    "        caption = caption2string(caption)\n",
    "    caption = np.asarray(caption)\n",
    "    if caption_type == 'id':\n",
    "        caption = caption.astype(np.int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e086c8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'], caption_type=expSettings.caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2946f2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption shape: (64,)\n",
      "Caption embeddings: tf.Tensor(\n",
      "[[-0.01346336  0.06881763 -0.05529514 ... -0.01518281 -0.00633275\n",
      "   0.01500697]\n",
      " [-0.01346336  0.06881763 -0.05529514 ... -0.01518281 -0.00633275\n",
      "   0.01500697]\n",
      " [-0.01346336  0.06881763 -0.05529514 ... -0.01518281 -0.00633275\n",
      "   0.01500697]\n",
      " ...\n",
      " [-0.00790838  0.06233827  0.01107207 ... -0.02398296 -0.03450323\n",
      "  -0.00868433]\n",
      " [-0.00790838  0.06233827  0.01107207 ... -0.02398296 -0.03450323\n",
      "  -0.00868433]\n",
      " [-0.00790838  0.06233827  0.01107207 ... -0.02398296 -0.03450323\n",
      "  -0.00868433]], shape=(64, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# test the sample dataset\n",
    "for cap in sample_sentence.take(1):\n",
    "    print(\"Caption shape:\", cap.numpy().shape)\n",
    "    emb, _ = text_encoder(cap, None)\n",
    "    print(\"Caption embeddings:\", emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4af2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples/demo'):\n",
    "    os.makedirs('samples/demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34dfd3",
   "metadata": {},
   "source": [
    "Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "620661c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        imshow = False\n",
    "        \n",
    "        for image, caption in dataset:\n",
    "            g_loss, d_loss = train_step(image, caption, hidden, imshow=imshow)\n",
    "            imshow = False\n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        print('======================================')\n",
    "        \n",
    "        # save the model\n",
    "        if True:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], 'samples/demo/train_{:02d}.jpg'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c752c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/GatherV2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/GatherV2_grad/Reshape:0\", shape=(None, 320), dtype=float32), dense_shape=Tensor(\"gradients/EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/GatherV2_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen_loss: 0.7468, disc_loss: 1.4494\n",
      "Time for epoch 1 is 42.7817 sec\n",
      "======================================\n",
      "Epoch 2, gen_loss: 1.8876, disc_loss: 0.3602\n",
      "Time for epoch 2 is 25.8533 sec\n",
      "======================================\n",
      "Epoch 3, gen_loss: 2.9052, disc_loss: 0.1175\n",
      "Time for epoch 3 is 25.5019 sec\n",
      "======================================\n",
      "Epoch 4, gen_loss: 3.6762, disc_loss: 0.0530\n",
      "Time for epoch 4 is 24.9849 sec\n",
      "======================================\n",
      "Epoch 5, gen_loss: 4.3117, disc_loss: 0.0279\n",
      "Time for epoch 5 is 24.3139 sec\n",
      "======================================\n",
      "Epoch 6, gen_loss: 4.7637, disc_loss: 0.0170\n",
      "Time for epoch 6 is 25.0501 sec\n",
      "======================================\n",
      "Epoch 7, gen_loss: 5.1496, disc_loss: 0.0116\n",
      "Time for epoch 7 is 25.4220 sec\n",
      "======================================\n",
      "Epoch 8, gen_loss: 5.4737, disc_loss: 0.0084\n",
      "Time for epoch 8 is 25.2321 sec\n",
      "======================================\n",
      "Epoch 9, gen_loss: 5.7559, disc_loss: 0.0063\n",
      "Time for epoch 9 is 24.8955 sec\n",
      "======================================\n",
      "Epoch 10, gen_loss: 6.0287, disc_loss: 0.0048\n",
      "Time for epoch 10 is 24.3862 sec\n",
      "======================================\n",
      "Epoch 11, gen_loss: 6.2457, disc_loss: 0.0039\n",
      "Time for epoch 11 is 24.9875 sec\n",
      "======================================\n",
      "Epoch 12, gen_loss: 6.4476, disc_loss: 0.0031\n",
      "Time for epoch 12 is 24.2506 sec\n",
      "======================================\n",
      "Epoch 13, gen_loss: 6.6298, disc_loss: 0.0026\n",
      "Time for epoch 13 is 34.3267 sec\n",
      "======================================\n",
      "Epoch 14, gen_loss: 6.8048, disc_loss: 0.0022\n",
      "Time for epoch 14 is 24.9301 sec\n",
      "======================================\n",
      "Epoch 15, gen_loss: 7.0083, disc_loss: 0.0018\n",
      "Time for epoch 15 is 24.9312 sec\n",
      "======================================\n",
      "Epoch 16, gen_loss: 7.1922, disc_loss: 0.0015\n",
      "Time for epoch 16 is 25.0151 sec\n",
      "======================================\n",
      "Epoch 17, gen_loss: 7.3491, disc_loss: 0.0013\n",
      "Time for epoch 17 is 24.8219 sec\n",
      "======================================\n",
      "Epoch 18, gen_loss: 7.5171, disc_loss: 0.0011\n",
      "Time for epoch 18 is 24.8686 sec\n",
      "======================================\n",
      "Epoch 19, gen_loss: 7.6211, disc_loss: 0.0010\n",
      "Time for epoch 19 is 24.8478 sec\n",
      "======================================\n",
      "Epoch 20, gen_loss: 7.7577, disc_loss: 0.0009\n",
      "Time for epoch 20 is 25.0468 sec\n",
      "======================================\n",
      "Epoch 21, gen_loss: 7.8659, disc_loss: 0.0008\n",
      "Time for epoch 21 is 25.2440 sec\n",
      "======================================\n",
      "Epoch 22, gen_loss: 7.9777, disc_loss: 0.0007\n",
      "Time for epoch 22 is 24.8257 sec\n",
      "======================================\n",
      "Epoch 23, gen_loss: 8.1071, disc_loss: 0.0006\n",
      "Time for epoch 23 is 24.3017 sec\n",
      "======================================\n",
      "Epoch 24, gen_loss: 8.2080, disc_loss: 0.0005\n",
      "Time for epoch 24 is 24.5667 sec\n",
      "======================================\n",
      "Epoch 25, gen_loss: 8.3189, disc_loss: 0.0005\n",
      "Time for epoch 25 is 24.6064 sec\n",
      "======================================\n",
      "Epoch 26, gen_loss: 8.4070, disc_loss: 0.0004\n",
      "Time for epoch 26 is 25.2070 sec\n",
      "======================================\n",
      "Epoch 27, gen_loss: 8.5281, disc_loss: 0.0004\n",
      "Time for epoch 27 is 24.9909 sec\n",
      "======================================\n",
      "Epoch 28, gen_loss: 8.6332, disc_loss: 0.0004\n",
      "Time for epoch 28 is 24.9312 sec\n",
      "======================================\n",
      "Epoch 29, gen_loss: 8.7360, disc_loss: 0.0003\n",
      "Time for epoch 29 is 24.4652 sec\n",
      "======================================\n",
      "Epoch 30, gen_loss: 8.8309, disc_loss: 0.0003\n",
      "Time for epoch 30 is 24.6216 sec\n",
      "======================================\n",
      "Epoch 31, gen_loss: 8.9125, disc_loss: 0.0003\n",
      "Time for epoch 31 is 24.4209 sec\n",
      "======================================\n",
      "Epoch 32, gen_loss: 9.0266, disc_loss: 0.0002\n",
      "Time for epoch 32 is 24.9864 sec\n",
      "======================================\n",
      "Epoch 33, gen_loss: 9.0987, disc_loss: 0.0002\n",
      "Time for epoch 33 is 25.0637 sec\n",
      "======================================\n",
      "Epoch 34, gen_loss: 9.1911, disc_loss: 0.0002\n",
      "Time for epoch 34 is 24.6944 sec\n",
      "======================================\n",
      "Epoch 35, gen_loss: 9.2839, disc_loss: 0.0002\n",
      "Time for epoch 35 is 24.8418 sec\n",
      "======================================\n",
      "Epoch 36, gen_loss: 9.3737, disc_loss: 0.0002\n",
      "Time for epoch 36 is 24.5338 sec\n",
      "======================================\n",
      "Epoch 37, gen_loss: 9.4646, disc_loss: 0.0002\n",
      "Time for epoch 37 is 25.3241 sec\n",
      "======================================\n",
      "Epoch 38, gen_loss: 9.5327, disc_loss: 0.0001\n",
      "Time for epoch 38 is 25.9561 sec\n",
      "======================================\n",
      "Epoch 39, gen_loss: 9.6237, disc_loss: 0.0001\n",
      "Time for epoch 39 is 24.5811 sec\n",
      "======================================\n",
      "Epoch 40, gen_loss: 9.6844, disc_loss: 0.0001\n",
      "Time for epoch 40 is 24.9719 sec\n",
      "======================================\n",
      "Epoch 41, gen_loss: 9.7816, disc_loss: 0.0001\n",
      "Time for epoch 41 is 24.8738 sec\n",
      "======================================\n",
      "Epoch 42, gen_loss: 9.8576, disc_loss: 0.0001\n",
      "Time for epoch 42 is 24.7391 sec\n",
      "======================================\n",
      "Epoch 43, gen_loss: 9.9432, disc_loss: 0.0001\n",
      "Time for epoch 43 is 25.1380 sec\n",
      "======================================\n",
      "Epoch 44, gen_loss: 10.0222, disc_loss: 0.0001\n",
      "Time for epoch 44 is 24.5298 sec\n",
      "======================================\n",
      "Epoch 45, gen_loss: 10.0744, disc_loss: 0.0001\n",
      "Time for epoch 45 is 24.7049 sec\n",
      "======================================\n",
      "Epoch 46, gen_loss: 10.1630, disc_loss: 0.0001\n",
      "Time for epoch 46 is 24.7293 sec\n",
      "======================================\n",
      "Epoch 47, gen_loss: 10.2438, disc_loss: 0.0001\n",
      "Time for epoch 47 is 24.4223 sec\n",
      "======================================\n",
      "Epoch 48, gen_loss: 10.3054, disc_loss: 0.0001\n",
      "Time for epoch 48 is 24.8836 sec\n",
      "======================================\n",
      "Epoch 49, gen_loss: 10.3779, disc_loss: 0.0001\n",
      "Time for epoch 49 is 24.9691 sec\n",
      "======================================\n",
      "Epoch 50, gen_loss: 10.4811, disc_loss: 0.0001\n",
      "Time for epoch 50 is 24.5109 sec\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a634d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_caption2string(ls):\n",
    "    return \" \".join([id2word_dict[idx] for idx in ls]).strip().split(' <PAD>')[0]\n",
    "\n",
    "def testing_data_generator(caption, index, caption_type='id'):\n",
    "    if caption_type == 'id':\n",
    "        caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator, caption_type='id'):\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    \n",
    "    if caption_type == 'sentence':\n",
    "        data['Captions_string'] = data['Captions'].apply(test_caption2string)\n",
    "        captions = data['Captions_string'].values\n",
    "    elif caption_type == 'id':\n",
    "        captions = data['Captions'].values\n",
    "        \n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    if caption_type == 'id':\n",
    "        caption = caption.astype(np.int)\n",
    "        \n",
    "    datagen_func = lambda cap, img: data_generator(cap, img, caption_type=caption_type)\n",
    "        \n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(datagen_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7abd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator, caption_type=expSettings.caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88cc621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (64,)\n",
      "Caption shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "# test the testing dataset\n",
    "for cap, img in testing_dataset.take(1):\n",
    "    print(\"Image shape:\", img.numpy().shape)\n",
    "    print(\"Caption shape:\", cap.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d07f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f624f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./inference/demo'):\n",
    "    os.makedirs('./inference/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d3d0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=0.1, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    print(sample_seed[0:3, :])\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "            if i == 0 and step == 1: \n",
    "                #print(captions)\n",
    "                text_embed_t, hidden_t = text_encoder(captions, hidden)\n",
    "                #print(text_embed_t)\n",
    "                #print(fake_image[0:1, 0:5, 0:5, :])\n",
    "                _, _, debug = generator(text_embed_t, sample_seed, debug_output=True)\n",
    "                print(debug[0][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[1][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[2][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[3][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[4][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[5][0:3, 0:5, 0:5, 0:5])\n",
    "                pred_logit, pred = discriminator(fake_image, text_embed_t)\n",
    "                #print(pred_logit)\n",
    "                \n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d12a53a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./checkpoints/demo\\ckpt-50\n"
     ]
    }
   ],
   "source": [
    "#checkpoint.restore(checkpoint_dir + f'/ckpt-50')\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Restoring from {latest_checkpoint}\")\n",
    "    checkpoint.restore(latest_checkpoint)\n",
    "else:\n",
    "    print(\"No checkpoint found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff59c63a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03997521  0.12315663  0.0012033  ...  0.09444813  0.04159125\n",
      "  -0.0623869 ]\n",
      " [ 0.02975813 -0.0556995  -0.02886274 ...  0.06946493  0.00538529\n",
      "  -0.11472443]\n",
      " [ 0.03350148 -0.11274898  0.0861249  ... -0.02996353  0.0239036\n",
      "  -0.06305609]]\n",
      "tf.Tensor(\n",
      "[[[[ 0.51452786  0.31561685  0.24741508  0.53797287 -0.08812072]]]\n",
      "\n",
      "\n",
      " [[[ 0.42233303  0.41356817  0.1980545   0.55985254 -0.09268352]]]\n",
      "\n",
      "\n",
      " [[[ 0.43788376  0.42990467  0.22601737  0.56862533 -0.09424648]]]], shape=(3, 1, 1, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-0.04447858 -0.05870918 -0.07735799 -0.01680809 -0.07863136]\n",
      "   [-0.05671952 -0.03642096 -0.02854978  0.40831465 -0.03101559]]\n",
      "\n",
      "  [[-0.04036732 -0.05539403 -0.04838633 -0.01496394 -0.05411673]\n",
      "   [-0.04058008 -0.04114917 -0.05409572 -0.00714706 -0.03851094]]]\n",
      "\n",
      "\n",
      " [[[-0.04692612 -0.06288823 -0.08255851 -0.01891819 -0.08199824]\n",
      "   [-0.06052252 -0.03786369 -0.02938552  0.39597896 -0.02984612]]\n",
      "\n",
      "  [[-0.04122457 -0.06067694 -0.05251662 -0.01640655 -0.05995019]\n",
      "   [-0.04510291 -0.04418432 -0.05489622 -0.00741348 -0.04413463]]]\n",
      "\n",
      "\n",
      " [[[-0.04487161 -0.0598391  -0.07916907 -0.01646305 -0.08084309]\n",
      "   [-0.06032321 -0.03762156 -0.03142097  0.38994938 -0.03199713]]\n",
      "\n",
      "  [[-0.04067549 -0.05921911 -0.04833015 -0.0151075  -0.05728642]\n",
      "   [-0.04430934 -0.04217754 -0.05547304 -0.00828807 -0.04008352]]]], shape=(3, 2, 2, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-2.28320861e+00 -9.96571258e-02 -1.41189468e+00 -1.19122922e+00\n",
      "    -1.21048677e+00]\n",
      "   [-3.78578186e+00 -1.21470582e+00 -2.49244666e+00 -2.64982700e+00\n",
      "    -2.98908329e+00]\n",
      "   [-3.63568735e+00 -7.47376382e-01 -2.45536876e+00 -1.55701780e+00\n",
      "    -2.29638338e+00]\n",
      "   [-1.89564228e+00 -5.85299850e-01 -1.46121681e+00 -1.15621030e+00\n",
      "    -1.40503621e+00]]\n",
      "\n",
      "  [[-3.23543978e+00 -8.52427125e-01 -1.75770020e+00 -1.51076555e+00\n",
      "    -1.22990584e+00]\n",
      "   [-7.66651917e+00 -2.40634680e+00 -4.36034870e+00 -3.63578010e+00\n",
      "    -4.05740309e+00]\n",
      "   [-7.90877390e+00 -1.30493581e+00 -4.73772573e+00 -4.14310694e+00\n",
      "    -5.42498732e+00]\n",
      "   [-4.81153631e+00 -1.05039239e+00 -2.53839159e+00 -2.21567702e+00\n",
      "    -2.72625160e+00]]\n",
      "\n",
      "  [[-3.65367556e+00 -1.06093836e+00 -5.62338112e-03 -1.77908885e+00\n",
      "     1.13416195e+01]\n",
      "   [-8.20604992e+00 -1.69969487e+00 -3.70908523e+00 -2.96893382e+00\n",
      "    -1.62744009e+00]\n",
      "   [-1.02634573e+01 -1.80473769e+00 -3.14092445e+00 -2.91131711e+00\n",
      "    -4.52863979e+00]\n",
      "   [-6.13823462e+00 -1.49614346e+00 -3.12140393e+00 -2.12274599e+00\n",
      "    -3.38532758e+00]]\n",
      "\n",
      "  [[-1.35803318e+00 -6.72123611e-01 -3.55472267e-01 -8.00117195e-01\n",
      "     1.59125137e+01]\n",
      "   [-4.06941175e+00 -1.37932622e+00 -1.52189684e+00 -1.54522407e+00\n",
      "     2.50020456e+00]\n",
      "   [-6.68530226e+00 -1.02430224e+00 -9.01195467e-01 -1.41854775e+00\n",
      "    -6.87920034e-01]\n",
      "   [-4.51941061e+00 -1.35814488e+00 -2.18154550e+00 -9.33653295e-01\n",
      "    -1.90384197e+00]]]\n",
      "\n",
      "\n",
      " [[[-2.42195868e+00 -1.05486147e-01 -1.49785638e+00 -1.26375151e+00\n",
      "    -1.28321350e+00]\n",
      "   [-4.01555014e+00 -1.28808856e+00 -2.64331961e+00 -2.81027865e+00\n",
      "    -3.17053723e+00]\n",
      "   [-3.85480356e+00 -7.92027950e-01 -2.60383368e+00 -1.65067708e+00\n",
      "    -2.43558335e+00]\n",
      "   [-2.00940537e+00 -6.20054662e-01 -1.54831588e+00 -1.22535086e+00\n",
      "    -1.48902190e+00]]\n",
      "\n",
      "  [[-3.43207049e+00 -9.03283894e-01 -1.86516118e+00 -1.60256255e+00\n",
      "    -1.30448270e+00]\n",
      "   [-8.13395882e+00 -2.55250978e+00 -4.62544394e+00 -3.85634804e+00\n",
      "    -4.30379677e+00]\n",
      "   [-8.38810921e+00 -1.38351524e+00 -5.02578354e+00 -4.39432669e+00\n",
      "    -5.75525522e+00]\n",
      "   [-5.10243225e+00 -1.11317134e+00 -2.69351482e+00 -2.35002089e+00\n",
      "    -2.89138794e+00]]\n",
      "\n",
      "  [[-3.87532496e+00 -1.12557089e+00 -5.06744208e-03 -1.88662088e+00\n",
      "     1.20355749e+01]\n",
      "   [-8.70431709e+00 -1.80177116e+00 -3.93300509e+00 -3.14880729e+00\n",
      "    -1.72513127e+00]\n",
      "   [-1.08870163e+01 -1.91436291e+00 -3.33032227e+00 -3.08710718e+00\n",
      "    -4.80377340e+00]\n",
      "   [-6.51235342e+00 -1.58670032e+00 -3.31254745e+00 -2.25212264e+00\n",
      "    -3.59223914e+00]]\n",
      "\n",
      "  [[-1.43944526e+00 -7.12115824e-01 -3.76597375e-01 -8.48201096e-01\n",
      "     1.68816051e+01]\n",
      "   [-4.31420088e+00 -1.46244705e+00 -1.61417961e+00 -1.63912225e+00\n",
      "     2.65636134e+00]\n",
      "   [-7.09185410e+00 -1.08665836e+00 -9.54880357e-01 -1.50496602e+00\n",
      "    -7.28664577e-01]\n",
      "   [-4.79512405e+00 -1.44107187e+00 -2.31516123e+00 -9.91214097e-01\n",
      "    -2.02065992e+00]]]\n",
      "\n",
      "\n",
      " [[[-2.41756892e+00 -1.05400801e-01 -1.49542558e+00 -1.26180637e+00\n",
      "    -1.28156173e+00]\n",
      "   [-4.00853157e+00 -1.28609586e+00 -2.63845968e+00 -2.80549216e+00\n",
      "    -3.16512632e+00]\n",
      "   [-3.84853911e+00 -7.91087568e-01 -2.59907484e+00 -1.64837384e+00\n",
      "    -2.43089104e+00]\n",
      "   [-2.00558114e+00 -6.19291246e-01 -1.54585004e+00 -1.22286487e+00\n",
      "    -1.48694420e+00]]\n",
      "\n",
      "  [[-3.42636275e+00 -9.01847005e-01 -1.86259615e+00 -1.59999168e+00\n",
      "    -1.30297649e+00]\n",
      "   [-8.12029076e+00 -2.54828215e+00 -4.61920977e+00 -3.85117507e+00\n",
      "    -4.29728460e+00]\n",
      "   [-8.37515926e+00 -1.38128865e+00 -5.01860237e+00 -4.38732481e+00\n",
      "    -5.74663734e+00]\n",
      "   [-5.09468079e+00 -1.11115777e+00 -2.68896008e+00 -2.34623265e+00\n",
      "    -2.88758802e+00]]\n",
      "\n",
      "  [[-3.87036490e+00 -1.12382865e+00 -5.36727440e-03 -1.88411713e+00\n",
      "     1.20166845e+01]\n",
      "   [-8.69180012e+00 -1.79872346e+00 -3.92820621e+00 -3.14510894e+00\n",
      "    -1.72410989e+00]\n",
      "   [-1.08719177e+01 -1.91180646e+00 -3.32543826e+00 -3.08238196e+00\n",
      "    -4.79709578e+00]\n",
      "   [-6.50264740e+00 -1.58494079e+00 -3.30755401e+00 -2.24893641e+00\n",
      "    -3.58640647e+00]]\n",
      "\n",
      "  [[-1.43818104e+00 -7.11550891e-01 -3.76364827e-01 -8.46937478e-01\n",
      "     1.68615742e+01]\n",
      "   [-4.30979204e+00 -1.46122873e+00 -1.61177862e+00 -1.63727117e+00\n",
      "     2.65174007e+00]\n",
      "   [-7.08189487e+00 -1.08539999e+00 -9.54022706e-01 -1.50307071e+00\n",
      "    -7.27294266e-01]\n",
      "   [-4.78765631e+00 -1.43865335e+00 -2.31094599e+00 -9.89782989e-01\n",
      "    -2.01815915e+00]]]], shape=(3, 4, 4, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-1.00725603e+00 -9.51231122e-01 -2.76538819e-01 -8.15597653e-01\n",
      "    -8.96903932e-01]\n",
      "   [-2.14755177e+00 -8.18437099e-01 -7.37810075e-01 -9.75016296e-01\n",
      "    -2.66973519e+00]\n",
      "   [-1.87976170e+00 -1.24824476e+00 -6.77380681e-01 -1.67970049e+00\n",
      "    -2.33808899e+00]\n",
      "   [-3.41769457e+00 -5.87188065e-01 -1.23055017e+00 -1.34886730e+00\n",
      "    -3.85289836e+00]\n",
      "   [-2.36921263e+00 -1.43216932e+00 -6.59692347e-01 -2.08102298e+00\n",
      "    -2.79335570e+00]]\n",
      "\n",
      "  [[-1.30071092e+00  2.03307581e+00  2.73781624e+01 -1.88479769e+00\n",
      "    -9.43608701e-01]\n",
      "   [-2.36528611e+00  8.11115837e+00  1.63119492e+02 -3.91485572e+00\n",
      "    -5.45474243e+00]\n",
      "   [-2.64155602e+00  3.23602982e+01  7.46893082e+01 -5.01981068e+00\n",
      "    -1.59238970e+00]\n",
      "   [-3.57623410e+00  3.14645348e+01  2.93596008e+02 -6.30419683e+00\n",
      "    -9.60387421e+00]\n",
      "   [-3.68539429e+00  5.09980125e+01  1.07101540e+02 -6.99228907e+00\n",
      "    -1.29180837e+00]]\n",
      "\n",
      "  [[-1.91646981e+00 -1.44393444e+00 -7.90494263e-01 -2.66176319e+00\n",
      "    -1.92876458e+00]\n",
      "   [-5.43634892e+00 -2.25532937e+00 -2.48222041e+00 -5.55755663e+00\n",
      "    -6.09460592e+00]\n",
      "   [-4.78240919e+00 -3.43250585e+00 -3.38819289e+00 -7.16770935e+00\n",
      "    -5.34239912e+00]\n",
      "   [-1.05822124e+01 -3.87392426e+00 -4.99623251e+00 -9.83549500e+00\n",
      "    -1.14955482e+01]\n",
      "   [-6.64247656e+00 -4.47865820e+00 -4.67919731e+00 -1.06213293e+01\n",
      "    -8.30504608e+00]]\n",
      "\n",
      "  [[-2.04951167e+00  2.02653360e+00  3.25337677e+01 -3.10668731e+00\n",
      "    -2.85546660e+00]\n",
      "   [-5.16891336e+00  1.09642305e+01  2.18539276e+02 -7.87396240e+00\n",
      "    -9.68880844e+00]\n",
      "   [-5.55179119e+00  3.16950302e+01  1.10424782e+02 -8.90537930e+00\n",
      "    -6.02756023e+00]\n",
      "   [-9.10842323e+00  5.75039749e+01  5.09161224e+02 -1.47567968e+01\n",
      "    -1.98711605e+01]\n",
      "   [-8.10571766e+00  8.89507523e+01  2.07958252e+02 -1.49758444e+01\n",
      "    -7.98599339e+00]]\n",
      "\n",
      "  [[-1.99039209e+00 -2.79786736e-01  2.41087780e+01 -3.37904787e+00\n",
      "    -2.64456749e+00]\n",
      "   [-6.04305601e+00 -1.89386559e+00 -1.64582884e+00 -7.44683552e+00\n",
      "    -6.41852522e+00]\n",
      "   [-6.18487072e+00 -4.68353558e+00 -4.82580900e+00 -1.01870108e+01\n",
      "    -6.01688385e+00]\n",
      "   [-1.48685322e+01 -5.94191694e+00 -7.29423237e+00 -1.63317490e+01\n",
      "    -1.52399216e+01]\n",
      "   [-9.10830212e+00 -5.51819611e+00 -7.66928959e+00 -1.76123161e+01\n",
      "    -1.15613947e+01]]]\n",
      "\n",
      "\n",
      " [[[-1.06833816e+00 -1.00914633e+00 -2.93387562e-01 -8.64582837e-01\n",
      "    -9.51125324e-01]\n",
      "   [-2.27831125e+00 -8.68187070e-01 -7.83511400e-01 -1.03465736e+00\n",
      "    -2.83238101e+00]\n",
      "   [-1.99360502e+00 -1.32444274e+00 -7.18240440e-01 -1.78086317e+00\n",
      "    -2.48007274e+00]\n",
      "   [-3.62460494e+00 -6.22724712e-01 -1.30608273e+00 -1.42926824e+00\n",
      "    -4.08625269e+00]\n",
      "   [-2.51316810e+00 -1.51904476e+00 -6.99803054e-01 -2.20663881e+00\n",
      "    -2.96268725e+00]]\n",
      "\n",
      "  [[-1.37974203e+00  2.16165876e+00  2.90432987e+01 -1.99902666e+00\n",
      "    -1.00045145e+00]\n",
      "   [-2.50880051e+00  8.60026550e+00  1.73037613e+02 -4.15225220e+00\n",
      "    -5.78667259e+00]\n",
      "   [-2.80166912e+00  3.43289032e+01  7.92241745e+01 -5.32457304e+00\n",
      "    -1.68846643e+00]\n",
      "   [-3.79299474e+00  3.33702888e+01  3.11446106e+02 -6.68600988e+00\n",
      "    -1.01871605e+01]\n",
      "   [-3.90940166e+00  5.40886230e+01  1.13607727e+02 -7.41735220e+00\n",
      "    -1.36710286e+00]]\n",
      "\n",
      "  [[-2.03308558e+00 -1.53134215e+00 -8.38950753e-01 -2.82312632e+00\n",
      "    -2.04609179e+00]\n",
      "   [-5.76625729e+00 -2.39287472e+00 -2.63339019e+00 -5.89477062e+00\n",
      "    -6.46483612e+00]\n",
      "   [-5.07283068e+00 -3.64154124e+00 -3.59502029e+00 -7.60272360e+00\n",
      "    -5.66699743e+00]\n",
      "   [-1.12266531e+01 -4.11063814e+00 -5.30154991e+00 -1.04337692e+01\n",
      "    -1.21954441e+01]\n",
      "   [-7.04634237e+00 -4.75166273e+00 -4.96445417e+00 -1.12671041e+01\n",
      "    -8.80975914e+00]]\n",
      "\n",
      "  [[-2.17361641e+00  2.14298415e+00  3.45066376e+01 -3.29571605e+00\n",
      "    -3.02866650e+00]\n",
      "   [-5.48366690e+00  1.16318512e+01  2.31824173e+02 -8.35312748e+00\n",
      "    -1.02782478e+01]\n",
      "   [-5.88912153e+00  3.36118546e+01  1.17129097e+02 -9.44605160e+00\n",
      "    -6.39343929e+00]\n",
      "   [-9.66290760e+00  6.10083771e+01  5.40140076e+02 -1.56544437e+01\n",
      "    -2.10804501e+01]\n",
      "   [-8.59743500e+00  9.43513870e+01  2.20624466e+02 -1.58869448e+01\n",
      "    -8.47078419e+00]]\n",
      "\n",
      "  [[-2.11128759e+00 -2.95065999e-01  2.55934448e+01 -3.58395886e+00\n",
      "    -2.80484343e+00]\n",
      "   [-6.41021490e+00 -2.00833678e+00 -1.74507809e+00 -7.89987659e+00\n",
      "    -6.80833149e+00]\n",
      "   [-6.56024313e+00 -4.96716881e+00 -5.12045431e+00 -1.08058615e+01\n",
      "    -6.38182592e+00]\n",
      "   [-1.57722187e+01 -6.30359554e+00 -7.73851728e+00 -1.73260651e+01\n",
      "    -1.61651974e+01]\n",
      "   [-9.66147518e+00 -5.85263968e+00 -8.13631821e+00 -1.86825123e+01\n",
      "    -1.22627172e+01]]]\n",
      "\n",
      "\n",
      " [[[-1.06622255e+00 -1.00750530e+00 -2.92768568e-01 -8.63158524e-01\n",
      "    -9.49526012e-01]\n",
      "   [-2.27389407e+00 -8.66730154e-01 -7.82388449e-01 -1.03239381e+00\n",
      "    -2.82656741e+00]\n",
      "   [-1.98962724e+00 -1.32205641e+00 -7.16473222e-01 -1.77748191e+00\n",
      "    -2.47570252e+00]\n",
      "   [-3.61787462e+00 -6.21445000e-01 -1.30375886e+00 -1.42621613e+00\n",
      "    -4.07849073e+00]\n",
      "   [-2.50864911e+00 -1.51698971e+00 -6.98469520e-01 -2.20268512e+00\n",
      "    -2.95733857e+00]]\n",
      "\n",
      "  [[-1.37713909e+00  2.15947294e+00  2.89924927e+01 -1.99590647e+00\n",
      "    -9.98189747e-01]\n",
      "   [-2.50327277e+00  8.58529568e+00  1.72729080e+02 -4.14494419e+00\n",
      "    -5.77568150e+00]\n",
      "   [-2.79698110e+00  3.42707291e+01  7.90812531e+01 -5.31504965e+00\n",
      "    -1.68479252e+00]\n",
      "   [-3.78559113e+00  3.33052711e+01  3.10896149e+02 -6.67401266e+00\n",
      "    -1.01688051e+01]\n",
      "   [-3.90268826e+00  5.39944649e+01  1.13417641e+02 -7.40442371e+00\n",
      "    -1.36342537e+00]]\n",
      "\n",
      "  [[-2.02967906e+00 -1.52927363e+00 -8.37948740e-01 -2.81836772e+00\n",
      "    -2.04271650e+00]\n",
      "   [-5.75610304e+00 -2.38841438e+00 -2.62956023e+00 -5.88419294e+00\n",
      "    -6.45369196e+00]\n",
      "   [-5.06473589e+00 -3.63610339e+00 -3.58821535e+00 -7.58924055e+00\n",
      "    -5.65781975e+00]\n",
      "   [-1.12078981e+01 -4.10433149e+00 -5.29173183e+00 -1.04151688e+01\n",
      "    -1.21755867e+01]\n",
      "   [-7.03551054e+00 -4.74443769e+00 -4.95564079e+00 -1.12480536e+01\n",
      "    -8.79550934e+00]]\n",
      "\n",
      "  [[-2.17050529e+00  2.14176559e+00  3.44533577e+01 -3.28990602e+00\n",
      "    -3.02427459e+00]\n",
      "   [-5.47494459e+00  1.16062689e+01  2.31455215e+02 -8.33950233e+00\n",
      "    -1.02617273e+01]\n",
      "   [-5.88043880e+00  3.35564880e+01  1.16950180e+02 -9.43139267e+00\n",
      "    -6.38228703e+00]\n",
      "   [-9.64776611e+00  6.09101944e+01  5.39326660e+02 -1.56299362e+01\n",
      "    -2.10470448e+01]\n",
      "   [-8.58456612e+00  9.42151337e+01  2.20300919e+02 -1.58633089e+01\n",
      "    -8.45740604e+00]]\n",
      "\n",
      "  [[-2.10817409e+00 -2.95377880e-01  2.55535316e+01 -3.57804489e+00\n",
      "    -2.80074096e+00]\n",
      "   [-6.40018559e+00 -2.00608969e+00 -1.74285281e+00 -7.88693237e+00\n",
      "    -6.79809666e+00]\n",
      "   [-6.55110788e+00 -4.96076584e+00 -5.11202002e+00 -1.07896976e+01\n",
      "    -6.37249136e+00]\n",
      "   [-1.57487507e+01 -6.29426003e+00 -7.72741461e+00 -1.73003025e+01\n",
      "    -1.61424694e+01]\n",
      "   [-9.64807510e+00 -5.84440231e+00 -8.12468815e+00 -1.86556988e+01\n",
      "    -1.22451801e+01]]]], shape=(3, 5, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[ 11.917688    -0.6881095    2.9528031   -0.88712275   5.699692  ]\n",
      "   [ 19.155954    -0.9763506   19.959347    -1.8134785   16.902773  ]\n",
      "   [ 27.580362    -1.561325     5.0329175   -2.2245884   14.45261   ]\n",
      "   [ 34.085247    -2.0686316   35.178715    -2.5066252   31.778482  ]\n",
      "   [ 39.4422      -2.2652109    7.4907007   -2.726571    17.673975  ]]\n",
      "\n",
      "  [[ 32.484276    -0.9675048   23.081701    -2.5652473   18.3301    ]\n",
      "   [ 69.00554     -1.6409132   51.360588    -5.4411445   37.873016  ]\n",
      "   [ 80.800804    -2.6705356   66.47622     -8.584037    44.26775   ]\n",
      "   [ 99.500565    -3.84804     98.67707    -10.160883    77.58443   ]\n",
      "   [ 93.497665    -4.595415    64.16006    -10.79641     63.69151   ]]\n",
      "\n",
      "  [[ 43.53222     -1.4128815   17.311468    -2.7712789   13.788824  ]\n",
      "   [ 68.263       -4.213336    63.518463    -5.0943937   21.013031  ]\n",
      "   [124.30232     -3.386237    43.77588     -8.687221    30.609394  ]\n",
      "   [138.59044     -9.831422   115.91925     -6.8384423   73.9832    ]\n",
      "   [158.9753      -6.3069897   56.374683   -10.825675    51.365456  ]]\n",
      "\n",
      "  [[ 29.038406    -2.2983835   12.347764    -4.3328643   12.315032  ]\n",
      "   [100.210144    -7.293041    91.08843     -8.1832905   88.48586   ]\n",
      "   [ 80.420876    -5.921735    45.872955   -16.157959    53.53603   ]\n",
      "   [209.87677    -15.422324   175.44801    -15.144394   231.21716   ]\n",
      "   [104.107025   -11.042987    44.023872   -18.518785    67.65475   ]]\n",
      "\n",
      "  [[ 40.054695    -2.4213302   17.194988    -3.662194    29.797836  ]\n",
      "   [ 83.58465     -5.561247    90.970245    -7.2669764   77.312454  ]\n",
      "   [111.45784     -6.506996    53.92683    -12.58699     98.55737   ]\n",
      "   [148.80424    -11.422178   168.63966    -12.704402   170.2706    ]\n",
      "   [155.79193    -11.325702    60.718987   -15.556891   130.64325   ]]]\n",
      "\n",
      "\n",
      " [[[ 12.637172    -0.7295066    3.128153    -0.94072515   6.0410166 ]\n",
      "   [ 20.315216    -1.0356263   21.170033    -1.9233401   17.926138  ]\n",
      "   [ 29.250792    -1.6559384    5.3350368   -2.359416    15.328118  ]\n",
      "   [ 36.149544    -2.1938455   37.313427    -2.6590946   33.70278   ]\n",
      "   [ 41.826733    -2.4022682    7.943062    -2.8919704   18.740252  ]]\n",
      "\n",
      "  [[ 34.448593    -1.025962    24.478144    -2.7204726   19.439247  ]\n",
      "   [ 73.19215     -1.7399914   54.47431     -5.7709646   40.16443   ]\n",
      "   [ 85.71332     -2.8326957   70.511955    -9.104957    46.95523   ]\n",
      "   [105.543106    -4.081552   104.66715    -10.778462    82.290794  ]\n",
      "   [ 99.167496    -4.8738074   68.053154   -11.452407    67.55188   ]]\n",
      "\n",
      "  [[ 46.16382     -1.4982283   18.357307    -2.9389694   14.620358  ]\n",
      "   [ 72.40717     -4.4690886   67.36567     -5.403659    22.278921  ]\n",
      "   [131.8534      -3.5919664   46.43145     -9.21493     32.4582    ]\n",
      "   [147.00722    -10.429015   122.9623      -7.2541957   78.475815  ]\n",
      "   [168.62727     -6.689693    59.791733   -11.482452    54.4835    ]]\n",
      "\n",
      "  [[ 30.790337    -2.4375741   13.088536    -4.5950613   13.056861  ]\n",
      "   [106.28722     -7.73603     96.60887     -8.679241    93.85675   ]\n",
      "   [ 85.30352     -6.2807255   48.647575   -17.139757    56.77861   ]\n",
      "   [222.63098    -16.360786   186.1117     -16.06438    245.27173   ]\n",
      "   [110.41478    -11.712825    46.687824   -19.643478    71.75599   ]]\n",
      "\n",
      "  [[ 42.475464    -2.5680616   18.234495    -3.884138    31.604694  ]\n",
      "   [ 88.65092     -5.8985076   96.48607     -7.7077975   82.00244   ]\n",
      "   [118.22076     -6.902233    57.204105   -13.351882   104.54917   ]\n",
      "   [157.84375    -12.115873   178.88986    -13.475657   180.61433   ]\n",
      "   [165.24599    -12.013797    64.40231    -16.501768   138.58197   ]]]\n",
      "\n",
      "\n",
      " [[[ 12.61175     -0.7282707    3.1212366   -0.9390937    6.030277  ]\n",
      "   [ 20.273596    -1.0338224   21.12925     -1.9196548   17.891516  ]\n",
      "   [ 29.196337    -1.6526655    5.322274    -2.3549204   15.297581  ]\n",
      "   [ 36.078964    -2.1895556   37.242374    -2.6540546   33.640987  ]\n",
      "   [ 41.748882    -2.3978176    7.9281416   -2.8862653   18.702797  ]]\n",
      "\n",
      "  [[ 34.38835     -1.0240746   24.433561    -2.7155633   19.403648  ]\n",
      "   [ 73.05847     -1.7367535   54.376865    -5.760326    40.090946  ]\n",
      "   [ 85.554276    -2.8273306   70.38019     -9.088504    46.867306  ]\n",
      "   [105.3434      -4.074319   104.47556    -10.758292    82.14113   ]\n",
      "   [ 98.98164     -4.8649917   67.925186   -11.430901    67.429016  ]]\n",
      "\n",
      "  [[ 46.08422     -1.4958347   18.326818    -2.9338388   14.592574  ]\n",
      "   [ 72.27809     -4.461261    67.24514     -5.394272    22.23904   ]\n",
      "   [131.61371     -3.5853214   46.346554    -9.198298    32.39779   ]\n",
      "   [146.73169    -10.409934   122.730194    -7.240496    78.32794   ]\n",
      "   [168.3189      -6.677672    59.68397    -11.461523    54.385334  ]]\n",
      "\n",
      "  [[ 30.737606    -2.4337735   13.067275    -4.587559    13.036708  ]\n",
      "   [106.09927     -7.722915    96.440926    -8.664454    93.694756  ]\n",
      "   [ 85.14997     -6.2698445   48.557674   -17.108877    56.671146  ]\n",
      "   [222.23788    -16.331001   185.77635    -16.03584    244.83366   ]\n",
      "   [110.22726    -11.691947    46.608746   -19.608297    71.622185  ]]\n",
      "\n",
      "  [[ 42.40292     -2.5637267   18.203684    -3.877512    31.552517  ]\n",
      "   [ 88.49712     -5.8881454   96.32007     -7.6944995   81.85982   ]\n",
      "   [118.008865    -6.8900714   57.096027   -13.328204   104.36439   ]\n",
      "   [157.56642    -12.094313   178.57658    -13.452214   180.29803   ]\n",
      "   [164.97101    -11.993093    64.289246   -16.473028   138.33768   ]]]], shape=(3, 5, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-7.15262234e-01  2.94623470e+00 -1.70036241e-01 -1.07667494e+00\n",
      "     1.21567574e+01]\n",
      "   [-2.18373036e+00  9.80788898e+00 -2.59042948e-01 -2.85827804e+00\n",
      "     1.60479279e+01]\n",
      "   [-2.37588143e+00  6.15936947e+00 -5.46217144e-01 -4.00785446e+00\n",
      "     5.53355179e+01]\n",
      "   [-2.37363505e+00  1.62123489e+01 -1.70854247e+00 -5.26722670e+00\n",
      "     3.82653275e+01]\n",
      "   [-2.47685909e+00  8.48806667e+00 -1.12576568e+00 -5.56408310e+00\n",
      "     7.10525360e+01]]\n",
      "\n",
      "  [[ 3.06407833e+01 -1.51136601e+00 -2.59479195e-01  4.24692841e+01\n",
      "    -2.38656378e+00]\n",
      "   [ 4.68306847e+01 -2.80205750e+00 -3.38140869e+00  4.20538635e+01\n",
      "    -6.02681112e+00]\n",
      "   [ 1.06298317e+02 -5.52416658e+00 -4.37395960e-01  1.41490555e+02\n",
      "    -8.69917774e+00]\n",
      "   [ 9.39543076e+01 -6.20175123e+00 -7.73552799e+00  7.98068161e+01\n",
      "    -1.36422291e+01]\n",
      "   [ 1.19974701e+02 -7.66158009e+00 -1.19721377e+00  1.70579620e+02\n",
      "    -1.27959700e+01]]\n",
      "\n",
      "  [[-2.71741033e+00  1.75397072e+01 -2.39330128e-01 -3.56634331e+00\n",
      "     4.54319191e+01]\n",
      "   [-7.02336836e+00  6.53012009e+01 -1.15504396e+00 -9.16762543e+00\n",
      "     5.48237495e+01]\n",
      "   [-8.99348068e+00  4.75961571e+01 -9.47558999e-01 -1.16621761e+01\n",
      "     1.75148956e+02]\n",
      "   [-1.18137770e+01  1.29608490e+02 -4.52931070e+00 -1.71530247e+01\n",
      "     1.00232567e+02]\n",
      "   [-1.55682220e+01  9.01797333e+01 -1.77837658e+00 -1.87796459e+01\n",
      "     2.66202545e+02]]\n",
      "\n",
      "  [[ 8.12325363e+01 -4.47714901e+00 -5.63345730e-01  1.15264542e+02\n",
      "    -6.32588005e+00]\n",
      "   [ 1.42898621e+02 -9.07854080e+00 -8.81259823e+00  1.32662521e+02\n",
      "    -1.38806839e+01]\n",
      "   [ 2.35345108e+02 -1.41985693e+01 -4.81737435e-01  3.43446930e+02\n",
      "    -2.19610062e+01]\n",
      "   [ 2.60496063e+02 -1.94372616e+01 -1.74065990e+01  2.52630386e+02\n",
      "    -3.12281551e+01]\n",
      "   [ 3.62477875e+02 -2.36843567e+01 -2.76700467e-01  5.41418579e+02\n",
      "    -3.48510590e+01]]\n",
      "\n",
      "  [[-4.97894144e+00  3.59200096e+01 -4.52782214e-01 -7.66455030e+00\n",
      "     8.91878433e+01]\n",
      "   [-1.10256586e+01  1.13635971e+02 -3.36531806e+00 -1.68366032e+01\n",
      "     1.11972870e+02]\n",
      "   [-1.48622561e+01  1.06554977e+02 -1.84464967e+00 -2.38738060e+01\n",
      "     3.03271545e+02]\n",
      "   [-2.19412041e+01  2.39362183e+02 -7.48107386e+00 -3.45089149e+01\n",
      "     2.25481049e+02]\n",
      "   [-2.66667213e+01  1.87650024e+02 -2.54575968e+00 -4.01706543e+01\n",
      "     5.08159821e+02]]]\n",
      "\n",
      "\n",
      " [[[-7.58507550e-01  3.12411952e+00 -1.80345237e-01 -1.14174330e+00\n",
      "     1.28892746e+01]\n",
      "   [-2.31590128e+00  1.03997526e+01 -2.74787545e-01 -3.03143454e+00\n",
      "     1.70182667e+01]\n",
      "   [-2.51986003e+00  6.53230572e+00 -5.79286873e-01 -4.25065708e+00\n",
      "     5.86845856e+01]\n",
      "   [-2.51698756e+00  1.71923962e+01 -1.81228316e+00 -5.58632421e+00\n",
      "     4.05833473e+01]\n",
      "   [-2.62651205e+00  8.99858284e+00 -1.19414222e+00 -5.90133286e+00\n",
      "     7.53545761e+01]]\n",
      "\n",
      "  [[ 3.24886780e+01 -1.60274446e+00 -2.75200576e-01  4.50340271e+01\n",
      "    -2.53077745e+00]\n",
      "   [ 4.96634369e+01 -2.97160316e+00 -3.58577418e+00  4.45983009e+01\n",
      "    -6.39158297e+00]\n",
      "   [ 1.12732071e+02 -5.85852957e+00 -4.63873476e-01  1.50051392e+02\n",
      "    -9.22564793e+00]\n",
      "   [ 9.96388474e+01 -6.57719278e+00 -8.20441818e+00  8.46351776e+01\n",
      "    -1.44686346e+01]\n",
      "   [ 1.27245285e+02 -8.12595367e+00 -1.26978099e+00  1.80913208e+02\n",
      "    -1.35707951e+01]]\n",
      "\n",
      "  [[-2.88167405e+00  1.86015358e+01 -2.53839225e-01 -3.78196311e+00\n",
      "     4.81764374e+01]\n",
      "   [-7.44884491e+00  6.92551956e+01 -1.22476947e+00 -9.72269917e+00\n",
      "     5.81387062e+01]\n",
      "   [-9.53805351e+00  5.04770241e+01 -1.00494230e+00 -1.23680420e+01\n",
      "     1.85749496e+02]\n",
      "   [-1.25298605e+01  1.37461365e+02 -4.80349636e+00 -1.81923294e+01\n",
      "     1.06299194e+02]\n",
      "   [-1.65120888e+01  9.56467819e+01 -1.88656139e+00 -1.99181118e+01\n",
      "     2.82341095e+02]]\n",
      "\n",
      "  [[ 8.61429367e+01 -4.74796200e+00 -5.97430289e-01  1.22232620e+02\n",
      "    -6.70835209e+00]\n",
      "   [ 1.51549423e+02 -9.62810135e+00 -9.34610271e+00  1.40694260e+02\n",
      "    -1.47207794e+01]\n",
      "   [ 2.49595169e+02 -1.50585117e+01 -5.10773659e-01  3.64239899e+02\n",
      "    -2.32907677e+01]\n",
      "   [ 2.76275238e+02 -2.06147690e+01 -1.84614983e+01  2.67932617e+02\n",
      "    -3.31201172e+01]\n",
      "   [ 3.84465820e+02 -2.51205120e+01 -2.93386936e-01  5.74263367e+02\n",
      "    -3.69638939e+01]]\n",
      "\n",
      "  [[-5.28023100e+00  3.80912704e+01 -4.79942888e-01 -8.12790203e+00\n",
      "     9.45775146e+01]\n",
      "   [-1.16930742e+01  1.20516670e+02 -3.56889892e+00 -1.78551178e+01\n",
      "     1.18748734e+02]\n",
      "   [-1.57621984e+01  1.13007523e+02 -1.95624626e+00 -2.53196011e+01\n",
      "     3.21633698e+02]\n",
      "   [-2.32718086e+01  2.53865692e+02 -7.93345976e+00 -3.66003189e+01\n",
      "     2.39142960e+02]\n",
      "   [-2.82843418e+01  1.99032974e+02 -2.69968605e+00 -4.26067009e+01\n",
      "     5.38980103e+02]]]\n",
      "\n",
      "\n",
      " [[[-7.56938577e-01  3.11721158e+00 -1.80030540e-01 -1.13953829e+00\n",
      "     1.28644762e+01]\n",
      "   [-2.31163645e+00  1.03800678e+01 -2.74154216e-01 -3.02562618e+00\n",
      "     1.69854641e+01]\n",
      "   [-2.51501799e+00  6.51829195e+00 -5.78180075e-01 -4.24273968e+00\n",
      "     5.85764046e+01]\n",
      "   [-2.51209331e+00  1.71596279e+01 -1.80891061e+00 -5.57582617e+00\n",
      "     4.05072823e+01]\n",
      "   [-2.62162733e+00  8.98330879e+00 -1.19186485e+00 -5.89023304e+00\n",
      "     7.52145844e+01]]\n",
      "\n",
      "  [[ 3.24289017e+01 -1.59982193e+00 -2.74626166e-01  4.49509125e+01\n",
      "    -2.52600884e+00]\n",
      "   [ 4.95693512e+01 -2.96617293e+00 -3.57925344e+00  4.45131378e+01\n",
      "    -6.37991381e+00]\n",
      "   [ 1.12531998e+02 -5.84776258e+00 -4.62949902e-01  1.49783615e+02\n",
      "    -9.20851231e+00]\n",
      "   [ 9.94643707e+01 -6.56539106e+00 -8.18965816e+00  8.44804001e+01\n",
      "    -1.44421263e+01]\n",
      "   [ 1.27002419e+02 -8.11098003e+00 -1.26721680e+00  1.80569534e+02\n",
      "    -1.35459356e+01]]\n",
      "\n",
      "  [[-2.87635732e+00  1.85687370e+01 -2.53322095e-01 -3.77511215e+00\n",
      "     4.80903931e+01]\n",
      "   [-7.43556070e+00  6.91312790e+01 -1.22234404e+00 -9.70536327e+00\n",
      "     5.80364914e+01]\n",
      "   [-9.52116489e+00  5.03852005e+01 -1.00304258e+00 -1.23456678e+01\n",
      "     1.85419861e+02]\n",
      "   [-1.25062933e+01  1.37210327e+02 -4.79514027e+00 -1.81591415e+01\n",
      "     1.06106499e+02]\n",
      "   [-1.64815807e+01  9.54721451e+01 -1.88321710e+00 -1.98814144e+01\n",
      "     2.81813629e+02]]\n",
      "\n",
      "  [[ 8.59913940e+01 -4.73954058e+00 -5.96207678e-01  1.22015633e+02\n",
      "    -6.69632196e+00]\n",
      "   [ 1.51280640e+02 -9.61105633e+00 -9.32937145e+00  1.40443054e+02\n",
      "    -1.46943130e+01]\n",
      "   [ 2.49159943e+02 -1.50318394e+01 -5.09704530e-01  3.63605621e+02\n",
      "    -2.32494659e+01]\n",
      "   [ 2.75781311e+02 -2.05780830e+01 -1.84283409e+01  2.67454132e+02\n",
      "    -3.30604897e+01]\n",
      "   [ 3.83748077e+02 -2.50744991e+01 -2.93061912e-01  5.73195251e+02\n",
      "    -3.68961143e+01]]\n",
      "\n",
      "  [[-5.27092552e+00  3.80244675e+01 -4.79168892e-01 -8.11364460e+00\n",
      "     9.44103012e+01]\n",
      "   [-1.16726255e+01  1.20303741e+02 -3.56257677e+00 -1.78240910e+01\n",
      "     1.18543770e+02]\n",
      "   [-1.57341757e+01  1.12810722e+02 -1.95291293e+00 -2.52750435e+01\n",
      "     3.21067444e+02]\n",
      "   [-2.32297497e+01  2.53416138e+02 -7.91963291e+00 -3.65351028e+01\n",
      "     2.38712708e+02]\n",
      "   [-2.82332592e+01  1.98672333e+02 -2.69494152e+00 -4.25288353e+01\n",
      "     5.38000244e+02]]]], shape=(3, 5, 5, 5), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 2.8339 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ca7c064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Courses\\24aut_deep_learning\\24aut-deep-learning\\deep-learning-comp3\\testing\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "--------------Evaluation Success-----------------\n",
      "C:\\Users\\User\\Courses\\24aut_deep_learning\\24aut-deep-learning\\deep-learning-comp3\n"
     ]
    }
   ],
   "source": [
    "%cd ./testing\n",
    "!python inception_score.py ../inference/demo output.csv 21\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa35e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  0 (unused)\n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efcd9b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  65664     \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  131328    \n",
      "                                                                 \n",
      " deconv_block (deconv_block)  multiple                 886016    \n",
      "                                                                 \n",
      " deconv_block_1 (deconv_bloc  multiple                 2655488   \n",
      " k)                                                              \n",
      "                                                                 \n",
      " deconv_block_2 (deconv_bloc  multiple                 886016    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " deconv_block_3 (deconv_bloc  multiple                 443648    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " deconv_block_4 (deconv_bloc  multiple                 333056    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " deconv_block_5 (deconv_bloc  multiple                 305408    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " conv_block (conv_block)     multiple                  295552    \n",
      "                                                                 \n",
      " conv_block_1 (conv_block)   multiple                  295552    \n",
      "                                                                 \n",
      " conv_block_2 (conv_block)   multiple                  295552    \n",
      "                                                                 \n",
      " conv_block_3 (conv_block)   multiple                  295552    \n",
      "                                                                 \n",
      " conv_block_4 (conv_block)   multiple                  295552    \n",
      "                                                                 \n",
      " conv_block_5 (conv_block)   multiple                  295552    \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  528384    \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  528384    \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  528384    \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  528384    \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  528384    \n",
      "                                                                 \n",
      " dense_8 (Dense)             multiple                  528384    \n",
      "                                                                 \n",
      " conv_block_6 (conv_block)   multiple                  149248    \n",
      "                                                                 \n",
      " conv_block_7 (conv_block)   multiple                  74048     \n",
      "                                                                 \n",
      " conv_block_8 (conv_block)   multiple                  1104      \n",
      "                                                                 \n",
      " conv_block_9 (conv_block)   multiple                  63        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,874,703\n",
      "Trainable params: 10,869,673\n",
      "Non-trainable params: 5,030\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d6f6c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2, 2, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             multiple                  65664     \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  1048704   \n",
      "                                                                 \n",
      " dense_11 (Dense)            multiple                  257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,702,337\n",
      "Trainable params: 1,114,625\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f10b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f537e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b128e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a11e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865cbfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
