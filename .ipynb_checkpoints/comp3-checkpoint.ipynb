{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a26ad83",
   "metadata": {},
   "source": [
    "# Competition 3: Team 21\n",
    "\n",
    "112062649 王俊皓\n",
    "\n",
    "112062650 廖士傑\n",
    "\n",
    "##  Reverse Image Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "414f827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project settings\n",
    "# enc / gen / dis: True means using a more complex setting, False means using a setting that is closer to template\n",
    "# enc_do_batchnorm: encoder will perform batch normalization to make the text encoding more variant\n",
    "# dis_backbone: the type of backbone in discriminator, can be 'resnet', 'vgg', 'simple'\n",
    "# delete_checkpoint: deletes all checkpoint files before training\n",
    "\n",
    "class experimental_settings:\n",
    "    def __init__(self,\n",
    "                 enc=True,\n",
    "                 enc_do_batchnorm=False,\n",
    "                 gen=True,\n",
    "                 dis=True,\n",
    "                 dis_backbone='simple',\n",
    "                 delete_checkpoint=False):\n",
    "        self.enc = enc\n",
    "        self.gen = gen\n",
    "        self.dis = dis\n",
    "        self.dis_backbone = dis_backbone\n",
    "        self.enc_do_batchnorm = enc_do_batchnorm\n",
    "        self.delete_checkpoint = delete_checkpoint # not implemented yet\n",
    "        \n",
    "        # ============================ #\n",
    "        # automatic\n",
    "        # ============================ #\n",
    "        \n",
    "        self.caption_type = 'sentence' if self.enc else 'id'\n",
    "\n",
    "\n",
    "expSettings = experimental_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32322ac2",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "15576cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7b646",
   "metadata": {},
   "source": [
    "GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d992c613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71f7c6",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "58104ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3bf3f5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d14b0e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n",
      "tf.Tensor(b'the flower shown has yellow anther red pistil and bright red petals <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def id2Sent(ids):\n",
    "    return \" \".join([id2word_dict[idx] for idx in ids]).strip()\n",
    "\n",
    "#def batch_id2Sent(batch_ids):\n",
    "    #return [id2Sent(ids) for ids in batch_ids]\n",
    "    \n",
    "def batch_id2Sent(batch_ids):\n",
    "    def process_single(ids):\n",
    "        # Convert a single tensor of IDs to a sentence\n",
    "        ids = ids.numpy()  # Convert Tensor to NumPy\n",
    "        sentence = \" \".join([id2word_dict.get(idx, \"<UNK>\") for idx in ids])  # Handle unknown IDs\n",
    "        return sentence\n",
    "\n",
    "    # Use tf.py_function to apply Python function inside the TensorFlow graph\n",
    "    sentences = tf.map_fn(\n",
    "        lambda ids: tf.py_function(process_single, [ids], tf.string),\n",
    "        batch_ids,\n",
    "        fn_output_signature=tf.string\n",
    "    )\n",
    "    return sentences\n",
    "\n",
    "\n",
    "print(sent2IdList(text))\n",
    "print(id2Sent(sent2IdList(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "23133915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "text2ImgData = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(text2ImgData)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d2c49264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption2string(cap):\n",
    "    output = []\n",
    "    for sen in cap:\n",
    "        s = \" \".join([id2word_dict[idx] for idx in sen]).strip()\n",
    "        output.append(s.split(' <PAD>')[0])\n",
    "    return output\n",
    "\n",
    "# adding caption as strings\n",
    "text2ImgData['Captions_string'] = text2ImgData['Captions'].apply(caption2string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cdf9314c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>Captions_string</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \\\n",
       "ID                                   \n",
       "6734  ./102flowers/image_06734.jpg   \n",
       "6736  ./102flowers/image_06736.jpg   \n",
       "6737  ./102flowers/image_06737.jpg   \n",
       "6738  ./102flowers/image_06738.jpg   \n",
       "6739  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                        Captions_string  \n",
       "ID                                                       \n",
       "6734  [the petals of the flower are pink in color an...  \n",
       "6736  [this flower has white petals and yellow pisti...  \n",
       "6737  [the petals on this flower are pink with white...  \n",
       "6738  [the flower has a smooth purple petal with whi...  \n",
       "6739  [this white flower has bright yellow stamen wi...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2ImgData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f4a6b09d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the petals of the flower are pink in color and have a yellow center',\n",
       "  'this flower is pink and white in color with petals that are multi colored',\n",
       "  'the purple petals have shades of white with white anther and filament',\n",
       "  'this flower has large pink petals and a white stigma in the center',\n",
       "  'this flower has petals that are pink and has a yellow stamen',\n",
       "  'a flower with short and wide petals that is light purple',\n",
       "  'this flower has small pink petals with a yellow center',\n",
       "  'this flower has large rounded pink petals with curved edges and purple veins',\n",
       "  'this flower has purple petals as well as a white stamen']]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2ImgData['Captions_string'][:1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5efa616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_SIZE = 64\n",
    "IMAGE_HEIGHT = IMAGE_SIZE\n",
    "IMAGE_WIDTH = IMAGE_SIZE\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path, caption_type='id'):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    if caption_type == 'id':\n",
    "        caption = tf.cast(caption, tf.int32)\n",
    "    elif caption_type == 'sentence':\n",
    "        caption = tf.convert_to_tensor(caption, dtype=tf.string)\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator, caption_type='id'):\n",
    "    # load the training data into two NumPy arrays\n",
    "    if filenames != None:\n",
    "        df = pd.read_pickle(filenames)\n",
    "    else:\n",
    "        df = text2ImgData\n",
    "    \n",
    "    if caption_type == 'id':\n",
    "        captions = df['Captions'].values\n",
    "    elif caption_type == 'sentence':\n",
    "        captions = df['Captions_string'].values\n",
    "    else:\n",
    "        raise ValueError('for dataset_generator, caption_type= should be \\'id\\' or \\'sentence\\'.')\n",
    "        \n",
    "    caption = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose one of them randomly for training\n",
    "    \n",
    "    # ============================================ #\n",
    "    # TODO: augmentation\n",
    "    # idea 1 (difficulty: easy)\n",
    "    #     training data has multiple captions, right now it picks a random one.\n",
    "    #     we can make it so that every caption is an entry and multiple captions link to the same image.\n",
    "    # idea 2 (difficulty: medium)\n",
    "    #     after text embedding, use the average of 2 caption embeddings to generate a new caption.\n",
    "    #     the data does not need to have an image tied to it, it just have the label 0 (fake image).\n",
    "    # ============================================ #\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    if caption_type == 'id':\n",
    "        caption = caption.astype(np.int)\n",
    "        \n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    datagen_func = lambda cap, img: data_generator(cap, img, caption_type=caption_type)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(datagen_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "710669ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = dataset_generator(\n",
    "    #data_path + '/text2ImgData.pkl',\n",
    "    None,\n",
    "    BATCH_SIZE, \n",
    "    training_data_generator, \n",
    "    caption_type=expSettings.caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d9640f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (64, 64, 64, 3)\n",
      "Caption shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "# dataset testing ground\n",
    "for img, cap in dataset.take(1):\n",
    "    print(\"Image shape:\", img.numpy().shape)\n",
    "    print(\"Caption shape:\", cap.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4e941524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2DTranspose, Conv2D, BatchNormalization, LeakyReLU, Dense, Dropout\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "# custom layers\n",
    "class flattened_dense(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a dense layer that is made compatible with convolution layers\n",
    "    by flattening the input first and followed by a dense layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=64, kernel_initializer=\"glorot_uniform\"):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(channels, kernel_initializer=kernel_initializer)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        fl = self.flatten(inputs)\n",
    "        return self.dense(fl)\n",
    "    \n",
    "class conv_block(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a convolution layer with batch normalization and leaky relu activation\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=128, kernel_size=1, strides=1, kernel_initializer=HeNormal()):\n",
    "        super().__init__()\n",
    "        self.conv = Conv2D(filters=filters,\n",
    "                           kernel_size = (kernel_size, kernel_size),\n",
    "                           strides=(strides, strides),\n",
    "                           padding='same',\n",
    "                           kernel_initializer=kernel_initializer)\n",
    "        self.bn = BatchNormalization()\n",
    "        self.activation = LeakyReLU(alpha=0.1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.activation(self.bn(self.conv(inputs)))\n",
    "    \n",
    "class deconv_block(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a deconvolution layer with batch normalization and leaky relu activation\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=128, kernel_size=4, strides=2, kernel_initializer='glorot_uniform'):\n",
    "        super().__init__()\n",
    "        self.deconv = Conv2DTranspose(filters=filters,\n",
    "                                    kernel_size = (kernel_size, kernel_size),\n",
    "                                    strides=(strides, strides),\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=kernel_initializer)\n",
    "        self.bn = BatchNormalization()\n",
    "        self.activation = LeakyReLU(alpha=0.1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.activation(self.bn(self.deconv(inputs)))\n",
    "    \n",
    "class dense_block(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    a dense layer with batch normalization and leaky relu activation\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=128, kernel_initializer='glorot_uniform'):\n",
    "        super().__init__()\n",
    "        self.d = Dense(filters, kernel_initializer=kernel_initializer)\n",
    "        self.bn = BatchNormalization()\n",
    "        self.activation = LeakyReLU(alpha=0.1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self.d(inputs)\n",
    "        outputs = self.bn(outputs)\n",
    "        return self.activation(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7b79a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas, experimental=False, do_batchnorm=False):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.exp=experimental\n",
    "        self.do_batchnorm = do_batchnorm\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "        if self.exp:\n",
    "            self.embed = hub.load('./checkpoints/universal_sentence_encoder')\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        if self.exp:\n",
    "            with tf.device('/CPU:0'): # TODO if you find a way to use GPU, go for it.\n",
    "                output_last = self.embed(text)\n",
    "                \n",
    "            state = hidden # not updating state for compatibility reasons\n",
    "            \n",
    "        else:\n",
    "            text = self.embedding(text)\n",
    "            output, state = self.gru(text, initial_state = hidden)\n",
    "            output_last = output[:, -1, :]\n",
    "        \n",
    "        # normalization in-batch\n",
    "        if self.do_batchnorm:\n",
    "            mean = tf.reduce_mean(output_last, axis=0, keepdims=True)  # Mean across the batch\n",
    "            std = tf.math.reduce_std(output_last, axis=0, keepdims=True)  # Std across the batch\n",
    "            normalized = (output_last - mean) / (std + 1e-6)  # Avoid division by zero\n",
    "        else:\n",
    "            normalized = output_last\n",
    "        \n",
    "        return normalized, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e7dcc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas, experimental=False):\n",
    "        super(Generator, self).__init__()\n",
    "        self.exp = experimental\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'], kernel_initializer=\"glorot_uniform\")\n",
    "        self.d2 = tf.keras.layers.Dense(64*64*3, kernel_initializer=\"glorot_uniform\")\n",
    "        if self.exp:\n",
    "            self.deconv_depth = int(math.log(IMAGE_SIZE, 2)) - 1\n",
    "            self.starter = dense_block(filters=2*2*512)\n",
    "            self.deconv = [\n",
    "                deconv_block(filters=512, kernel_initializer=HeNormal()),\n",
    "                conv_block(filters=512, kernel_size=1, strides=1),\n",
    "                deconv_block(filters=256, kernel_initializer=HeNormal()),\n",
    "                #conv_block(filters=256, kernel_size=1, strides=1),\n",
    "                #Dropout(0.2),\n",
    "                deconv_block(filters=128, kernel_initializer=HeNormal()),\n",
    "                conv_block(filters=128, kernel_size=1, strides=1),\n",
    "                #Dropout(0.2),\n",
    "                deconv_block(filters=64, kernel_initializer=HeNormal()),\n",
    "                #conv_block(filters=64, kernel_size=1, strides=1),\n",
    "                deconv_block(filters=32, kernel_initializer=HeNormal()),\n",
    "                conv_block(filters=32, kernel_size=1, strides=1)\n",
    "            ]\n",
    "            self.headf = conv_block(filters=3, kernel_size=1, strides=1)\n",
    "            \n",
    "    def call(self, text, noise_z, debug_output=False, training=False):\n",
    "        # deconvolution\n",
    "        if self.exp:\n",
    "            noisy_text = tf.concat([text, noise_z], axis=1) * 10 # amplify the input a bit, they seem fairly close to 0.\n",
    "            img = self.starter(noisy_text)\n",
    "            \n",
    "            img = tf.reshape(img, [-1, 2, 2, 512])\n",
    "            debug = []\n",
    "            for layer in self.deconv:\n",
    "                if isinstance(layer, tf.keras.layers.Dropout) and training:\n",
    "                    continue\n",
    "                debug.append(img)\n",
    "                img = layer(img)\n",
    "\n",
    "            img = self.headf(img)\n",
    "            logits = tf.reshape(img, [-1, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "            output = tf.nn.tanh(logits)\n",
    "        \n",
    "        # concatenate input text and random noise\n",
    "        else:\n",
    "            text = self.flatten(text)\n",
    "            text = self.d1(text)\n",
    "            text = tf.nn.leaky_relu(text)\n",
    "            text_concat = tf.concat([noise_z, text], axis=1)\n",
    "            text_concat = self.d2(text_concat)\n",
    "        \n",
    "            logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "            output = tf.nn.tanh(logits)\n",
    "            debug_output = output\n",
    "        \n",
    "        if debug_output:\n",
    "            return logits, output, debug\n",
    "        else:\n",
    "            return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a78cfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas, experimental=False, backbone='simple'):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.exp = experimental\n",
    "        self.bbtype = backbone\n",
    "        self.hparas = hparas\n",
    "        if self.exp:\n",
    "            if self.bbtype == 'resnet':\n",
    "                self.resnet_base = ResNet50(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), weights='imagenet', include_top=False)\n",
    "                for layer in self.resnet_base.layers:\n",
    "                    layer.trainable = False\n",
    "            elif self.bbtype == 'vgg':\n",
    "                self.vgg_base = VGG16(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), weights='imagenet', include_top=False)\n",
    "                for layer in self.vgg_base.layers:\n",
    "                    layer.trainable = False\n",
    "            elif self.bbtype == 'simple':\n",
    "                self.conv = [\n",
    "                    conv_block(filters=512, kernel_size=3, strides=2),\n",
    "                    #conv_block(filters=512, kernel_size=3, strides=1),\n",
    "                    conv_block(filters=512, kernel_size=1, strides=1),\n",
    "                    conv_block(filters=256, kernel_size=3, strides=2),\n",
    "                    #conv_block(filters=256, kernel_size=3, strides=1),\n",
    "                    conv_block(filters=256, kernel_size=1, strides=1),\n",
    "                    conv_block(filters=128, kernel_size=3, strides=2),\n",
    "                    #conv_block(filters=128, kernel_size=3, strides=1),\n",
    "                    conv_block(filters=128, kernel_size=1, strides=1),\n",
    "                    #conv_block(filters=64, kernel_size=3, strides=2),\n",
    "                    #conv_block(filters=64, kernel_size=3, strides=1),\n",
    "                    #conv_block(filters=64, kernel_size=1, strides=1)\n",
    "                ]\n",
    "                # more simple one\n",
    "                #self.conv1 = conv_block(filters=256, kernel_size=3, strides=1)\n",
    "                #self.conv2 = conv_block(filters=64, kernel_size=3, strides=1)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, img, text):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d_text(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        if self.exp:\n",
    "            if self.bbtype == 'resnet':\n",
    "                img = self.resnet_base(img)\n",
    "            elif self.bbtype == 'vgg':\n",
    "                img = self.vgg_base(img)\n",
    "            elif self.bbtype == 'simple':\n",
    "                for layer in self.conv: # see init for spec\n",
    "                    img = layer(img)\n",
    "        img = self.flatten(img)\n",
    "        img = self.d_img(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=1)\n",
    "        \n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac84cb",
   "metadata": {},
   "source": [
    "Parameters and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46263c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LR_GEN': 1e-4,\n",
    "    'LR_DIS': 1e-5,\n",
    "    'LR_DECAY': 0.5,                          # unused\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 1000,                            # number of epoch for demo\n",
    "    'N_SAMPLE': num_training_sample,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 1                           # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a3d61caf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(hparas, \n",
    "                           experimental=expSettings.enc,\n",
    "                           do_batchnorm=expSettings.enc_do_batchnorm)\n",
    "\n",
    "generator = Generator(hparas,\n",
    "                      experimental=expSettings.gen)\n",
    "\n",
    "discriminator = Discriminator(hparas,\n",
    "                              experimental=expSettings.dis,\n",
    "                              backbone=expSettings.dis_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "90824abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (64, 64, 64, 3)\n",
      "Caption shape: (64,)\n",
      "Caption embed shape: (64, 512)\n"
     ]
    }
   ],
   "source": [
    "# test text encoder\n",
    "for img, cap in dataset.take(1):\n",
    "    print(\"Image shape:\", img.numpy().shape)\n",
    "    print(\"Caption shape:\", cap.shape)\n",
    "    with tf.device('/CPU:0'):\n",
    "        output, _ = text_encoder(cap, text_encoder.initialize_hidden_state())\n",
    "        print(\"Caption embed shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "220a1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3a45f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.nn import sigmoid_cross_entropy_with_logits\n",
    "\n",
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = sigmoid_cross_entropy_with_logits(tf.ones_like(real_logits), real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = sigmoid_cross_entropy_with_logits(tf.zeros_like(fake_logits), fake_logits)\n",
    "    total_loss = tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss)\n",
    "                                 \n",
    "    return total_loss\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return tf.reduce_mean(sigmoid_cross_entropy_with_logits(tf.ones_like(fake_output), fake_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "21f720d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR_GEN'], clipvalue=0.1)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR_DIS'], clipvalue=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "24d5f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "if expSettings.delete_checkpoint:\n",
    "    for f in os.listdir(hparas['CHECKPOINTS_DIR']):\n",
    "        file_path = os.path.join(hparas['CHECKPOINTS_DIR'], f)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "\n",
    "# one benefit of tf.train.Checkpoint() API is we can save everything seperately\n",
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 text_encoder=text_encoder,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "ckptManager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=hparas['CHECKPOINTS_DIR'], max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b35f60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, caption, hidden, imshow=False):\n",
    "    # random noise for generator\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=0.1)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        text_embed, hidden = text_encoder(caption, hidden)\n",
    "        _, fake_image = generator(text_embed, noise, training=True)\n",
    "        if imshow:\n",
    "            plt.imshow(fake_image[0])\n",
    "\n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "        g_loss = generator_loss(fake_logits)\n",
    "        d_loss = discriminator_loss(real_logits, fake_logits)\n",
    "\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grad_d = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_d, discriminator.trainable_variables))\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c25ce5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5d5af",
   "metadata": {},
   "source": [
    "Sample Debugging (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d68c0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "09b260b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size, caption_type='id'):\n",
    "    if caption_type == 'sentence':\n",
    "        caption = caption2string(caption)\n",
    "    caption = np.asarray(caption)\n",
    "    if caption_type == 'id':\n",
    "        caption = caption.astype(np.int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e086c8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'], caption_type=expSettings.caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2946f2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption shape: (64,)\n",
      "Caption embeddings: tf.Tensor(\n",
      "[[-0.01346336  0.06881763 -0.05529514 ... -0.01518281 -0.00633275\n",
      "   0.01500697]\n",
      " [-0.01346336  0.06881763 -0.05529514 ... -0.01518281 -0.00633275\n",
      "   0.01500697]\n",
      " [-0.01346336  0.06881763 -0.05529514 ... -0.01518281 -0.00633275\n",
      "   0.01500697]\n",
      " ...\n",
      " [-0.00790838  0.06233827  0.01107207 ... -0.02398296 -0.03450323\n",
      "  -0.00868433]\n",
      " [-0.00790838  0.06233827  0.01107207 ... -0.02398296 -0.03450323\n",
      "  -0.00868433]\n",
      " [-0.00790838  0.06233827  0.01107207 ... -0.02398296 -0.03450323\n",
      "  -0.00868433]], shape=(64, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# test the sample dataset\n",
    "for cap in sample_sentence.take(1):\n",
    "    print(\"Caption shape:\", cap.numpy().shape)\n",
    "    emb, _ = text_encoder(cap, text_encoder.initialize_hidden_state())\n",
    "    print(\"Caption embeddings:\", emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c4af2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples/demo'):\n",
    "    os.makedirs('samples/demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34dfd3",
   "metadata": {},
   "source": [
    "Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "620661c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    lowest_gen_loss = 1e10\n",
    "    \n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        batchid = 0\n",
    "        start = time.time()\n",
    "        imshow = False\n",
    "        \n",
    "        for image, caption in dataset:\n",
    "            batchid += 1\n",
    "            g_loss, d_loss = train_step(image, caption, hidden, imshow=imshow)\n",
    "            imshow = False\n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "        \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        # save the model if lower gen loss is achieved\n",
    "        if g_total_loss < lowest_gen_loss:\n",
    "            lowest_gen_loss = g_total_loss\n",
    "            ckptManager.save()\n",
    "            print('new lowest. saving model.')\n",
    "        elif g_total_loss < 3 * lowest_gen_loss:\n",
    "            ckptManager.save()\n",
    "            print('within save threshold. saving model.')\n",
    "        else:\n",
    "            lowest_gen_loss *= 1.02\n",
    "            \n",
    "        print('======================================')\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed, hidden)\n",
    "            save_images(fake_image,\n",
    "                        [ni, ni],\n",
    "                        'samples/demo/train_{:02d}.jpg'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6c752c76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/GatherV2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/GatherV2_grad/Reshape:0\", shape=(None, 320), dtype=float32), dense_shape=Tensor(\"gradients/EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/GatherV2_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen_loss: 5.2334, disc_loss: 0.0927\n",
      "Time for epoch 1 is 28.9406 sec\n",
      "new lowest. saving model.\n",
      "======================================\n",
      "Epoch 2, gen_loss: 7.7874, disc_loss: 0.0089\n",
      "Time for epoch 2 is 21.6097 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 3, gen_loss: 8.0913, disc_loss: 0.0043\n",
      "Time for epoch 3 is 20.9124 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 4, gen_loss: 9.5240, disc_loss: 0.0119\n",
      "Time for epoch 4 is 20.9053 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 5, gen_loss: 7.9706, disc_loss: 0.0114\n",
      "Time for epoch 5 is 20.0451 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 6, gen_loss: 7.5204, disc_loss: 0.0084\n",
      "Time for epoch 6 is 21.4251 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 7, gen_loss: 6.9531, disc_loss: 0.0074\n",
      "Time for epoch 7 is 20.9597 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 8, gen_loss: 7.1805, disc_loss: 0.0150\n",
      "Time for epoch 8 is 21.2675 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 9, gen_loss: 8.1372, disc_loss: 0.0069\n",
      "Time for epoch 9 is 20.5607 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 10, gen_loss: 8.1769, disc_loss: 0.0052\n",
      "Time for epoch 10 is 20.9451 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 11, gen_loss: 8.6065, disc_loss: 0.0056\n",
      "Time for epoch 11 is 22.1934 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 12, gen_loss: 9.2415, disc_loss: 0.0028\n",
      "Time for epoch 12 is 22.6139 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n",
      "Epoch 13, gen_loss: 8.9139, disc_loss: 0.0022\n",
      "Time for epoch 13 is 20.8197 sec\n",
      "within save threshold. saving model.\n",
      "======================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1748\\32322937.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'N_EPOCH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1748\\992725410.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaption\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mbatchid\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimshow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mimshow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mg_total_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2454\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2456\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1861\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_20241101\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_caption2string(ls):\n",
    "    return \" \".join([id2word_dict[idx] for idx in ls]).strip().split(' <PAD>')[0]\n",
    "\n",
    "def testing_data_generator(caption, index, caption_type='id'):\n",
    "    if caption_type == 'id':\n",
    "        caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator, caption_type='id'):\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    \n",
    "    if caption_type == 'sentence':\n",
    "        data['Captions_string'] = data['Captions'].apply(test_caption2string)\n",
    "        captions = data['Captions_string'].values\n",
    "    elif caption_type == 'id':\n",
    "        captions = data['Captions'].values\n",
    "        \n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    if caption_type == 'id':\n",
    "        caption = caption.astype(np.int)\n",
    "        \n",
    "    datagen_func = lambda cap, img: data_generator(cap, img, caption_type=caption_type)\n",
    "        \n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(datagen_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7abd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator, caption_type=expSettings.caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the testing dataset\n",
    "for cap, img in testing_dataset.take(1):\n",
    "    print(\"Image shape:\", img.numpy().shape)\n",
    "    print(\"Caption shape:\", cap.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f624f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./inference/demo'):\n",
    "    os.makedirs('./inference/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=0.1, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    print(sample_seed[0:3, :])\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "            if i == 0 and step == 1: \n",
    "                #print(captions)\n",
    "                text_embed_t, hidden_t = text_encoder(captions, hidden)\n",
    "                #print(text_embed_t)\n",
    "                print(fake_image[0:1, 0:5, 0:5, :])\n",
    "                img_logits, _, debug = generator(text_embed_t, sample_seed, debug_output=True)\n",
    "                print(debug[0][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[1][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[2][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[3][0:3, 0:5, 0:5, 0:5])\n",
    "                print(debug[4][0:3, 0:5, 0:5, 0:5])\n",
    "                print(img_logits[0:3, 0:5, 0:5, :])\n",
    "                pred_logit, pred = discriminator(fake_image, text_embed_t)\n",
    "                print(pred_logit)\n",
    "                \n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a53a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checkpoint.restore(checkpoint_dir + f'/ckpt-50')\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Restoring from {latest_checkpoint}\")\n",
    "    checkpoint.restore(latest_checkpoint)\n",
    "else:\n",
    "    print(\"No checkpoint found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59c63a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8616a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in generator.starter.variables:\n",
    "    print(var.name, var.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in discriminator.variables:\n",
    "    print(var.name, var.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./testing\n",
    "!python inception_score.py ../inference/demo output.csv 21\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd9b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f6c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f10b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f537e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b128e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a11e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865cbfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba08b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304dd53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
